context,decision,context_input_gpt3,context_input_chatGPT,context_input_T5
"At some point, every Arachne application needs to start; to bootstrap
itself from a static project or deployment artifact, initialize what
needs initializing, and begin servicing requests, connecting to
databases, processing data, etc.

There are several logically inherent subtasks to this bootstrapping process, which can be broken down as follows.

- Starting the JVM
    - Assembling the project's dependencies
    - Building a JVM classpath
    - Starting a JVM
- Arachne Specific
    - Reading the initial user-supplied configuration (i.e, the configuration scripts from [ADR-005](adr-005-user-facing-config.md))
    - Initializing the Arachne configuration given a project's set of modules (described in [ADR-002](adr-002-configuration.md) and [ADR-004](adr-004-module-loading.md))
- Application Specific
    - Instantiate user and module-defined objects that needs to exist at runtime.
    - Start and stop user and module-defined services

As discussed in [ADR-004](adr-004-module-loading.md), tasks in the ""starting the JVM"" category are not in-scope for Arachne; rather, they are offloaded to whatever build/dependency tool the project is using (usually either [boot](http://boot-clj.com) or [leiningen](http://leiningen.org).)

This leaves the Arachne and application-specific startup tasks. Arachne should provide an orderly, structured startup (and shutdown) procedure, and make it possible for modules and application authors to hook into it to ensure that their own code initializes, starts and stops as desired.

Additionally, it must be possible for different system components to have dependencies on eachother, such that when starting, services start *after* the services upon which they depend. Stopping should occur in reverse-dependency order, such that a service is never in a state where it is running but one of its dependencies is stopped.","#### Components

Arachne uses the [Component](https://github.com/stuartsierra/component) library to manage system components. Instead of requiring users to define a component system map manually, however, Arachne itself builds one based upon the Arachne config via *Configuration Entities* that appear in the configuration.

Component entities may be added to the config directly by end users (via a initialization script as per [ADR-005](adr-005-user-facing-config.md)), or by modules in their `configure` function ([ADR-004](adr-004-module-loading.md).)

Component entities have attributes which indicates which other components they depend upon. Circular dependencies are not allowed; the component dependency structure must form a Directed Acyclic Graph (DAG.) The dependency attributes also specify the key that Component will use to `assoc` dependencies.

Component entities also have an attribute that specifies a *component constructor function* (via a fully qualified name.) Component constructor functions must take two arguments: the configuration, and the entity ID of the component that is to be constructed. When invoked, a component constructor must return a runtime component object, to be used by the Component library. This may be any object that implements `clojure.lang.Associative`, and may also optionally satisfy Component's `Lifecycle` protocol.

#### Arachne Runtime

The top-level entity in an Arachne system is a reified *Arachne Runtime* object. This object contains both the Component system object, and the configuration value upon which the runtime is based. It satisfies the `Lifecycle` protocol itself; when it is started or stopped, all of the component objects it contains are started or stopped in the appropriate order.

The constructor function for a Runtime takes a configuration value and some number of ""roots""; entity IDs or lookup refs of Component entities in the config. Only these root components and their transitive dependencies will be instantiated or added to the Component system. In other words, only component entities that are actually used will be instantiated; unused component entities defined in the config will be ignored.

A `lookup` function will be provided to find the runtime object instance of a component, given its entity ID or lookup ref in the configuraiton.

#### Startup Procedure

Arachne will rely upon an external build tool (such as boot or leiningen.) to handle downloading dependencies, assembling a classpath, and starting a JVM.

Once JVM with the correct classpath is running, the following steps are required to yield a running Arachne runtime:

1. Determine a set of modules to use (the ""active modules"")
2. Build a configuration schema by querying each active module using its `schema` function ([ADR-004](module-loading.md))
3. Update the config with initial configuration data from user init scripts ([ADR-005](adr-005-user-facing-config.md))
4. In module dependency order, give each module a chance to query and update the configuration using its `configure` function ([ADR-004](module-loading.md))
5. Create a new Arachne runtime, given the configuration and a set of root components.
6. Call the runtime's `start` method. 

The Arachne codebase will provide entry points to automatically perform these steps for common development and production scenarios. Alternatively, they can always be be executed individually in a REPL, or composed in custom startup functions.
 
","Architectural Decision Record

## Context
At some point, every Arachne application needs to start; to bootstrap
itself from a static project or deployment artifact, initialize what
needs initializing, and begin servicing requests, connecting to
databases, processing data, etc.

There are several logically inherent subtasks to this bootstrapping process, which can be broken down as follows.

- Starting the JVM
    - Assembling the project's dependencies
    - Building a JVM classpath
    - Starting a JVM
- Arachne Specific
    - Reading the initial user-supplied configuration (i.e, the configuration scripts from [ADR-005](adr-005-user-facing-config.md))
    - Initializing the Arachne configuration given a project's set of modules (described in [ADR-002](adr-002-configuration.md) and [ADR-004](adr-004-module-loading.md))
- Application Specific
    - Instantiate user and module-defined objects that needs to exist at runtime.
    - Start and stop user and module-defined services

As discussed in [ADR-004](adr-004-module-loading.md), tasks in the ""starting the JVM"" category are not in-scope for Arachne; rather, they are offloaded to whatever build/dependency tool the project is using (usually either [boot](http://boot-clj.com) or [leiningen](http://leiningen.org).)

This leaves the Arachne and application-specific startup tasks. Arachne should provide an orderly, structured startup (and shutdown) procedure, and make it possible for modules and application authors to hook into it to ensure that their own code initializes, starts and stops as desired.

Additionally, it must be possible for different system components to have dependencies on eachother, such that when starting, services start *after* the services upon which they depend. Stopping should occur in reverse-dependency order, such that a service is never in a state where it is running but one of its dependencies is stopped.

## Decision
","## Context

At some point, every Arachne application needs to start; to bootstrap
itself from a static project or deployment artifact, initialize what
needs initializing, and begin servicing requests, connecting to
databases, processing data, etc.

There are several logically inherent subtasks to this bootstrapping process, which can be broken down as follows.

- Starting the JVM
    - Assembling the project's dependencies
    - Building a JVM classpath
    - Starting a JVM
- Arachne Specific
    - Reading the initial user-supplied configuration (i.e, the configuration scripts from [ADR-005](adr-005-user-facing-config.md))
    - Initializing the Arachne configuration given a project's set of modules (described in [ADR-002](adr-002-configuration.md) and [ADR-004](adr-004-module-loading.md))
- Application Specific
    - Instantiate user and module-defined objects that needs to exist at runtime.
    - Start and stop user and module-defined services

As discussed in [ADR-004](adr-004-module-loading.md), tasks in the ""starting the JVM"" category are not in-scope for Arachne; rather, they are offloaded to whatever build/dependency tool the project is using (usually either [boot](http://boot-clj.com) or [leiningen](http://leiningen.org).)

This leaves the Arachne and application-specific startup tasks. Arachne should provide an orderly, structured startup (and shutdown) procedure, and make it possible for modules and application authors to hook into it to ensure that their own code initializes, starts and stops as desired.

Additionally, it must be possible for different system components to have dependencies on eachother, such that when starting, services start *after* the services upon which they depend. Stopping should occur in reverse-dependency order, such that a service is never in a state where it is running but one of its dependencies is stopped.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
At some point, every Arachne application needs to start; to bootstrap
itself from a static project or deployment artifact, initialize what
needs initializing, and begin servicing requests, connecting to
databases, processing data, etc.

There are several logically inherent subtasks to this bootstrapping process, which can be broken down as follows.

- Starting the JVM
    - Assembling the project's dependencies
    - Building a JVM classpath
    - Starting a JVM
- Arachne Specific
    - Reading the initial user-supplied configuration (i.e, the configuration scripts from [ADR-005](adr-005-user-facing-config.md))
    - Initializing the Arachne configuration given a project's set of modules (described in [ADR-002](adr-002-configuration.md) and [ADR-004](adr-004-module-loading.md))
- Application Specific
    - Instantiate user and module-defined objects that needs to exist at runtime.
    - Start and stop user and module-defined services

As discussed in [ADR-004](adr-004-module-loading.md), tasks in the ""starting the JVM"" category are not in-scope for Arachne; rather, they are offloaded to whatever build/dependency tool the project is using (usually either [boot](http://boot-clj.com) or [leiningen](http://leiningen.org).)

This leaves the Arachne and application-specific startup tasks. Arachne should provide an orderly, structured startup (and shutdown) procedure, and make it possible for modules and application authors to hook into it to ensure that their own code initializes, starts and stops as desired.

Additionally, it must be possible for different system components to have dependencies on eachother, such that when starting, services start *after* the services upon which they depend. Stopping should occur in reverse-dependency order, such that a service is never in a state where it is running but one of its dependencies is stopped."
"[ADR-002](adr-002-configuration.md) indicates that we will store the
entire application config in a single rich data structure with a schema.

### Config as Database

This implies that it should be possible to easily search, query and
update the configuration value. It also implies that the configuration
value is general enough to store arbitrary data; we don't know what
kinds of things users or module authors will need to include.

If what we need is a system that allows you to define, query, and
update arbitrary data with a schema, then we are looking for a
database.

Required data store characteristics:

1. It must be available under a permissive open source
   license. Anything else will impose unwanted restrictions on who can
   use Arachne.
2. It can operate embedded in a JVM process. We do not want to force
   users to install anything else or run multiple processes just to
   get Arachne to work.
3. The database must be serializable. It must be possible to write the
   entire configuration to disk, and then reconstitute it in the same
   exact state in a separate process.
4. Because modules build up the schema progressively, the schema must
   be inherently extensible. It should be possible for modules to
   progressively add both new entity types and new attributes to
   existing entity types.
5. It should be usable from Clojure without a painful impedance mismatch.

### Configuration as Ontology

As an extension of the rationale discussed in
[ADR-002](adr-002-configuration.md), it is useful to enumerate the
possible use cases of the configuration and configuration schema
together.

- The configuration is read by the application during bootstrap and
  controls the behavior of the application.
- The configuration schema defines what types of values the
  application can or will read to modify its structure and behavior at
  boot time and run time.
- The configuration is how an application author communicates their
  intent about how their application should fit together and run, at a
  higher, more conceptual level than code.
- The configuration schema is how module authors communicate to
  application authors what settings, entities and structures
  are available for them to use in their applications.
- The configuration schema is how module authors communicate to other
  potential module authors what their extension points are; module
  extenders can safely read or write any entities/attributes declared
  by the modules upon which they depend.
- The configuration schema can be used to validate a particular
  configuration, and explain where and how it deviates from what is
  actually supported.
- The configuration can be exposed (via user interfaces of various
  types) to end users for analytics and debugging, explaining the
  structure of their application and why things are the way they are.
- A serialization of the configuration, together with a particular
  codebase (identified by a git SHA) form a precise, complete, 100%
  reproducible definition of the behavior of an application.

To the extent that the configuration schema expresses and communicates
the ""categories of being"" or ""possibility space"" of an application, it
is a formal [Ontology](https://en.wikipedia.org/wiki/Ontology). This is
a desirable characteristic, and to the degree that it is practical to
do so, it will be useful to learn from or re-use existing work around
formal ontological systems.


### Implementation Options

There are instances of four broad categories of data stores that match
the first three of the data store characteristics defined above.

- Relational (Derby, HSQLDB, etc)
- Key/value (BerkelyDB, hashtables, etc)
- RDF/RDFs/OWL stores (Jena)
- Datomic-style (Datascript)

We can eliminate relational solutions fairly quickly; SQL schemas are
not generally extensible or flexible, failing condition #4. In
addition, they do not fare well on #5 -- using SQL for queries and updates
is not particularly fluent in Clojure.

Similarly, we can eliminate key/value style data stores. In general,
these do not have schemas at all (or at least, not the type of rich
schema that provides a meaningful data contract or ontology, which is the point
for Arachne.)

This leaves solutions based on the RDF stack, and Datomic-style data
stores. Both are viable options which would provide unique benefits
for Arachne, and both have different drawbacks.

Explaining the core technical characteristics of RDF/OWL and Datomic
is beyond the scope of this document; please see the
[Jena](https://jena.apache.org/documentation/index.html) and
 [Datomic](http://docs.datomic.com) documentation for more
details. More information on RDF, OWL and the Semantic web in general:

- [Wikipedia article on RDF](https://en.wikipedia.org/wiki/Resource_Description_Framework)
- [Wikipedia article on OWL](https://en.wikipedia.org/wiki/Web_Ontology_Language)
- [OWL Semantics](http://www.w3.org/TR/owl-semantics/) standards document.

### RDF

The clear choice for a JVM-based, permissively licensed,
standards-compliant RDF API is Apache Jena.

#### Benefits for Arachne

- OWL is a good fit insofar as Arachne's goal is to define an
  ontology of applications. The point of the configuration schema is
  first and foremost to serve as unambiguous communication regarding
  the types of entities that can exist in an application, and what the
  possible relationships between them are. By definition, this is
  defining an ontology, and is the exact use case which OWL is
  designed to address.
- Information model is a good fit for Clojure: tuples and declarative logic.
- Open and extensible by design.
- Well researched by very smart people, likely to avoid common
  mistakes that would result from building an ontology-like system
  ourselves.
- Existing technology, well known beyond the Clojure
  ecosystem. Existing tools could work with Arachne project
  configurations out of the box.
- The open-world assumption is a good fit for Arachne's per-module
  schema modeling, since modules cannot know what other modules might
  be present in the application.
- We're likely to want to introduce RDFs/OWL to the application
  anyway, at some point, as an abstract entity meta-schema (note: this
  has not been firmly decided yet.)

#### Tradeoffs for Arachne (with mitigations)

- OWL is complex. Learning to use it effectively is a skill in its own
  right and it might be asking a lot to require of module authors.
- OWLs representation of some common concepts can be verbose and/or
  convoluted in ways that would make schema more difficult to
  read/write. (e.g, Restriction classes)
- OWL is not a schema. Although the open world assumption is valid and
  good when writing ontologies, it means that OWL inferencing is
  incapable of performing many of the kind of validations we would
  want to apply once we do have a complete configuration and want to
  check it for correctness. For example, open-world reasoning can
  never validate a `owl:minCardinality` rule.
    - Mitigation: Although OWL inferencing cannot provide closed-world
    validation of a given RDF dataset, such tools do exist. Some
    mechanisms for validating a particular closed set of RDF triples
    include:
       1. Writing SPARQL queries that catch various types of validation errors.
       2. Deriving validation errors using Jena's rules engine.
       3. Using an existing RDF validator such as
      [Eyeball](https://jena.apache.org/documentation/tools/eyeball-getting-started.html)
      (although, unfortunately, Eyeball does not seem to be well
      maintained.)
    - For Clojure, it would be possible to validate a given OWL class
      by generating a specification using `clojure.spec` that could be
      applied to concrete instances of the class in their map form.
- Jena's API is aggressively object oriented and at odds with Clojure
  idioms.
    - Mitigation: Write a data-oriented wrapper (note: I have a
    working proof of concept already.)
- SPARQL is a string-based query language, as opposed to a composable data API.
    - Mitigation: It is possible to hook into Jena's ARQ query engine
      at the object layer, and expose a data-oriented API from there,
      with SPARQL semantics but an API similar to Datomic datalog.
- OWL inferencing is known to have performance issues with complex
  inferences. While Arachne configurations are tiny (as knowledge bases
  go), and we are unlikely to use the more esoteric derivations, it is
  unknown whether this will cause problems with the kinds of
  ontologies we do need.
    - Mitigation: We could restrict ourselves to the OWL DL or even
      OWL Lite sub-languages, which have more tractable inferencing
      rules.
- Jena's APIs are such that it is impossible to write an immutable
  version of a RDF model (at least without breaking most of Jena's
  API.) It's trivial to write a data-oriented wrapper, but intractable
  to write a persistent immutable one.

### Datomic

Note that Datomic itself does not satisfy the first requirement; it is
closed-source, proprietary software. There *is* an open source
project, Datascript, which emulates Datomic's APIs (without any of the
storage elements). Either one would work for Arachne, since Arachne
only needs the subset of features they both support. In, fact, if
Arachne goes the Datomic-inspired route, we would probably want to
support *both*: Datomic, for those who have an existing investment
there, and Datascript for those who desire open source all the way.

#### Benefits for Arachne

- Well known to most Clojurists
- Highly idiomatic to use from Clojure
- There is no question that it would be performant and technically
  suitable for Arachne-sized data.
- Datomic's schema is a real validating schema; data transacted to
  Datomic must always be valid.
- Datomic Schema is open and extensible.

#### Tradeoffs for Arachne (with mitigations)

- The expressivity of Datomic's schema is anemic compared to RDFs/OWL;
  for example, it has no built-in notion of types. It is focused
  towards data storage and integrity rather than defining a public
  ontology, which would be useful for Arachne.
    - Mitigation: If we did want something more ontologically focused,
      it is possible to build an ontology system on top of Datomic
      using meta-attributes and Datalog rules. Examples of such
      systems already exist.
- If we did build our own ontology system on top of Datomic (or use an
  existing one) we would still be responsible for ""getting it right"",
  ensuring that it meets any potential use case for Arachne while
  maintaining internal and logical consistency.
    - Mitigation: we could still use the work that has been done in
      the OWL world and re-implement a subset of axioms and
      derivations on top of Datomic.
- Any ontological system built on top of Datomic would be novel to
  module authors, and therefore would require careful, extensive
  documentation regarding its capabilities and usage.
- To satisfy users of Datomic as well as those who have a requirement
  for open source, it will be necessary to abstract across both
  Datomic and Datascript.
    - Mitigation: This work is already done (provided users stay
      within the subset of features that is supported by both
      products.)","The steering group decided the RDF/OWL approach is too high-risk to
wrap in Clojure and implement at this time, while the rewards are
mostly intangible ""openness"" and ""interoperability"" rather than
something that will help move Arachne forward in the short term.

Therefore, we will use a Datomic style schema for Arachne's configuration.

Users may use either Datomic Pro, Datomic Free or Datascript at
runtime in their applications. We will provide a ""multiplexer""
configuration implementation that utilizes both, and asserts that the
results are equal: this can be used by module authors to ensure they
stay within the subset of features supported by both platforms.

Before Arachne leaves ""alpha"" status (that is, before it is declared
ready for experimental production use or for the release of
third-party modules), we will revisit the question of whether OWL
would be more appropriate, and whether we have encountered issues that
OWL would have made easier. If so, and if time allows, we reserve the
option to either refactor the configuration layer to use Jena as a
primary store (porting existing modules), or provide an OWL
view/rendering of an ontology stored in Datomic.

","Architectural Decision Record

## Context
[ADR-002](adr-002-configuration.md) indicates that we will store the
entire application config in a single rich data structure with a schema.

### Config as Database

This implies that it should be possible to easily search, query and
update the configuration value. It also implies that the configuration
value is general enough to store arbitrary data; we don't know what
kinds of things users or module authors will need to include.

If what we need is a system that allows you to define, query, and
update arbitrary data with a schema, then we are looking for a
database.

Required data store characteristics:

1. It must be available under a permissive open source
   license. Anything else will impose unwanted restrictions on who can
   use Arachne.
2. It can operate embedded in a JVM process. We do not want to force
   users to install anything else or run multiple processes just to
   get Arachne to work.
3. The database must be serializable. It must be possible to write the
   entire configuration to disk, and then reconstitute it in the same
   exact state in a separate process.
4. Because modules build up the schema progressively, the schema must
   be inherently extensible. It should be possible for modules to
   progressively add both new entity types and new attributes to
   existing entity types.
5. It should be usable from Clojure without a painful impedance mismatch.

### Configuration as Ontology

As an extension of the rationale discussed in
[ADR-002](adr-002-configuration.md), it is useful to enumerate the
possible use cases of the configuration and configuration schema
together.

- The configuration is read by the application during bootstrap and
  controls the behavior of the application.
- The configuration schema defines what types of values the
  application can or will read to modify its structure and behavior at
  boot time and run time.
- The configuration is how an application author communicates their
  intent about how their application should fit together and run, at a
  higher, more conceptual level than code.
- The configuration schema is how module authors communicate to
  application authors what settings, entities and structures
  are available for them to use in their applications.
- The configuration schema is how module authors communicate to other
  potential module authors what their extension points are; module
  extenders can safely read or write any entities/attributes declared
  by the modules upon which they depend.
- The configuration schema can be used to validate a particular
  configuration, and explain where and how it deviates from what is
  actually supported.
- The configuration can be exposed (via user interfaces of various
  types) to end users for analytics and debugging, explaining the
  structure of their application and why things are the way they are.
- A serialization of the configuration, together with a particular
  codebase (identified by a git SHA) form a precise, complete, 100%
  reproducible definition of the behavior of an application.

To the extent that the configuration schema expresses and communicates
the ""categories of being"" or ""possibility space"" of an application, it
is a formal [Ontology](https://en.wikipedia.org/wiki/Ontology). This is
a desirable characteristic, and to the degree that it is practical to
do so, it will be useful to learn from or re-use existing work around
formal ontological systems.


### Implementation Options

There are instances of four broad categories of data stores that match
the first three of the data store characteristics defined above.

- Relational (Derby, HSQLDB, etc)
- Key/value (BerkelyDB, hashtables, etc)
- RDF/RDFs/OWL stores (Jena)
- Datomic-style (Datascript)

We can eliminate relational solutions fairly quickly; SQL schemas are
not generally extensible or flexible, failing condition #4. In
addition, they do not fare well on #5 -- using SQL for queries and updates
is not particularly fluent in Clojure.

Similarly, we can eliminate key/value style data stores. In general,
these do not have schemas at all (or at least, not the type of rich
schema that provides a meaningful data contract or ontology, which is the point
for Arachne.)

This leaves solutions based on the RDF stack, and Datomic-style data
stores. Both are viable options which would provide unique benefits
for Arachne, and both have different drawbacks.

Explaining the core technical characteristics of RDF/OWL and Datomic
is beyond the scope of this document; please see the
[Jena](https://jena.apache.org/documentation/index.html) and
 [Datomic](http://docs.datomic.com) documentation for more
details. More information on RDF, OWL and the Semantic web in general:

- [Wikipedia article on RDF](https://en.wikipedia.org/wiki/Resource_Description_Framework)
- [Wikipedia article on OWL](https://en.wikipedia.org/wiki/Web_Ontology_Language)
- [OWL Semantics](http://www.w3.org/TR/owl-semantics/) standards document.

### RDF

The clear choice for a JVM-based, permissively licensed,
standards-compliant RDF API is Apache Jena.

#### Benefits for Arachne

- OWL is a good fit insofar as Arachne's goal is to define an
  ontology of applications. The point of the configuration schema is
  first and foremost to serve as unambiguous communication regarding
  the types of entities that can exist in an application, and what the
  possible relationships between them are. By definition, this is
  defining an ontology, and is the exact use case which OWL is
  designed to address.
- Information model is a good fit for Clojure: tuples and declarative logic.
- Open and extensible by design.
- Well researched by very smart people, likely to avoid common
  mistakes that would result from building an ontology-like system
  ourselves.
- Existing technology, well known beyond the Clojure
  ecosystem. Existing tools could work with Arachne project
  configurations out of the box.
- The open-world assumption is a good fit for Arachne's per-module
  schema modeling, since modules cannot know what other modules might
  be present in the application.
- We're likely to want to introduce RDFs/OWL to the application
  anyway, at some point, as an abstract entity meta-schema (note: this
  has not been firmly decided yet.)

#### Tradeoffs for Arachne (with mitigations)

- OWL is complex. Learning to use it effectively is a skill in its own
  right and it might be asking a lot to require of module authors.
- OWLs representation of some common concepts can be verbose and/or
  convoluted in ways that would make schema more difficult to
  read/write. (e.g, Restriction classes)
- OWL is not a schema. Although the open world assumption is valid and
  good when writing ontologies, it means that OWL inferencing is
  incapable of performing many of the kind of validations we would
  want to apply once we do have a complete configuration and want to
  check it for correctness. For example, open-world reasoning can
  never validate a `owl:minCardinality` rule.
    - Mitigation: Although OWL inferencing cannot provide closed-world
    validation of a given RDF dataset, such tools do exist. Some
    mechanisms for validating a particular closed set of RDF triples
    include:
       1. Writing SPARQL queries that catch various types of validation errors.
       2. Deriving validation errors using Jena's rules engine.
       3. Using an existing RDF validator such as
      [Eyeball](https://jena.apache.org/documentation/tools/eyeball-getting-started.html)
      (although, unfortunately, Eyeball does not seem to be well
      maintained.)
    - For Clojure, it would be possible to validate a given OWL class
      by generating a specification using `clojure.spec` that could be
      applied to concrete instances of the class in their map form.
- Jena's API is aggressively object oriented and at odds with Clojure
  idioms.
    - Mitigation: Write a data-oriented wrapper (note: I have a
    working proof of concept already.)
- SPARQL is a string-based query language, as opposed to a composable data API.
    - Mitigation: It is possible to hook into Jena's ARQ query engine
      at the object layer, and expose a data-oriented API from there,
      with SPARQL semantics but an API similar to Datomic datalog.
- OWL inferencing is known to have performance issues with complex
  inferences. While Arachne configurations are tiny (as knowledge bases
  go), and we are unlikely to use the more esoteric derivations, it is
  unknown whether this will cause problems with the kinds of
  ontologies we do need.
    - Mitigation: We could restrict ourselves to the OWL DL or even
      OWL Lite sub-languages, which have more tractable inferencing
      rules.
- Jena's APIs are such that it is impossible to write an immutable
  version of a RDF model (at least without breaking most of Jena's
  API.) It's trivial to write a data-oriented wrapper, but intractable
  to write a persistent immutable one.

### Datomic

Note that Datomic itself does not satisfy the first requirement; it is
closed-source, proprietary software. There *is* an open source
project, Datascript, which emulates Datomic's APIs (without any of the
storage elements). Either one would work for Arachne, since Arachne
only needs the subset of features they both support. In, fact, if
Arachne goes the Datomic-inspired route, we would probably want to
support *both*: Datomic, for those who have an existing investment
there, and Datascript for those who desire open source all the way.

#### Benefits for Arachne

- Well known to most Clojurists
- Highly idiomatic to use from Clojure
- There is no question that it would be performant and technically
  suitable for Arachne-sized data.
- Datomic's schema is a real validating schema; data transacted to
  Datomic must always be valid.
- Datomic Schema is open and extensible.

#### Tradeoffs for Arachne (with mitigations)

- The expressivity of Datomic's schema is anemic compared to RDFs/OWL;
  for example, it has no built-in notion of types. It is focused
  towards data storage and integrity rather than defining a public
  ontology, which would be useful for Arachne.
    - Mitigation: If we did want something more ontologically focused,
      it is possible to build an ontology system on top of Datomic
      using meta-attributes and Datalog rules. Examples of such
      systems already exist.
- If we did build our own ontology system on top of Datomic (or use an
  existing one) we would still be responsible for ""getting it right"",
  ensuring that it meets any potential use case for Arachne while
  maintaining internal and logical consistency.
    - Mitigation: we could still use the work that has been done in
      the OWL world and re-implement a subset of axioms and
      derivations on top of Datomic.
- Any ontological system built on top of Datomic would be novel to
  module authors, and therefore would require careful, extensive
  documentation regarding its capabilities and usage.
- To satisfy users of Datomic as well as those who have a requirement
  for open source, it will be necessary to abstract across both
  Datomic and Datascript.
    - Mitigation: This work is already done (provided users stay
      within the subset of features that is supported by both
      products.)

## Decision
","## Context

[ADR-002](adr-002-configuration.md) indicates that we will store the
entire application config in a single rich data structure with a schema.

### Config as Database

This implies that it should be possible to easily search, query and
update the configuration value. It also implies that the configuration
value is general enough to store arbitrary data; we don't know what
kinds of things users or module authors will need to include.

If what we need is a system that allows you to define, query, and
update arbitrary data with a schema, then we are looking for a
database.

Required data store characteristics:

1. It must be available under a permissive open source
   license. Anything else will impose unwanted restrictions on who can
   use Arachne.
2. It can operate embedded in a JVM process. We do not want to force
   users to install anything else or run multiple processes just to
   get Arachne to work.
3. The database must be serializable. It must be possible to write the
   entire configuration to disk, and then reconstitute it in the same
   exact state in a separate process.
4. Because modules build up the schema progressively, the schema must
   be inherently extensible. It should be possible for modules to
   progressively add both new entity types and new attributes to
   existing entity types.
5. It should be usable from Clojure without a painful impedance mismatch.

### Configuration as Ontology

As an extension of the rationale discussed in
[ADR-002](adr-002-configuration.md), it is useful to enumerate the
possible use cases of the configuration and configuration schema
together.

- The configuration is read by the application during bootstrap and
  controls the behavior of the application.
- The configuration schema defines what types of values the
  application can or will read to modify its structure and behavior at
  boot time and run time.
- The configuration is how an application author communicates their
  intent about how their application should fit together and run, at a
  higher, more conceptual level than code.
- The configuration schema is how module authors communicate to
  application authors what settings, entities and structures
  are available for them to use in their applications.
- The configuration schema is how module authors communicate to other
  potential module authors what their extension points are; module
  extenders can safely read or write any entities/attributes declared
  by the modules upon which they depend.
- The configuration schema can be used to validate a particular
  configuration, and explain where and how it deviates from what is
  actually supported.
- The configuration can be exposed (via user interfaces of various
  types) to end users for analytics and debugging, explaining the
  structure of their application and why things are the way they are.
- A serialization of the configuration, together with a particular
  codebase (identified by a git SHA) form a precise, complete, 100%
  reproducible definition of the behavior of an application.

To the extent that the configuration schema expresses and communicates
the ""categories of being"" or ""possibility space"" of an application, it
is a formal [Ontology](https://en.wikipedia.org/wiki/Ontology). This is
a desirable characteristic, and to the degree that it is practical to
do so, it will be useful to learn from or re-use existing work around
formal ontological systems.


### Implementation Options

There are instances of four broad categories of data stores that match
the first three of the data store characteristics defined above.

- Relational (Derby, HSQLDB, etc)
- Key/value (BerkelyDB, hashtables, etc)
- RDF/RDFs/OWL stores (Jena)
- Datomic-style (Datascript)

We can eliminate relational solutions fairly quickly; SQL schemas are
not generally extensible or flexible, failing condition #4. In
addition, they do not fare well on #5 -- using SQL for queries and updates
is not particularly fluent in Clojure.

Similarly, we can eliminate key/value style data stores. In general,
these do not have schemas at all (or at least, not the type of rich
schema that provides a meaningful data contract or ontology, which is the point
for Arachne.)

This leaves solutions based on the RDF stack, and Datomic-style data
stores. Both are viable options which would provide unique benefits
for Arachne, and both have different drawbacks.

Explaining the core technical characteristics of RDF/OWL and Datomic
is beyond the scope of this document; please see the
[Jena](https://jena.apache.org/documentation/index.html) and
 [Datomic](http://docs.datomic.com) documentation for more
details. More information on RDF, OWL and the Semantic web in general:

- [Wikipedia article on RDF](https://en.wikipedia.org/wiki/Resource_Description_Framework)
- [Wikipedia article on OWL](https://en.wikipedia.org/wiki/Web_Ontology_Language)
- [OWL Semantics](http://www.w3.org/TR/owl-semantics/) standards document.

### RDF

The clear choice for a JVM-based, permissively licensed,
standards-compliant RDF API is Apache Jena.

#### Benefits for Arachne

- OWL is a good fit insofar as Arachne's goal is to define an
  ontology of applications. The point of the configuration schema is
  first and foremost to serve as unambiguous communication regarding
  the types of entities that can exist in an application, and what the
  possible relationships between them are. By definition, this is
  defining an ontology, and is the exact use case which OWL is
  designed to address.
- Information model is a good fit for Clojure: tuples and declarative logic.
- Open and extensible by design.
- Well researched by very smart people, likely to avoid common
  mistakes that would result from building an ontology-like system
  ourselves.
- Existing technology, well known beyond the Clojure
  ecosystem. Existing tools could work with Arachne project
  configurations out of the box.
- The open-world assumption is a good fit for Arachne's per-module
  schema modeling, since modules cannot know what other modules might
  be present in the application.
- We're likely to want to introduce RDFs/OWL to the application
  anyway, at some point, as an abstract entity meta-schema (note: this
  has not been firmly decided yet.)

#### Tradeoffs for Arachne (with mitigations)

- OWL is complex. Learning to use it effectively is a skill in its own
  right and it might be asking a lot to require of module authors.
- OWLs representation of some common concepts can be verbose and/or
  convoluted in ways that would make schema more difficult to
  read/write. (e.g, Restriction classes)
- OWL is not a schema. Although the open world assumption is valid and
  good when writing ontologies, it means that OWL inferencing is
  incapable of performing many of the kind of validations we would
  want to apply once we do have a complete configuration and want to
  check it for correctness. For example, open-world reasoning can
  never validate a `owl:minCardinality` rule.
    - Mitigation: Although OWL inferencing cannot provide closed-world
    validation of a given RDF dataset, such tools do exist. Some
    mechanisms for validating a particular closed set of RDF triples
    include:
       1. Writing SPARQL queries that catch various types of validation errors.
       2. Deriving validation errors using Jena's rules engine.
       3. Using an existing RDF validator such as
      [Eyeball](https://jena.apache.org/documentation/tools/eyeball-getting-started.html)
      (although, unfortunately, Eyeball does not seem to be well
      maintained.)
    - For Clojure, it would be possible to validate a given OWL class
      by generating a specification using `clojure.spec` that could be
      applied to concrete instances of the class in their map form.
- Jena's API is aggressively object oriented and at odds with Clojure
  idioms.
    - Mitigation: Write a data-oriented wrapper (note: I have a
    working proof of concept already.)
- SPARQL is a string-based query language, as opposed to a composable data API.
    - Mitigation: It is possible to hook into Jena's ARQ query engine
      at the object layer, and expose a data-oriented API from there,
      with SPARQL semantics but an API similar to Datomic datalog.
- OWL inferencing is known to have performance issues with complex
  inferences. While Arachne configurations are tiny (as knowledge bases
  go), and we are unlikely to use the more esoteric derivations, it is
  unknown whether this will cause problems with the kinds of
  ontologies we do need.
    - Mitigation: We could restrict ourselves to the OWL DL or even
      OWL Lite sub-languages, which have more tractable inferencing
      rules.
- Jena's APIs are such that it is impossible to write an immutable
  version of a RDF model (at least without breaking most of Jena's
  API.) It's trivial to write a data-oriented wrapper, but intractable
  to write a persistent immutable one.

### Datomic

Note that Datomic itself does not satisfy the first requirement; it is
closed-source, proprietary software. There *is* an open source
project, Datascript, which emulates Datomic's APIs (without any of the
storage elements). Either one would work for Arachne, since Arachne
only needs the subset of features they both support. In, fact, if
Arachne goes the Datomic-inspired route, we would probably want to
support *both*: Datomic, for those who have an existing investment
there, and Datascript for those who desire open source all the way.

#### Benefits for Arachne

- Well known to most Clojurists
- Highly idiomatic to use from Clojure
- There is no question that it would be performant and technically
  suitable for Arachne-sized data.
- Datomic's schema is a real validating schema; data transacted to
  Datomic must always be valid.
- Datomic Schema is open and extensible.

#### Tradeoffs for Arachne (with mitigations)

- The expressivity of Datomic's schema is anemic compared to RDFs/OWL;
  for example, it has no built-in notion of types. It is focused
  towards data storage and integrity rather than defining a public
  ontology, which would be useful for Arachne.
    - Mitigation: If we did want something more ontologically focused,
      it is possible to build an ontology system on top of Datomic
      using meta-attributes and Datalog rules. Examples of such
      systems already exist.
- If we did build our own ontology system on top of Datomic (or use an
  existing one) we would still be responsible for ""getting it right"",
  ensuring that it meets any potential use case for Arachne while
  maintaining internal and logical consistency.
    - Mitigation: we could still use the work that has been done in
      the OWL world and re-implement a subset of axioms and
      derivations on top of Datomic.
- Any ontological system built on top of Datomic would be novel to
  module authors, and therefore would require careful, extensive
  documentation regarding its capabilities and usage.
- To satisfy users of Datomic as well as those who have a requirement
  for open source, it will be necessary to abstract across both
  Datomic and Datascript.
    - Mitigation: This work is already done (provided users stay
      within the subset of features that is supported by both
      products.)","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
[ADR-002](adr-002-configuration.md) indicates that we will store the
entire application config in a single rich data structure with a schema.

### Config as Database

This implies that it should be possible to easily search, query and
update the configuration value. It also implies that the configuration
value is general enough to store arbitrary data; we don't know what
kinds of things users or module authors will need to include.

If what we need is a system that allows you to define, query, and
update arbitrary data with a schema, then we are looking for a
database.

Required data store characteristics:

1. It must be available under a permissive open source
   license. Anything else will impose unwanted restrictions on who can
   use Arachne.
2. It can operate embedded in a JVM process. We do not want to force
   users to install anything else or run multiple processes just to
   get Arachne to work.
3. The database must be serializable. It must be possible to write the
   entire configuration to disk, and then reconstitute it in the same
   exact state in a separate process.
4. Because modules build up the schema progressively, the schema must
   be inherently extensible. It should be possible for modules to
   progressively add both new entity types and new attributes to
   existing entity types.
5. It should be usable from Clojure without a painful impedance mismatch.

### Configuration as Ontology

As an extension of the rationale discussed in
[ADR-002](adr-002-configuration.md), it is useful to enumerate the
possible use cases of the configuration and configuration schema
together.

- The configuration is read by the application during bootstrap and
  controls the behavior of the application.
- The configuration schema defines what types of values the
  application can or will read to modify its structure and behavior at
  boot time and run time.
- The configuration is how an application author communicates their
  intent about how their application should fit together and run, at a
  higher, more conceptual level than code.
- The configuration schema is how module authors communicate to
  application authors what settings, entities and structures
  are available for them to use in their applications.
- The configuration schema is how module authors communicate to other
  potential module authors what their extension points are; module
  extenders can safely read or write any entities/attributes declared
  by the modules upon which they depend.
- The configuration schema can be used to validate a particular
  configuration, and explain where and how it deviates from what is
  actually supported.
- The configuration can be exposed (via user interfaces of various
  types) to end users for analytics and debugging, explaining the
  structure of their application and why things are the way they are.
- A serialization of the configuration, together with a particular
  codebase (identified by a git SHA) form a precise, complete, 100%
  reproducible definition of the behavior of an application.

To the extent that the configuration schema expresses and communicates
the ""categories of being"" or ""possibility space"" of an application, it
is a formal [Ontology](https://en.wikipedia.org/wiki/Ontology). This is
a desirable characteristic, and to the degree that it is practical to
do so, it will be useful to learn from or re-use existing work around
formal ontological systems.


### Implementation Options

There are instances of four broad categories of data stores that match
the first three of the data store characteristics defined above.

- Relational (Derby, HSQLDB, etc)
- Key/value (BerkelyDB, hashtables, etc)
- RDF/RDFs/OWL stores (Jena)
- Datomic-style (Datascript)

We can eliminate relational solutions fairly quickly; SQL schemas are
not generally extensible or flexible, failing condition #4. In
addition, they do not fare well on #5 -- using SQL for queries and updates
is not particularly fluent in Clojure.

Similarly, we can eliminate key/value style data stores. In general,
these do not have schemas at all (or at least, not the type of rich
schema that provides a meaningful data contract or ontology, which is the point
for Arachne.)

This leaves solutions based on the RDF stack, and Datomic-style data
stores. Both are viable options which would provide unique benefits
for Arachne, and both have different drawbacks.

Explaining the core technical characteristics of RDF/OWL and Datomic
is beyond the scope of this document; please see the
[Jena](https://jena.apache.org/documentation/index.html) and
 [Datomic](http://docs.datomic.com) documentation for more
details. More information on RDF, OWL and the Semantic web in general:

- [Wikipedia article on RDF](https://en.wikipedia.org/wiki/Resource_Description_Framework)
- [Wikipedia article on OWL](https://en.wikipedia.org/wiki/Web_Ontology_Language)
- [OWL Semantics](http://www.w3.org/TR/owl-semantics/) standards document.

### RDF

The clear choice for a JVM-based, permissively licensed,
standards-compliant RDF API is Apache Jena.

#### Benefits for Arachne

- OWL is a good fit insofar as Arachne's goal is to define an
  ontology of applications. The point of the configuration schema is
  first and foremost to serve as unambiguous communication regarding
  the types of entities that can exist in an application, and what the
  possible relationships between them are. By definition, this is
  defining an ontology, and is the exact use case which OWL is
  designed to address.
- Information model is a good fit for Clojure: tuples and declarative logic.
- Open and extensible by design.
- Well researched by very smart people, likely to avoid common
  mistakes that would result from building an ontology-like system
  ourselves.
- Existing technology, well known beyond the Clojure
  ecosystem. Existing tools could work with Arachne project
  configurations out of the box.
- The open-world assumption is a good fit for Arachne's per-module
  schema modeling, since modules cannot know what other modules might
  be present in the application.
- We're likely to want to introduce RDFs/OWL to the application
  anyway, at some point, as an abstract entity meta-schema (note: this
  has not been firmly decided yet.)

#### Tradeoffs for Arachne (with mitigations)

- OWL is complex. Learning to use it effectively is a skill in its own
  right and it might be asking a lot to require of module authors.
- OWLs representation of some common concepts can be verbose and/or
  convoluted in ways that would make schema more difficult to
  read/write. (e.g, Restriction classes)
- OWL is not a schema. Although the open world assumption is valid and
  good when writing ontologies, it means that OWL inferencing is
  incapable of performing many of the kind of validations we would
  want to apply once we do have a complete configuration and want to
  check it for correctness. For example, open-world reasoning can
  never validate a `owl:minCardinality` rule.
    - Mitigation: Although OWL inferencing cannot provide closed-world
    validation of a given RDF dataset, such tools do exist. Some
    mechanisms for validating a particular closed set of RDF triples
    include:
       1. Writing SPARQL queries that catch various types of validation errors.
       2. Deriving validation errors using Jena's rules engine.
       3. Using an existing RDF validator such as
      [Eyeball](https://jena.apache.org/documentation/tools/eyeball-getting-started.html)
      (although, unfortunately, Eyeball does not seem to be well
      maintained.)
    - For Clojure, it would be possible to validate a given OWL class
      by generating a specification using `clojure.spec` that could be
      applied to concrete instances of the class in their map form.
- Jena's API is aggressively object oriented and at odds with Clojure
  idioms.
    - Mitigation: Write a data-oriented wrapper (note: I have a
    working proof of concept already.)
- SPARQL is a string-based query language, as opposed to a composable data API.
    - Mitigation: It is possible to hook into Jena's ARQ query engine
      at the object layer, and expose a data-oriented API from there,
      with SPARQL semantics but an API similar to Datomic datalog.
- OWL inferencing is known to have performance issues with complex
  inferences. While Arachne configurations are tiny (as knowledge bases
  go), and we are unlikely to use the more esoteric derivations, it is
  unknown whether this will cause problems with the kinds of
  ontologies we do need.
    - Mitigation: We could restrict ourselves to the OWL DL or even
      OWL Lite sub-languages, which have more tractable inferencing
      rules.
- Jena's APIs are such that it is impossible to write an immutable
  version of a RDF model (at least without breaking most of Jena's
  API.) It's trivial to write a data-oriented wrapper, but intractable
  to write a persistent immutable one.

### Datomic

Note that Datomic itself does not satisfy the first requirement; it is
closed-source, proprietary software. There *is* an open source
project, Datascript, which emulates Datomic's APIs (without any of the
storage elements). Either one would work for Arachne, since Arachne
only needs the subset of features they both support. In, fact, if
Arachne goes the Datomic-inspired route, we would probably want to
support *both*: Datomic, for those who have an existing investment
there, and Datascript for those who desire open source all the way.

#### Benefits for Arachne

- Well known to most Clojurists
- Highly idiomatic to use from Clojure
- There is no question that it would be performant and technically
  suitable for Arachne-sized data.
- Datomic's schema is a real validating schema; data transacted to
  Datomic must always be valid.
- Datomic Schema is open and extensible.

#### Tradeoffs for Arachne (with mitigations)

- The expressivity of Datomic's schema is anemic compared to RDFs/OWL;
  for example, it has no built-in notion of types. It is focused
  towards data storage and integrity rather than defining a public
  ontology, which would be useful for Arachne.
    - Mitigation: If we did want something more ontologically focused,
      it is possible to build an ontology system on top of Datomic
      using meta-attributes and Datalog rules. Examples of such
      systems already exist.
- If we did build our own ontology system on top of Datomic (or use an
  existing one) we would still be responsible for ""getting it right"",
  ensuring that it meets any potential use case for Arachne while
  maintaining internal and logical consistency.
    - Mitigation: we could still use the work that has been done in
      the OWL world and re-implement a subset of axioms and
      derivations on top of Datomic.
- Any ontological system built on top of Datomic would be novel to
  module authors, and therefore would require careful, extensive
  documentation regarding its capabilities and usage.
- To satisfy users of Datomic as well as those who have a requirement
  for open source, it will be necessary to abstract across both
  Datomic and Datascript.
    - Mitigation: This work is already done (provided users stay
      within the subset of features that is supported by both
      products.)"
"In addition to handling arbitrary HTTP requests, we would like for Arachne to make it easy to serve up certain types of well-known resources, such as static HTML, images, CSS, and JavaScript.

These ""static assets"" can generally be served to users as files directly, without processing at the time they are served. However, it is extremely useful to provide *pre-processing*, to convert assets in one format to another format prior to serving them. Examples of such transformations include:

- SCSS/LESS to CSS
- CoffeeScript to JavaScript
- ClojureScript to JavaScript
- Full-size images to thumbnails
- Compress files using gzip

Additionally, in some cases, several such transformations might be required, on the same resource. For example, a file might need to be converted from CoffeeScript to JavaScript, then minified, then gzipped.

In this case, asset transformations form a logical pipeline, applying a set of transformations in a known order to resources that meet certain criteria.

Arachne needs a module that defines a way to specify what assets are, and what transformations ought to apply and in what order. Like everything else, this system needs to be open to extension by other modules, to provide custom processing steps.

### Development vs Production

Regardless of how the asset pipeline is implemented, it must provide a good development experience such that the developer can see their changes immediately. When the user modifies an asset file, it should be automatically reflected in the running application in near realtime. This keeps development cycle times low, and provides a fluid, low-friction development experience that allows developers to focus on their application.

Production usage, however, has a different set of priorities. Being able to reflect changes is less important; instead, minimizing processing cost and response time is paramount. In production, systems will generally want to do as much processing as they can ahead of time (during or before deployment), and then cache aggressively.

### Deployment & Distribution

For development and simple deployments, Arachne should be capable of serving assets itself. However, whatever technique it uses to implement the asset pipeline, it should also be capable of sending the final assets to a separate cache or CDN such that they can be served statically with optimal efficiency. This may be implemented as a separate module from the core asset pipeline, however.

### Entirely Static Sites

There is a large class of websites which actually do not require any dynamic behavior at all; they can be built entirely from static assets (and associated pre-processing.) Examples of frameworks that cater specifically to this type of ""static site generation"" include Jekyll, Middleman, Brunch, and many more.

By including the asset pipeline module, and *not* the HTTP or Pedestal modules, Arachne also ought to be able to function as a capable and extensible static site generator.","Arachne will use Boot to provide an abstract asset pipeline. Boot has built-in support for immutable Filesets, temp directory management, and file watchers.

As with everything in Arachne, the pipeline will be specified as pure data in the configuration, specifying inputs, outputs, and transformations explicitly.

Modules that participate in the asset pipeline will develop against a well-defined API built around Boot Filesets.

","Architectural Decision Record

## Context
In addition to handling arbitrary HTTP requests, we would like for Arachne to make it easy to serve up certain types of well-known resources, such as static HTML, images, CSS, and JavaScript.

These ""static assets"" can generally be served to users as files directly, without processing at the time they are served. However, it is extremely useful to provide *pre-processing*, to convert assets in one format to another format prior to serving them. Examples of such transformations include:

- SCSS/LESS to CSS
- CoffeeScript to JavaScript
- ClojureScript to JavaScript
- Full-size images to thumbnails
- Compress files using gzip

Additionally, in some cases, several such transformations might be required, on the same resource. For example, a file might need to be converted from CoffeeScript to JavaScript, then minified, then gzipped.

In this case, asset transformations form a logical pipeline, applying a set of transformations in a known order to resources that meet certain criteria.

Arachne needs a module that defines a way to specify what assets are, and what transformations ought to apply and in what order. Like everything else, this system needs to be open to extension by other modules, to provide custom processing steps.

### Development vs Production

Regardless of how the asset pipeline is implemented, it must provide a good development experience such that the developer can see their changes immediately. When the user modifies an asset file, it should be automatically reflected in the running application in near realtime. This keeps development cycle times low, and provides a fluid, low-friction development experience that allows developers to focus on their application.

Production usage, however, has a different set of priorities. Being able to reflect changes is less important; instead, minimizing processing cost and response time is paramount. In production, systems will generally want to do as much processing as they can ahead of time (during or before deployment), and then cache aggressively.

### Deployment & Distribution

For development and simple deployments, Arachne should be capable of serving assets itself. However, whatever technique it uses to implement the asset pipeline, it should also be capable of sending the final assets to a separate cache or CDN such that they can be served statically with optimal efficiency. This may be implemented as a separate module from the core asset pipeline, however.

### Entirely Static Sites

There is a large class of websites which actually do not require any dynamic behavior at all; they can be built entirely from static assets (and associated pre-processing.) Examples of frameworks that cater specifically to this type of ""static site generation"" include Jekyll, Middleman, Brunch, and many more.

By including the asset pipeline module, and *not* the HTTP or Pedestal modules, Arachne also ought to be able to function as a capable and extensible static site generator.

## Decision
","## Context

In addition to handling arbitrary HTTP requests, we would like for Arachne to make it easy to serve up certain types of well-known resources, such as static HTML, images, CSS, and JavaScript.

These ""static assets"" can generally be served to users as files directly, without processing at the time they are served. However, it is extremely useful to provide *pre-processing*, to convert assets in one format to another format prior to serving them. Examples of such transformations include:

- SCSS/LESS to CSS
- CoffeeScript to JavaScript
- ClojureScript to JavaScript
- Full-size images to thumbnails
- Compress files using gzip

Additionally, in some cases, several such transformations might be required, on the same resource. For example, a file might need to be converted from CoffeeScript to JavaScript, then minified, then gzipped.

In this case, asset transformations form a logical pipeline, applying a set of transformations in a known order to resources that meet certain criteria.

Arachne needs a module that defines a way to specify what assets are, and what transformations ought to apply and in what order. Like everything else, this system needs to be open to extension by other modules, to provide custom processing steps.

### Development vs Production

Regardless of how the asset pipeline is implemented, it must provide a good development experience such that the developer can see their changes immediately. When the user modifies an asset file, it should be automatically reflected in the running application in near realtime. This keeps development cycle times low, and provides a fluid, low-friction development experience that allows developers to focus on their application.

Production usage, however, has a different set of priorities. Being able to reflect changes is less important; instead, minimizing processing cost and response time is paramount. In production, systems will generally want to do as much processing as they can ahead of time (during or before deployment), and then cache aggressively.

### Deployment & Distribution

For development and simple deployments, Arachne should be capable of serving assets itself. However, whatever technique it uses to implement the asset pipeline, it should also be capable of sending the final assets to a separate cache or CDN such that they can be served statically with optimal efficiency. This may be implemented as a separate module from the core asset pipeline, however.

### Entirely Static Sites

There is a large class of websites which actually do not require any dynamic behavior at all; they can be built entirely from static assets (and associated pre-processing.) Examples of frameworks that cater specifically to this type of ""static site generation"" include Jekyll, Middleman, Brunch, and many more.

By including the asset pipeline module, and *not* the HTTP or Pedestal modules, Arachne also ought to be able to function as a capable and extensible static site generator.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
In addition to handling arbitrary HTTP requests, we would like for Arachne to make it easy to serve up certain types of well-known resources, such as static HTML, images, CSS, and JavaScript.

These ""static assets"" can generally be served to users as files directly, without processing at the time they are served. However, it is extremely useful to provide *pre-processing*, to convert assets in one format to another format prior to serving them. Examples of such transformations include:

- SCSS/LESS to CSS
- CoffeeScript to JavaScript
- ClojureScript to JavaScript
- Full-size images to thumbnails
- Compress files using gzip

Additionally, in some cases, several such transformations might be required, on the same resource. For example, a file might need to be converted from CoffeeScript to JavaScript, then minified, then gzipped.

In this case, asset transformations form a logical pipeline, applying a set of transformations in a known order to resources that meet certain criteria.

Arachne needs a module that defines a way to specify what assets are, and what transformations ought to apply and in what order. Like everything else, this system needs to be open to extension by other modules, to provide custom processing steps.

### Development vs Production

Regardless of how the asset pipeline is implemented, it must provide a good development experience such that the developer can see their changes immediately. When the user modifies an asset file, it should be automatically reflected in the running application in near realtime. This keeps development cycle times low, and provides a fluid, low-friction development experience that allows developers to focus on their application.

Production usage, however, has a different set of priorities. Being able to reflect changes is less important; instead, minimizing processing cost and response time is paramount. In production, systems will generally want to do as much processing as they can ahead of time (during or before deployment), and then cache aggressively.

### Deployment & Distribution

For development and simple deployments, Arachne should be capable of serving assets itself. However, whatever technique it uses to implement the asset pipeline, it should also be capable of sending the final assets to a separate cache or CDN such that they can be served statically with optimal efficiency. This may be implemented as a separate module from the core asset pipeline, however.

### Entirely Static Sites

There is a large class of websites which actually do not require any dynamic behavior at all; they can be built entirely from static assets (and associated pre-processing.) Examples of frameworks that cater specifically to this type of ""static site generation"" include Jekyll, Middleman, Brunch, and many more.

By including the asset pipeline module, and *not* the HTTP or Pedestal modules, Arachne also ought to be able to function as a capable and extensible static site generator."
"Historically, error handling has not been Clojure's strong suit. For the most part, errors take the form of a JVM exception, with a long stack trace that includes a lot of Clojure's implementation as well as stack frames that pertain directly to user code.

Additionally, prior to the advent of `clojure.spec`, Clojure errors were often ""deep"": a very generic error (like a NullPointerException) would be thrown from far within a branch, rather than eagerly validating inputs.

There are Clojure libraries which make an attempt to improve the situation, but they typically do it by overriding Clojure's default exception printing functions across the board, and are sometimes ""lossy"", dropping information that could be desirable to a developer.

Spec provides an opportunity to improve the situation across the board, and with Arachne we want to be on the leading edge of providing helpful error messages that point straight to the problem, minimize time spent trying to figure out what's going on, and let developers get straight back to working on what matters to them.

Ideally, Arachne's error handling should exhibit the following qualities:

- Never hide possibly relevant information.
- Allow module developers to be as helpful as possible to people using their tools.
- Provide rich, colorful, multi-line detailed explanations of what went wrong (when applicable.)
- Be compatible with existing Clojure error-handling practices for errors thrown from libraries that Arachne doesn't control.
- Not violate expectations of experienced Clojure programmers.
- Be robust enough not to cause additional problems.
- Not break existing logging tools for production use.","We will separate the problems of creating rich exceptions, and catching them and displaying them to the user.

### Creating Errors

Whenever a well-behaved Arachne module needs to report an error, it should throw an info-bearing exception. This exception should be formed such that it is handled gracefully by any JVM tooling; the message should be terse but communicative, containing key information with no newlines.

However, in the `ex-data`, the exception will also contain much more detailed information, that can be used (in the correct context) to provide much more detailed or verbose errors. Specifically, it may contain the following keys:

- `:arachne.error/message` - The short-form error message (the same as the Exception message.)
- `:arachne.error/explanation` - a long-form error message, complete with newlines and formatting.
- `:arachne.error/suggestions` - Zero or more suggestions on how the error might be fixed.
- `:arachne.error/type` - a namespaced keyword that uniquely identifies the type of error.
- `:arachne.error/spec` - The spec that failed (if applicable)
- `:arachne.error/failed-data` - The data that failed to match the spec (if applicable)
- `:arachne.error/explain-data` - An explain-data for the spec that failed (if applicable).
- `:arachne.error/env` - A map of the locals in the env at the time the error was thrown.

Exceptions may, of course, contain additional data; these are the common keys that tools can use to more effectively render errors.

There will be a suite of tools, provided with Arachne's core, for conveniently generating errors that match this pattern.

### Displaying Errors

We will use a pluggable ""error handling system"", where users can explicitly install an exception handler other than the default.

If the user does not install any exception handlers, errors will be handled the same way as they are by default (usually, dumped with the message and stack trace to  `System/err`.) This will not change.

However, Arachne will also provide a function that a user can invoke in their main process, prior to doing anything else. Invoking this function will install a set of default exception handlers that will handle errors in a richer, more Arachne-specific way. This includes printing out the long-form error, or even (eventually) popping open a graphical data browser/debugger (if applicable.)

","Architectural Decision Record

## Context
Historically, error handling has not been Clojure's strong suit. For the most part, errors take the form of a JVM exception, with a long stack trace that includes a lot of Clojure's implementation as well as stack frames that pertain directly to user code.

Additionally, prior to the advent of `clojure.spec`, Clojure errors were often ""deep"": a very generic error (like a NullPointerException) would be thrown from far within a branch, rather than eagerly validating inputs.

There are Clojure libraries which make an attempt to improve the situation, but they typically do it by overriding Clojure's default exception printing functions across the board, and are sometimes ""lossy"", dropping information that could be desirable to a developer.

Spec provides an opportunity to improve the situation across the board, and with Arachne we want to be on the leading edge of providing helpful error messages that point straight to the problem, minimize time spent trying to figure out what's going on, and let developers get straight back to working on what matters to them.

Ideally, Arachne's error handling should exhibit the following qualities:

- Never hide possibly relevant information.
- Allow module developers to be as helpful as possible to people using their tools.
- Provide rich, colorful, multi-line detailed explanations of what went wrong (when applicable.)
- Be compatible with existing Clojure error-handling practices for errors thrown from libraries that Arachne doesn't control.
- Not violate expectations of experienced Clojure programmers.
- Be robust enough not to cause additional problems.
- Not break existing logging tools for production use.

## Decision
","## Context

Historically, error handling has not been Clojure's strong suit. For the most part, errors take the form of a JVM exception, with a long stack trace that includes a lot of Clojure's implementation as well as stack frames that pertain directly to user code.

Additionally, prior to the advent of `clojure.spec`, Clojure errors were often ""deep"": a very generic error (like a NullPointerException) would be thrown from far within a branch, rather than eagerly validating inputs.

There are Clojure libraries which make an attempt to improve the situation, but they typically do it by overriding Clojure's default exception printing functions across the board, and are sometimes ""lossy"", dropping information that could be desirable to a developer.

Spec provides an opportunity to improve the situation across the board, and with Arachne we want to be on the leading edge of providing helpful error messages that point straight to the problem, minimize time spent trying to figure out what's going on, and let developers get straight back to working on what matters to them.

Ideally, Arachne's error handling should exhibit the following qualities:

- Never hide possibly relevant information.
- Allow module developers to be as helpful as possible to people using their tools.
- Provide rich, colorful, multi-line detailed explanations of what went wrong (when applicable.)
- Be compatible with existing Clojure error-handling practices for errors thrown from libraries that Arachne doesn't control.
- Not violate expectations of experienced Clojure programmers.
- Be robust enough not to cause additional problems.
- Not break existing logging tools for production use.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Historically, error handling has not been Clojure's strong suit. For the most part, errors take the form of a JVM exception, with a long stack trace that includes a lot of Clojure's implementation as well as stack frames that pertain directly to user code.

Additionally, prior to the advent of `clojure.spec`, Clojure errors were often ""deep"": a very generic error (like a NullPointerException) would be thrown from far within a branch, rather than eagerly validating inputs.

There are Clojure libraries which make an attempt to improve the situation, but they typically do it by overriding Clojure's default exception printing functions across the board, and are sometimes ""lossy"", dropping information that could be desirable to a developer.

Spec provides an opportunity to improve the situation across the board, and with Arachne we want to be on the leading edge of providing helpful error messages that point straight to the problem, minimize time spent trying to figure out what's going on, and let developers get straight back to working on what matters to them.

Ideally, Arachne's error handling should exhibit the following qualities:

- Never hide possibly relevant information.
- Allow module developers to be as helpful as possible to people using their tools.
- Provide rich, colorful, multi-line detailed explanations of what went wrong (when applicable.)
- Be compatible with existing Clojure error-handling practices for errors thrown from libraries that Arachne doesn't control.
- Not violate expectations of experienced Clojure programmers.
- Be robust enough not to cause additional problems.
- Not break existing logging tools for production use."
"Arachne has several very explicit goals that make the practice and
discipline of architecture very important:

- We want to think deeply about all our architectural decisions,
  exploring all alternatives and making a careful, considered,
  well-researched choice.
- We want to be as transparent as possible in our decision-making
  process.
- We don't want decisions to be made unilaterally in a
  vacuum. Specifically, we want to give our steering group the
  opportunity to review every major decision.
- Despite being a geographically and temporally distributed team, we
  want our contributors to have a strong shared understanding of the
  technical rationale behind decisions.
- We want to be able to revisit prior decisions to determine fairly if
  they still make sense, and if the motivating circumstances or
  conditions have changed.","We will document every architecture-level decision for Arachne and its
core modules with an
[Architecture Decision Record](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions). These
are a well structured, relatively lightweight way to capture
architectural proposals. They can serve as an artifact for discussion,
and remain as an enduring record of the context and motivation of past
decisions.

The workflow will be:

1. A developer creates an ADR document outlining an approach for a
   particular question or problem. The ADR has an initial status of ""proposed.""
2. The developers and steering group discuss the ADR. During this
   period, the ADR should be updated to reflect additional context,
   concerns raised, and proposed changes.
3. Once consensus is reached, ADR can be transitioned to either an
   ""accepted"" or ""rejected"" state.
4. Only after an ADR is accepted should implementing code be committed
   to the master branch of the relevant project/module.
5. If a decision is revisited and a different conclusion is reached, a
   new ADR should be created documenting the context and rationale for
   the change. The new ADR should reference the old one, and once the
   new one is accepted, the old one should (in its ""status"" section)
   be updated to point to the new one. The old ADR should not be
   removed or otherwise modified except for the annotation pointing to
   the new ADR.

","Architectural Decision Record

## Context
Arachne has several very explicit goals that make the practice and
discipline of architecture very important:

- We want to think deeply about all our architectural decisions,
  exploring all alternatives and making a careful, considered,
  well-researched choice.
- We want to be as transparent as possible in our decision-making
  process.
- We don't want decisions to be made unilaterally in a
  vacuum. Specifically, we want to give our steering group the
  opportunity to review every major decision.
- Despite being a geographically and temporally distributed team, we
  want our contributors to have a strong shared understanding of the
  technical rationale behind decisions.
- We want to be able to revisit prior decisions to determine fairly if
  they still make sense, and if the motivating circumstances or
  conditions have changed.

## Decision
","## Context

Arachne has several very explicit goals that make the practice and
discipline of architecture very important:

- We want to think deeply about all our architectural decisions,
  exploring all alternatives and making a careful, considered,
  well-researched choice.
- We want to be as transparent as possible in our decision-making
  process.
- We don't want decisions to be made unilaterally in a
  vacuum. Specifically, we want to give our steering group the
  opportunity to review every major decision.
- Despite being a geographically and temporally distributed team, we
  want our contributors to have a strong shared understanding of the
  technical rationale behind decisions.
- We want to be able to revisit prior decisions to determine fairly if
  they still make sense, and if the motivating circumstances or
  conditions have changed.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Arachne has several very explicit goals that make the practice and
discipline of architecture very important:

- We want to think deeply about all our architectural decisions,
  exploring all alternatives and making a careful, considered,
  well-researched choice.
- We want to be as transparent as possible in our decision-making
  process.
- We don't want decisions to be made unilaterally in a
  vacuum. Specifically, we want to give our steering group the
  opportunity to review every major decision.
- Despite being a geographically and temporally distributed team, we
  want our contributors to have a strong shared understanding of the
  technical rationale behind decisions.
- We want to be able to revisit prior decisions to determine fairly if
  they still make sense, and if the motivating circumstances or
  conditions have changed."
"Most applications need to store and manipulate data. In the current state of the art in Clojure, this is usually done in a straightforward, ad-hoc way. Users write schema, interact with their database, and parse data from user input into a persistence format using explicit code.

This is acceptable, if you're writing a custom, concrete application from scratch. But it will not work for Arachne. Arachne's modules need to be able to read and write domain data, while also being compatible with multiple backend storage modules. 

For example a user/password based authentication module needs to be able to read and write user records to the application database, and it should work whether a user is using a Datomic, SQL or NoSQL database.

In other words, Arachne cannot function well in a world in which every module is required to interoperate directly against one of several alternative modules. Instead, there needs to be a way for modules to ""speak a common language"" for data manipulation and persistence.

### Other use cases

Data persistence isn't the only concern. There are many other situations where having a common, abstract data model is highly useful. These include:

- quickly defining API endpoints based on a data model
- HTML & mobile form generation
- generalized data validation tools
- unified administration & metrics tools

### Modeling & Manipulation

There are actually two distinct concepts at play; data *modeling* and data *manipulation*.

**Modeling** is the activity of defining the abstract shape of the data; essentially, it is writing schema, but in a way that is not specific to any concrete implementation. Modules can then use the data model to generate concrete schema, generate API endpoints, forms, validate data, etc.

**Manipulation** is the activity of using the model to create, read update or delete actual data. For an abstract data manipulation layer, this generally means a polymorphic API that supports some common set of implementations, which can be extended to concrete CRUD operations 

### Existing solutions: ORMs

Most frameworks have some answer to this problem. Rails has ActiveRecord, Elixir has Ecto, old-school Java has Hibernate, etc. In every case, they try to paper over what it looks like to access the actual database, and provide an idiomatic API in the language to read and persist data. This language-level API is uniformly designed to make the database ""easy"" to use, but also has the effect of providing a common abstraction point for extensions.

Unfortunately, ORMs also exhibit a common set of problems. By their very nature, they are an extra level of indirection. They provide abstraction, but given how complex databases are the abstraction is always ""leaky"" in significant ways. Using them effectively requires a thorough understanding not only of the ORM's APIs, but also the underlying database implementation, and what the ORM is doing to map the data from one format to another.

ORMs are also tied more or less tightly to the relational model. Attempts to extend ActiveRecord (for example) to non-relational data stores have had varying levels of success.

### Database ""migrations""

One other function is to make sure that the concrete database schema matches the abstract data model that the application is using. Most ORMs implement this using some form of ""database migrations"", which serve as a repeatable series of all changes made to a database. Ideally, these are not redundant with the abstract data model, to avoid repeating the same information twice and also to ensure consistency.","Arachne will provide a lightweight model for data abstraction and persistence, oriented around the Entity/Attribute mode. To avoid word salad and acronyms loaded with baggage and false expectations, we will give it a semantically clean name. We will be free to define this name, and set expectations around what it is and how it is to be used. I suggest ""Chimera"", as it is in keeping with the Greek mythology theme and has several relevant connotations.

Chimera consists of two parts:

- An entity model, to allow application authors to easily specify the shape of their domain data in their Arachne configuration.
- A set of persistence operations, oriented around plain Clojure data (maps, sets and vectors) that can be implemented meaningfully against multiple types of adapters. Individual operations are granular and can be both consumed and provided á la carte; adapters that don't support certain behaviors can omit them (at the cost of compatibility with modules that need them.)

Although support for any arbitrary database cannot be guaranteed, the persistence operations are designed to support a majority of commonly used systems, including relational SQL databases, document stores, tuple stores, Datomic, or other ""NoSQL"" type systems.

At the data model level, Chimera should be a powerful, easy to use way to specify the structure of your data, as data. Modules can then read this data and expose new functionality driven by the application domain model. It needs to be flexible enough that it can be ""projected"" as schema into diverse types of adapters, and customizable enough that it can be configured to adapt to existing database installations.

#### Adapters

Chimera _Adapters_ are Arachne modules which take the abstract data structures and operations defined by Chimera, and extend them to specific databases or database APIs such as JDBC, Datomic, MongoDB, etc.

When applicable, there can also be ""abstract adapters"" that do the bulk of the work of adapting Chimera to some particular genre of database. For example, most key/value stores have similar semantics and core operations: there will likely be a ""Key/Value Adapter"" that does the bulk of the work for adapting Chimera's operations to key/value storage, and then several thin _concrete_ adapters that implement the actual get/put commands for Cassandra, DynamoDB, Redis, etc.

### Limitations and Drawbacks

Chimera is designed to make a limited set of common operations *possible* to write generically. It is not and cannot ever be a complete interface to every database. Application developers _can_ and _should_ understand and use the native APIs of their selected database, or use a dedicated wrapper module that exposes the full power of their selected technology. Chimera represents only a single dimension of functionality; the entity/attribute model. By definition, it cannot provide access to the unique and powerful features that different databases provide and which their users ought to leverage.

It is also important to recognize that there are problems (even problems that modules might want to tackle) for which Chimera's basic entity/attribute model is simply not a good fit. If the entity model isn't a good fit, <u>do not use</u> Chimera. Instead, find (or write) an Arachne module that defines a data modeling abstraction better suited for the task at hand. 

Examples of applications that might not be a good fit for Chimera include:

- Extremely sparse or ""wide"" data
- Dynamic data which cannot have pre-defined attributes or structure
- Unstructured heterogeneous data (such as large binary or sampling data)
- Data that cannot be indexed and requires distributed or streaming data processing to handle effectively

### Modeling

The data model for an Arachne application is, like everything else, data in the Configuration. Chimera defines a set of DSL forms that application authors can use to define data models programmatically, and of course modules can also read, write and modify these definitions as part of their normal configuration process.

Note: The configuration schema, including the schema for the data model, is _itself_ defined using Chimera. This requires some special bootstrapping in the core module. It also implies that Arachne core has a dependency on Chimera. This does not mean that modules are required to use Chimera or that Chimera has some special status relative to other conceivable data models; it just means that it is a good fit for modeling the kind of data that needs to be stored in the configuration.

#### Modeling: Entity Types

_Entity types_ are entities that define the structure and content for a _domain entity_. Entity types specify a set of optional and required _attributes_ that entities of that type must have.

Entity types may have one or more supertypes. Semantically, supertypes imply that any entity which is an instance of the subtype is also an instance of the supertype. Therefore, the set of attributes that are valid or required for an entity are the attributes of its types and all ancestor types.

Entity types define only data structures. They are not objects or classes; they do not define methods or behaviors.

In addition to defining the structure of entities themselves, entity types can have additional config attributes that serve as implementation-specific hints. For example, an entity type could have an attribute to override the name of the SQL table used for persistence. This config attribute would be defined and used by the SQL module, not by Chimera itself.

The basic attributes of the entity type, as defined by Chimera, are:

- The name of the type (as a namespace-qualified keyword)
- Any supertypes it may have
- What attributes can be applied to entities of this type

#### Attribute Definitions

Attribute Definition entities define what types of values can be associated with an entity. They specify:

1. The name of the attribute (as a namespace-qualified keyword)
1. The min and max cardinality of an attribute (thereby specifying whether it is required or optional)
1. The type of allowed values (see the section on _Value Types_ below)
1. Whether the attribute is a _key_. The values of a key attribute are expected to be globally unique, guaranteed to be present, and serve as a way to find specific entities, no matter what the underlying storage mechanism.
1. Whether the attribute is _indexed_. This is primarily a hint to the underlying database implementation.

Like entity types, attribute definitions may have any number of additional attributes, to modify behavior in an implementation-specific way.

##### Value Types

The value of an attribute may be one of three types:

1. A **reference** is a value that is itself an entity. The attribute must specify the entity type of the target entity. 
2. A **component** is a reference, with the added semantic implication that the value entity is a logical ""part"" of the parent entity. It will be retrieved automatically, along with the parent, and will also be deleted/retracted along with the parent entity.
3. A **primitive** is a simple, atomic value. Primitives may be one of several defined types, which map more or less directly to primitive types on the JVM:
   - Boolean (JVM `java.lang.Boolean`) 
   - String (JVM `java.lang.String`)
   - Keyword (Clojure `clojure.lang.Keyword`)
   - 64 bit integer (JVM `java.lang.Long`)
   - 64 bit floating point decimal (JVM `java.lang.Double`)
   - Arbitrary precision integer (JVM `java.math.BigInteger`)
   - Arbitrary precision decimal (JVM `java.math.BigDecimal`)
   - Instant (absolute time with millisecond resolution) (JVM `java.util.Date`)
   - UUID (JVM `java.util.UUID`)
   - Bytes (JVM byte array). Since not all storages support binary data, and might need to serialize it with base64, this should be fairly small.
   
   This set of primitives represent a reasonable common denominator that is supportable on most target databases. Note that the set is not closed: modules can specify new primitive types that are logically ""subtypes"" of the generic primitives. Entirely new types can also be defined (with the caveat that they will only work with adapters for which an implementation has been defined.)
   
#### Validation

All attribute names are namespace-qualified keywords. If there are specs registered using those keywords, they can be used to validate the corresponding values. 

Clojure requires that a namespace be loaded before the specs defined in it are globally registered. To ensure that all relevant specs are loaded before an application runs, Chimera provides config attributes that specify namespaces containing specs. Arachne will ensure that these namespaces are loaded first, so module authors can ensure that their specs are loaded before  they are needed.

Chimera also provides a `generate-spec` operation which programmatically builds a spec for a given entity type, that can validate a full entity map of that type.

#### Schema & Migration Operations

In order for data persistence to actually work, the schema of a particular database instance (at least, for those that have schema) needs to be compatible with the application's data model, as defined by Chimera's entity types and attributes.

See [ADR-16](adr-016-db-migrations.md) for an in-depth discussion of database migrations work, and the ramifications for how a Chimera data model is declared in the configuration.

### Entity Manipulation

The previous section discussed the data _model_, and how to define the general shape and structure of entities in an application. Entity _manipulation_ refers to how the operations available to create, read, update, delete specific *instances* of those entities.

#### Data Representation 
Domain entities are represented, in application code, as simple Clojure maps. In their function as Chimera entities, they are pure data; not objects. They are not required to support any additional protocols.

Entity keys are restricted to being namespace-qualified keywords, which correspond with the attribute names defined in configuration (see _Attribute Definitions_ above). Other keys will be ignored in Chimera's operations. Values may be any Clojure value, subject to spec validation before certain operations.

Cardinality-many attributes *must* use a Clojure sequence, even if there is only one value.

Reference values are represented in one of two ways; as a nested map, or as a _lookup reference_.

Nested maps are straightforward. For example: 

````
{:myapp.person/id 123
 :myapp.person/name ""Bill""
 :myapp.person/friends [{:myapp.person/id 42
                          :myapp.person/name ""Joe""}]}
````

Lookup references are special values that identify an attribute (which must be a key) and value to indicate the target reference. Chimera provides a tagged literal specifially for lookup references.

````
{:myapp.person/id 123
 :myapp.person/name ""Bill""
 :myapp.person/friends [#chimera.key[:myapp.person/id 42]]}
````

All Chimera operations that return data should use one of these representations. 

Both representations are largely equivalent, but there is an important note about passing nested maps to persistence operations: the intended semantics for any nested maps must be the same as the parent map. For example, you cannot call `create` and expect the top-level entity to be created while the nested entity is updated.

Entities do *not* need to explicitly declare their entity type. Types may be derived from inspecting the set of keys and comparing it to the Entity Types defined in the configuration.

#### Persistence Operations

The following basic operations are defined:

- `get` - Given an attribute name and value, return a set of matching entity maps from the database. Results are not guaranteed to be found unless the attribute is indexed. Results may be truncated if there are more than can be efficiently returned.
- `create` - Given a full entity map, transactionally store it in the database. Adapters _may_ throw an error if an entity with the same key attribute and value already exists.
- `update` - Given a map of attributes and values update each of the attributes provided attributes to have new values. The map must contain at least one key attribute. Also takes a set of attribute names which will be deleted/retracted from the entity. Adapters _may_ throw an error if no entity exists for the given key.
- `delete` - Given a key attribute and a value, remove the entity and all its attributes and components.

All these operations should be transactional if possible. Adapters which cannot provide transactional behavior for these operations should note this fact clearly in their documentation, so their users do not make false assumptions about the integrity of their systems.

Each of these operations has its own protocol which may be required by modules, or satisfied by adapters à la carte. Thus, a module that does not require the full set of operations can still work with an adapter, as long as it satisfies the operations that it *does* need.

This set of operations is not exhaustive; other modules and adapters are free to extend Chimera and define additional operations, with different or stricter semantics. These operations are those that it is possible to implement consistently, in a reasonably performant way, against a ""broad enough"" set of very different types of databases.

To make it possible for them to be composed more flexibly, operations are expressed as data, not as direct methods.

#### Capability Model

Adapters must specify a list of what operations they support. Modules should validate this list at runtime, to ensure the adapter works with the operations that they require.

In addition to specifying whether an operation is supported or not, adapters must specify whether they support the operation idempotently and/or transactionally.

","Architectural Decision Record

## Context
Most applications need to store and manipulate data. In the current state of the art in Clojure, this is usually done in a straightforward, ad-hoc way. Users write schema, interact with their database, and parse data from user input into a persistence format using explicit code.

This is acceptable, if you're writing a custom, concrete application from scratch. But it will not work for Arachne. Arachne's modules need to be able to read and write domain data, while also being compatible with multiple backend storage modules. 

For example a user/password based authentication module needs to be able to read and write user records to the application database, and it should work whether a user is using a Datomic, SQL or NoSQL database.

In other words, Arachne cannot function well in a world in which every module is required to interoperate directly against one of several alternative modules. Instead, there needs to be a way for modules to ""speak a common language"" for data manipulation and persistence.

### Other use cases

Data persistence isn't the only concern. There are many other situations where having a common, abstract data model is highly useful. These include:

- quickly defining API endpoints based on a data model
- HTML & mobile form generation
- generalized data validation tools
- unified administration & metrics tools

### Modeling & Manipulation

There are actually two distinct concepts at play; data *modeling* and data *manipulation*.

**Modeling** is the activity of defining the abstract shape of the data; essentially, it is writing schema, but in a way that is not specific to any concrete implementation. Modules can then use the data model to generate concrete schema, generate API endpoints, forms, validate data, etc.

**Manipulation** is the activity of using the model to create, read update or delete actual data. For an abstract data manipulation layer, this generally means a polymorphic API that supports some common set of implementations, which can be extended to concrete CRUD operations 

### Existing solutions: ORMs

Most frameworks have some answer to this problem. Rails has ActiveRecord, Elixir has Ecto, old-school Java has Hibernate, etc. In every case, they try to paper over what it looks like to access the actual database, and provide an idiomatic API in the language to read and persist data. This language-level API is uniformly designed to make the database ""easy"" to use, but also has the effect of providing a common abstraction point for extensions.

Unfortunately, ORMs also exhibit a common set of problems. By their very nature, they are an extra level of indirection. They provide abstraction, but given how complex databases are the abstraction is always ""leaky"" in significant ways. Using them effectively requires a thorough understanding not only of the ORM's APIs, but also the underlying database implementation, and what the ORM is doing to map the data from one format to another.

ORMs are also tied more or less tightly to the relational model. Attempts to extend ActiveRecord (for example) to non-relational data stores have had varying levels of success.

### Database ""migrations""

One other function is to make sure that the concrete database schema matches the abstract data model that the application is using. Most ORMs implement this using some form of ""database migrations"", which serve as a repeatable series of all changes made to a database. Ideally, these are not redundant with the abstract data model, to avoid repeating the same information twice and also to ensure consistency.

## Decision
","## Context

Most applications need to store and manipulate data. In the current state of the art in Clojure, this is usually done in a straightforward, ad-hoc way. Users write schema, interact with their database, and parse data from user input into a persistence format using explicit code.

This is acceptable, if you're writing a custom, concrete application from scratch. But it will not work for Arachne. Arachne's modules need to be able to read and write domain data, while also being compatible with multiple backend storage modules. 

For example a user/password based authentication module needs to be able to read and write user records to the application database, and it should work whether a user is using a Datomic, SQL or NoSQL database.

In other words, Arachne cannot function well in a world in which every module is required to interoperate directly against one of several alternative modules. Instead, there needs to be a way for modules to ""speak a common language"" for data manipulation and persistence.

### Other use cases

Data persistence isn't the only concern. There are many other situations where having a common, abstract data model is highly useful. These include:

- quickly defining API endpoints based on a data model
- HTML & mobile form generation
- generalized data validation tools
- unified administration & metrics tools

### Modeling & Manipulation

There are actually two distinct concepts at play; data *modeling* and data *manipulation*.

**Modeling** is the activity of defining the abstract shape of the data; essentially, it is writing schema, but in a way that is not specific to any concrete implementation. Modules can then use the data model to generate concrete schema, generate API endpoints, forms, validate data, etc.

**Manipulation** is the activity of using the model to create, read update or delete actual data. For an abstract data manipulation layer, this generally means a polymorphic API that supports some common set of implementations, which can be extended to concrete CRUD operations 

### Existing solutions: ORMs

Most frameworks have some answer to this problem. Rails has ActiveRecord, Elixir has Ecto, old-school Java has Hibernate, etc. In every case, they try to paper over what it looks like to access the actual database, and provide an idiomatic API in the language to read and persist data. This language-level API is uniformly designed to make the database ""easy"" to use, but also has the effect of providing a common abstraction point for extensions.

Unfortunately, ORMs also exhibit a common set of problems. By their very nature, they are an extra level of indirection. They provide abstraction, but given how complex databases are the abstraction is always ""leaky"" in significant ways. Using them effectively requires a thorough understanding not only of the ORM's APIs, but also the underlying database implementation, and what the ORM is doing to map the data from one format to another.

ORMs are also tied more or less tightly to the relational model. Attempts to extend ActiveRecord (for example) to non-relational data stores have had varying levels of success.

### Database ""migrations""

One other function is to make sure that the concrete database schema matches the abstract data model that the application is using. Most ORMs implement this using some form of ""database migrations"", which serve as a repeatable series of all changes made to a database. Ideally, these are not redundant with the abstract data model, to avoid repeating the same information twice and also to ensure consistency.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Most applications need to store and manipulate data. In the current state of the art in Clojure, this is usually done in a straightforward, ad-hoc way. Users write schema, interact with their database, and parse data from user input into a persistence format using explicit code.

This is acceptable, if you're writing a custom, concrete application from scratch. But it will not work for Arachne. Arachne's modules need to be able to read and write domain data, while also being compatible with multiple backend storage modules. 

For example a user/password based authentication module needs to be able to read and write user records to the application database, and it should work whether a user is using a Datomic, SQL or NoSQL database.

In other words, Arachne cannot function well in a world in which every module is required to interoperate directly against one of several alternative modules. Instead, there needs to be a way for modules to ""speak a common language"" for data manipulation and persistence.

### Other use cases

Data persistence isn't the only concern. There are many other situations where having a common, abstract data model is highly useful. These include:

- quickly defining API endpoints based on a data model
- HTML & mobile form generation
- generalized data validation tools
- unified administration & metrics tools

### Modeling & Manipulation

There are actually two distinct concepts at play; data *modeling* and data *manipulation*.

**Modeling** is the activity of defining the abstract shape of the data; essentially, it is writing schema, but in a way that is not specific to any concrete implementation. Modules can then use the data model to generate concrete schema, generate API endpoints, forms, validate data, etc.

**Manipulation** is the activity of using the model to create, read update or delete actual data. For an abstract data manipulation layer, this generally means a polymorphic API that supports some common set of implementations, which can be extended to concrete CRUD operations 

### Existing solutions: ORMs

Most frameworks have some answer to this problem. Rails has ActiveRecord, Elixir has Ecto, old-school Java has Hibernate, etc. In every case, they try to paper over what it looks like to access the actual database, and provide an idiomatic API in the language to read and persist data. This language-level API is uniformly designed to make the database ""easy"" to use, but also has the effect of providing a common abstraction point for extensions.

Unfortunately, ORMs also exhibit a common set of problems. By their very nature, they are an extra level of indirection. They provide abstraction, but given how complex databases are the abstraction is always ""leaky"" in significant ways. Using them effectively requires a thorough understanding not only of the ORM's APIs, but also the underlying database implementation, and what the ORM is doing to map the data from one format to another.

ORMs are also tied more or less tightly to the relational model. Attempts to extend ActiveRecord (for example) to non-relational data stores have had varying levels of success.

### Database ""migrations""

One other function is to make sure that the concrete database schema matches the abstract data model that the application is using. Most ORMs implement this using some form of ""database migrations"", which serve as a repeatable series of all changes made to a database. Ideally, these are not redundant with the abstract data model, to avoid repeating the same information twice and also to ensure consistency."
"Per [ADR-003](adr-003-config-implementation.md), Arachne uses
Datomic-shaped data for configuration. Although this is a flexible,
extensible data structure which is a great fit for programmatic
manipulation, in its literal form it is quite verbose.

It is quite difficult to understand the structure of Datomic data by
reading its native textual representation, and it is similarly hard to
write, containing enough repeated elements that copying and pasting
quickly becomes the default.

One of Arachne's core values is ease of use and a fluent experience
for developers. Since much of a developer's interaction with Arachne
will be writing to the config, it is of paramount importance that
there be some easy way to create configuration data.

The question is, what is the best way for developers of Arachne
applications to interact with their application's configuration?

#### Option: Raw Datomic Txdata

This would require end users to write Datomic transaction data by hand
in order to configure their application.

This is the ""simplest"" option, and has the fewest moving
parts. However, as mentioned above, it is very far from ideal for
human interactions.

#### Option: Custom EDN data formats

In this scenario, users would write EDN data in some some nested
structure of maps, sets, seqs and primitives. This is currently the
most common way to configure Clojure applications.

Each module would then need to provide a mapping from the EDN config
format to the underlying Datomic-style config data.

Because Arachne's configuration is so much broader, and defines so
much more of an application than a typical application config file, 
it is questionable if standard nested EDN data would be a good fit 
for representing it.

#### Option: Code-based configuration

Another option would be to go in the direction of some other
frameworks, such as Ruby on Rails, and have the user-facing
configuration be *code* rather than data.

It should be noted that the primary motivation for having a
data-oriented configuration language, that it makes it easier to
interact with programmatically, doesn't really apply in Arachne's
case. Since applications are always free to interact richly with
Arachne's full configuration database, the ability to programmatically
manipulate the precursor data is moot. As such, one major argument
against a code-based configuration strategy does not apply.","Developers will have the option of writing configuration using either
native Datomic-style, data, or code-based *configuration
scripts*. Configuration scripts are Clojure files which, when
evaluated, update a configuration stored in an atom currently in
context (using a dynamically bound var.)

Configuration scripts are Clojure source files in a distinct directory
that by convention is *outside* the application's classpath:
configuration code is conceptually and physically separate from
application code. Conceptually, loading the configuration scripts
could take place in an entirely different process from the primary
application, serializing the resulting config before handing it to the
runtime application.

To further emphasize the difference between configuration scripts and
runtime code, and because they are not on the classpath, configuration
scripts will not have namespaces and will instead include each other
via Clojure's `load` function.

Arachne will provide code supporting the ability of module authors to
write ""configuration DSLs"" for users to invoke from their
configuration scripts. These DSLs will emphasize making it easy to
create appropriate entities in the configuration. In general, DSL
forms will have an imperative style: they will convert their arguments
to configuration data and immediately transact it to the context
configuration.

As a trivial example, instead of writing the verbose configuration data:

```clojure
{:arachne/id :my.app/server
 :arachne.http.server/port 8080
 :arachne.http.server/debug true}
 ```

You could write the corresponding DSL:

```clojure
(server :id :my.app/server, :port 8080, :debug true)
```

Note that this is an illustrative example and does not represent the
actual DSL or config for the HTTP module.

DSLs should make heavy use of Spec to make errors as comprehensible as possible.

","Architectural Decision Record

## Context
Per [ADR-003](adr-003-config-implementation.md), Arachne uses
Datomic-shaped data for configuration. Although this is a flexible,
extensible data structure which is a great fit for programmatic
manipulation, in its literal form it is quite verbose.

It is quite difficult to understand the structure of Datomic data by
reading its native textual representation, and it is similarly hard to
write, containing enough repeated elements that copying and pasting
quickly becomes the default.

One of Arachne's core values is ease of use and a fluent experience
for developers. Since much of a developer's interaction with Arachne
will be writing to the config, it is of paramount importance that
there be some easy way to create configuration data.

The question is, what is the best way for developers of Arachne
applications to interact with their application's configuration?

#### Option: Raw Datomic Txdata

This would require end users to write Datomic transaction data by hand
in order to configure their application.

This is the ""simplest"" option, and has the fewest moving
parts. However, as mentioned above, it is very far from ideal for
human interactions.

#### Option: Custom EDN data formats

In this scenario, users would write EDN data in some some nested
structure of maps, sets, seqs and primitives. This is currently the
most common way to configure Clojure applications.

Each module would then need to provide a mapping from the EDN config
format to the underlying Datomic-style config data.

Because Arachne's configuration is so much broader, and defines so
much more of an application than a typical application config file, 
it is questionable if standard nested EDN data would be a good fit 
for representing it.

#### Option: Code-based configuration

Another option would be to go in the direction of some other
frameworks, such as Ruby on Rails, and have the user-facing
configuration be *code* rather than data.

It should be noted that the primary motivation for having a
data-oriented configuration language, that it makes it easier to
interact with programmatically, doesn't really apply in Arachne's
case. Since applications are always free to interact richly with
Arachne's full configuration database, the ability to programmatically
manipulate the precursor data is moot. As such, one major argument
against a code-based configuration strategy does not apply.

## Decision
","## Context

Per [ADR-003](adr-003-config-implementation.md), Arachne uses
Datomic-shaped data for configuration. Although this is a flexible,
extensible data structure which is a great fit for programmatic
manipulation, in its literal form it is quite verbose.

It is quite difficult to understand the structure of Datomic data by
reading its native textual representation, and it is similarly hard to
write, containing enough repeated elements that copying and pasting
quickly becomes the default.

One of Arachne's core values is ease of use and a fluent experience
for developers. Since much of a developer's interaction with Arachne
will be writing to the config, it is of paramount importance that
there be some easy way to create configuration data.

The question is, what is the best way for developers of Arachne
applications to interact with their application's configuration?

#### Option: Raw Datomic Txdata

This would require end users to write Datomic transaction data by hand
in order to configure their application.

This is the ""simplest"" option, and has the fewest moving
parts. However, as mentioned above, it is very far from ideal for
human interactions.

#### Option: Custom EDN data formats

In this scenario, users would write EDN data in some some nested
structure of maps, sets, seqs and primitives. This is currently the
most common way to configure Clojure applications.

Each module would then need to provide a mapping from the EDN config
format to the underlying Datomic-style config data.

Because Arachne's configuration is so much broader, and defines so
much more of an application than a typical application config file, 
it is questionable if standard nested EDN data would be a good fit 
for representing it.

#### Option: Code-based configuration

Another option would be to go in the direction of some other
frameworks, such as Ruby on Rails, and have the user-facing
configuration be *code* rather than data.

It should be noted that the primary motivation for having a
data-oriented configuration language, that it makes it easier to
interact with programmatically, doesn't really apply in Arachne's
case. Since applications are always free to interact richly with
Arachne's full configuration database, the ability to programmatically
manipulate the precursor data is moot. As such, one major argument
against a code-based configuration strategy does not apply.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Per [ADR-003](adr-003-config-implementation.md), Arachne uses
Datomic-shaped data for configuration. Although this is a flexible,
extensible data structure which is a great fit for programmatic
manipulation, in its literal form it is quite verbose.

It is quite difficult to understand the structure of Datomic data by
reading its native textual representation, and it is similarly hard to
write, containing enough repeated elements that copying and pasting
quickly becomes the default.

One of Arachne's core values is ease of use and a fluent experience
for developers. Since much of a developer's interaction with Arachne
will be writing to the config, it is of paramount importance that
there be some easy way to create configuration data.

The question is, what is the best way for developers of Arachne
applications to interact with their application's configuration?

#### Option: Raw Datomic Txdata

This would require end users to write Datomic transaction data by hand
in order to configure their application.

This is the ""simplest"" option, and has the fewest moving
parts. However, as mentioned above, it is very far from ideal for
human interactions.

#### Option: Custom EDN data formats

In this scenario, users would write EDN data in some some nested
structure of maps, sets, seqs and primitives. This is currently the
most common way to configure Clojure applications.

Each module would then need to provide a mapping from the EDN config
format to the underlying Datomic-style config data.

Because Arachne's configuration is so much broader, and defines so
much more of an application than a typical application config file, 
it is questionable if standard nested EDN data would be a good fit 
for representing it.

#### Option: Code-based configuration

Another option would be to go in the direction of some other
frameworks, such as Ruby on Rails, and have the user-facing
configuration be *code* rather than data.

It should be noted that the primary motivation for having a
data-oriented configuration language, that it makes it easier to
interact with programmatically, doesn't really apply in Arachne's
case. Since applications are always free to interact richly with
Arachne's full configuration database, the ability to programmatically
manipulate the precursor data is moot. As such, one major argument
against a code-based configuration strategy does not apply."
"In general, Arachne's philosophy embraces the concepts of immutability and reproducibility; rather than _changing_ something, replace it with something new. Usually, this simplifies the mental model and reduces the number of variables, reducing the ways in which things can go wrong.

But there is one area where this approach just can't work: administering changes to a production database. Databases must have a stable existence across time. You can't throw away all your data every time you want to make a change.

And yet, some changes in the database do need to happen. Data models change. New fields are added. Entity relationships are refactored. 

The challenge is to provide a way to provide measured, safe, reproducible change across time which is _also_ compatible with Arachne's target of defining and describing all relevant parts of an application (including it's data model (and therefore schema)) in a configuration.

Compounding the challenge is the need to build a system that can define concrete schema for different types of databases, based on a common data model (such as Chimera's, as described in [ADR-15](adr-015-data-abstraction-model.md).)

### Prior Art

Several systems to do this already exist. The best known is probably Rails' [Active Record Migrations](http://guides.rubyonrails.org/active_record_migrations.html), which is oriented around making schema changes to a relational database.

Another solution of interest is [Liquibase](http://www.liquibase.org), a system which reifies database changes as data and explicitly applies them to a relation database.  

### Scenarios

There are a variety of ""user stories"" to accomodate. Some examples include:

1. You are a new developer on a project, and want to create a local database that will work with the current HEAD of the codebase, for local development.
2. You are responsible for the production deployment of your project, and your team has a new software version ready to go, but it requires some new fields to be added to the database before the new code will run.
3. You want to set up a staging environment that is an exact mirror of your current production system.
4. You and a fellow developer are merging your branches for different features. You both made different changes to the data model, and you need to be sure they are compatible after the merge.
5. You recognize that you made a mistake earlier in development, and stored a currency value as a floating point number. You need to create a new column in the database which uses a fixed-point type, and copy over all the existing values, using rounding logic that you've agreed on with domain experts.","Chimera will explicitly define the concept of a migration, and reify migrations as entities in the configuration.

A migration represents an atomic set of changes to the schema of a database. For any given database instance, either a migration has logically been applied, or it hasn't. Migrations have unique IDs, expressed as namespace-qualified keywords.

Every migration has one or more ""parent"" migrations (except for a single, special ""initial"" migration, which has no parent). A migration may not be applied to a database unless all of its parents have already been applied.

Migrations are also have a _signature_. The signature is an MD5 checksum of the _actual content_ of the migration as it is applied to the database (whether that be txdata for Datomic, a string for SQL DDL, a JSON string, etc.) This is used to ensure that a migration is not ""changed"" after it has already been applied to some persistent database.

Adapters are responsible for exposing an implementation of migrations (and accompanying config DSL) that is appropriate for the database type.

Chimera Adapters must additionally satisfy two runtime operations:

- `has-migration?` - takes ID and signature of a particular migration, and returns true if the migration has been successfully applied to the database. This implies that databases must be ""migration aware"" and store the IDs/signatures of migrations that have already been applied.
- `migrate` - given a specific migration, run the migration and record that the migration has been applied.

### Migration Types

There are four basic types of migrations. 

1. **Native migrations**. These are instances of the migration type directly implemented by a database adapter, and are specific to the type of DB being used. For example, a native migration against a SQL database would be implemented (primarily) via a SQL string. A native migration can only be used by adapters of the appropriate type.
2. **Chimera migrations**. These define migrations using Chimera's entity/attribute data model. They are abstract, and should work against multiple different types of adapters. Chimera migrations should be supported by all Chimera adapters.
4. **Sentinel migrations**. These are used to coordinate manual changes to an existing database with the code that requires them. They will always fail to automatically apply to an existing database: the database admin must add the migration record explicitly after they perform the manual migration task.  _(Note, actually implementing these can be deferred until if or when they are needed)_.

### Structure & Usage

Because migrations may have one or more parents, migrations form a directed acyclic graph.

This is appropriate, and combines well with Arachne's composability model. A module may define a sequence of migrations that build up a data model, and extending modules can branch from any point to build their own data model that shares structure with it. Modules may also depend upon a chain of migrations specified in two dependent modules, to indicate that it requires both of them.

In the configuration, a Chimera **database component** may depend on any number of migration components. These migrations, and all their ancestors, form a ""database definition"", and represent the complete schema of a concrete database instance (as far as Chimera is concerned.)

When a database component is started and connects to the underlying data store, it verifies that all the specifies migrations have been applied. If they have not, it fails to start. This guarantees the safety of an Arachne system; a given application simply will not start if it is not compatible with the specified database.

##### Parallel Migrations

This does create an opportunity for problems: if two migrations which have no dependency relatinship (""parallel migrations"") have operations that are incompatible, or would yield different results depending on the order in which they are applied, then these operations ""conflict"" and applying them to a database could result in errors or non-deterministic behavior. 

If the parallel migrations are both Chimera migrations, then Arachne is aware of their internal structure and can detect the conflict and refuse to start or run the migrations, before it actually touches the database. 

Unfortunately, Arachne cannot detect conflicting parallel migrations for other migration types. It is the responsibility of application developers to ensure that parallel migrations are logically isolate and can coexist in the same database without conflict. 

Therefore, it is advisable in general for public modules to only use Chimera migrations. In addition to making them as broadly compatible as possible, and will also make it more tractable for application authors to avoid conflicting parallel migrations, since they only have to worry about those that they themselves create.

### Chimera Migrations & Entity Types

One drawback of using Chimera migrations is that you cannot see a full entity type defined in one place, just from reading a config DSL script. This cannot be avoided: in a real, living application, entities are defined over time, in many different migrations as the application grows, not all at once. Each Chimera migration contains only a fragment of the full data model.

However, this poses a usability problem; both for developers, and for machine consumption. There are many reasons for developers or modules to view or query the entity type model as a ""point in time"" snapshot, rather than just a series of incremental changes.

To support this use case, the Chimera module creates a flat entity type model for each database by ""rolling up"" the individual Chimera entity definition forms into a single, full data structure graph. This ""canonical entity model"" can then be used to render schema diagrams for users, or be queried by other modules.

### Applying Migrations

When and how to invoke an Adapter's `migrate` function is not defined, since different teams will wish to do it in different ways.

Some possibilities include:

1. The application calls ""migrate"" every time it is started (this is only advisable if the database has excellent support for transactional and atomic migrations.) In this scenario, developers only need to worry about deploying the code.
2. The devops team can manually invoke the ""migrate"" function for each new configuration, prior to deployment.
3. In a continuous-deployment setup, a CI server could run a battery of tests against a clone of the production database and invoke ""migrate"" automatically if they pass.
4. The development team can inspect the set of migrations and generate a set of native SQL or txdata statements for handoff to a dedicated DBA team for review and commit prior to deployment.

### Databases without migrations

Not every application wants to use Chimera's migration system. Some situations where migrations may not be a good fit include:

- You prefer to manage your own database schema.
- You are working with an existing database that predates Arachne.
- You need to work with a database administered by a separate team.

However, you still may wish to utilize Chimera's entity model, and leverage modules that define Chimera migrations.

To support this, Chimera allows you to (in the configuration) designate a database component as ""**assert-only**"". Assert-only databases never have migrations applied, and they do not require the database to track any concept of migrations. Instead, they inspect the Chimera entity model (after rolling up all declared migrations) and assert that the database _already_ has compatible schema installed. If it does, everything starts up as normal; if it does not, the component fails to start.

Of course, the schema that Chimera expects most likely will not be an exact match for what is present in the database. To accomodate this, Chimera adapters defines a set of _override_ configuration entities (and accompanying DSL). Users can apply these overrides to change the behavior of the mappings that Chimera uses to query and store data. 

Note that Chimera Overrides are incompatible with actually running migrations: they can be used only on an ""assert-only"" database.

### Migration Rollback

Generalized rollback of migrations is intractable, given the variety of databases Chimera intends to support. Use one of the following strategies instead:

- For development and testing, be constantly creating and throwing away new databases.
- Back up your database before running a migration
- If you can't afford the downtime or data loss associated with restoring a backup, manually revert the changes from the unwanted migration.
   
","Architectural Decision Record

## Context
In general, Arachne's philosophy embraces the concepts of immutability and reproducibility; rather than _changing_ something, replace it with something new. Usually, this simplifies the mental model and reduces the number of variables, reducing the ways in which things can go wrong.

But there is one area where this approach just can't work: administering changes to a production database. Databases must have a stable existence across time. You can't throw away all your data every time you want to make a change.

And yet, some changes in the database do need to happen. Data models change. New fields are added. Entity relationships are refactored. 

The challenge is to provide a way to provide measured, safe, reproducible change across time which is _also_ compatible with Arachne's target of defining and describing all relevant parts of an application (including it's data model (and therefore schema)) in a configuration.

Compounding the challenge is the need to build a system that can define concrete schema for different types of databases, based on a common data model (such as Chimera's, as described in [ADR-15](adr-015-data-abstraction-model.md).)

### Prior Art

Several systems to do this already exist. The best known is probably Rails' [Active Record Migrations](http://guides.rubyonrails.org/active_record_migrations.html), which is oriented around making schema changes to a relational database.

Another solution of interest is [Liquibase](http://www.liquibase.org), a system which reifies database changes as data and explicitly applies them to a relation database.  

### Scenarios

There are a variety of ""user stories"" to accomodate. Some examples include:

1. You are a new developer on a project, and want to create a local database that will work with the current HEAD of the codebase, for local development.
2. You are responsible for the production deployment of your project, and your team has a new software version ready to go, but it requires some new fields to be added to the database before the new code will run.
3. You want to set up a staging environment that is an exact mirror of your current production system.
4. You and a fellow developer are merging your branches for different features. You both made different changes to the data model, and you need to be sure they are compatible after the merge.
5. You recognize that you made a mistake earlier in development, and stored a currency value as a floating point number. You need to create a new column in the database which uses a fixed-point type, and copy over all the existing values, using rounding logic that you've agreed on with domain experts.

## Decision
","## Context

In general, Arachne's philosophy embraces the concepts of immutability and reproducibility; rather than _changing_ something, replace it with something new. Usually, this simplifies the mental model and reduces the number of variables, reducing the ways in which things can go wrong.

But there is one area where this approach just can't work: administering changes to a production database. Databases must have a stable existence across time. You can't throw away all your data every time you want to make a change.

And yet, some changes in the database do need to happen. Data models change. New fields are added. Entity relationships are refactored. 

The challenge is to provide a way to provide measured, safe, reproducible change across time which is _also_ compatible with Arachne's target of defining and describing all relevant parts of an application (including it's data model (and therefore schema)) in a configuration.

Compounding the challenge is the need to build a system that can define concrete schema for different types of databases, based on a common data model (such as Chimera's, as described in [ADR-15](adr-015-data-abstraction-model.md).)

### Prior Art

Several systems to do this already exist. The best known is probably Rails' [Active Record Migrations](http://guides.rubyonrails.org/active_record_migrations.html), which is oriented around making schema changes to a relational database.

Another solution of interest is [Liquibase](http://www.liquibase.org), a system which reifies database changes as data and explicitly applies them to a relation database.  

### Scenarios

There are a variety of ""user stories"" to accomodate. Some examples include:

1. You are a new developer on a project, and want to create a local database that will work with the current HEAD of the codebase, for local development.
2. You are responsible for the production deployment of your project, and your team has a new software version ready to go, but it requires some new fields to be added to the database before the new code will run.
3. You want to set up a staging environment that is an exact mirror of your current production system.
4. You and a fellow developer are merging your branches for different features. You both made different changes to the data model, and you need to be sure they are compatible after the merge.
5. You recognize that you made a mistake earlier in development, and stored a currency value as a floating point number. You need to create a new column in the database which uses a fixed-point type, and copy over all the existing values, using rounding logic that you've agreed on with domain experts.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
In general, Arachne's philosophy embraces the concepts of immutability and reproducibility; rather than _changing_ something, replace it with something new. Usually, this simplifies the mental model and reduces the number of variables, reducing the ways in which things can go wrong.

But there is one area where this approach just can't work: administering changes to a production database. Databases must have a stable existence across time. You can't throw away all your data every time you want to make a change.

And yet, some changes in the database do need to happen. Data models change. New fields are added. Entity relationships are refactored. 

The challenge is to provide a way to provide measured, safe, reproducible change across time which is _also_ compatible with Arachne's target of defining and describing all relevant parts of an application (including it's data model (and therefore schema)) in a configuration.

Compounding the challenge is the need to build a system that can define concrete schema for different types of databases, based on a common data model (such as Chimera's, as described in [ADR-15](adr-015-data-abstraction-model.md).)

### Prior Art

Several systems to do this already exist. The best known is probably Rails' [Active Record Migrations](http://guides.rubyonrails.org/active_record_migrations.html), which is oriented around making schema changes to a relational database.

Another solution of interest is [Liquibase](http://www.liquibase.org), a system which reifies database changes as data and explicitly applies them to a relation database.  

### Scenarios

There are a variety of ""user stories"" to accomodate. Some examples include:

1. You are a new developer on a project, and want to create a local database that will work with the current HEAD of the codebase, for local development.
2. You are responsible for the production deployment of your project, and your team has a new software version ready to go, but it requires some new fields to be added to the database before the new code will run.
3. You want to set up a staging environment that is an exact mirror of your current production system.
4. You and a fellow developer are merging your branches for different features. You both made different changes to the data model, and you need to be sure they are compatible after the merge.
5. You recognize that you made a mistake earlier in development, and stored a currency value as a floating point number. You need to create a new column in the database which uses a fixed-point type, and copy over all the existing values, using rounding logic that you've agreed on with domain experts."
"In [ADR-003](adr-003-config-implementation.md) it was decided to use a Datomic-based configuration, the alternative being something more semantically or ontologically descriptive such as RDF+OWL. 

Although we elected to use Datomic, Datomic does not itself offer much ontological modeling capacity. It has no built-in notion of types/classes, and its attribute specifications are limited to what is necessary for efficient storage and indexing, rather than expressive or validative power.

Ideally, we want modules to be able to communicate additional information about the structure and intent of their domain model, including:

- Types of entities which can exist
- Relationships between those types
- Logical constraints on the values of attributes:
    - more fine grained cardinality; optional/required attributes
    - valid value ranges
    - target entity type (for ref attributes)

This additional data could serve three purposes:

- Documentation about the intended purpose and structure of the configuration defined by a module.
- Deeper, more specific validation of user-supplied configuration values
- Machine-readable integration point for tools which consume and produce Arachne configurations.","- We will add meta-attributes to the schema of every configuration, expressing basic ontological relationships.
- These attributes will be semantically compatible with OWL (such that we could conceivably in the future generate an OWL ontology from a config schema)
- The initial set of these attributes will be minimal, and targeted towards the information necessary to generate rich schema diagrams
  - classes and superclass
  - attribute domain
  - attribute range (for ref attributes)
  - min and max cardinality
- Arachne core will provide some (optional) utility functions for schema generation, to make writing module schemas less verbose.

","Architectural Decision Record

## Context
In [ADR-003](adr-003-config-implementation.md) it was decided to use a Datomic-based configuration, the alternative being something more semantically or ontologically descriptive such as RDF+OWL. 

Although we elected to use Datomic, Datomic does not itself offer much ontological modeling capacity. It has no built-in notion of types/classes, and its attribute specifications are limited to what is necessary for efficient storage and indexing, rather than expressive or validative power.

Ideally, we want modules to be able to communicate additional information about the structure and intent of their domain model, including:

- Types of entities which can exist
- Relationships between those types
- Logical constraints on the values of attributes:
    - more fine grained cardinality; optional/required attributes
    - valid value ranges
    - target entity type (for ref attributes)

This additional data could serve three purposes:

- Documentation about the intended purpose and structure of the configuration defined by a module.
- Deeper, more specific validation of user-supplied configuration values
- Machine-readable integration point for tools which consume and produce Arachne configurations.

## Decision
","## Context

In [ADR-003](adr-003-config-implementation.md) it was decided to use a Datomic-based configuration, the alternative being something more semantically or ontologically descriptive such as RDF+OWL. 

Although we elected to use Datomic, Datomic does not itself offer much ontological modeling capacity. It has no built-in notion of types/classes, and its attribute specifications are limited to what is necessary for efficient storage and indexing, rather than expressive or validative power.

Ideally, we want modules to be able to communicate additional information about the structure and intent of their domain model, including:

- Types of entities which can exist
- Relationships between those types
- Logical constraints on the values of attributes:
    - more fine grained cardinality; optional/required attributes
    - valid value ranges
    - target entity type (for ref attributes)

This additional data could serve three purposes:

- Documentation about the intended purpose and structure of the configuration defined by a module.
- Deeper, more specific validation of user-supplied configuration values
- Machine-readable integration point for tools which consume and produce Arachne configurations.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
In [ADR-003](adr-003-config-implementation.md) it was decided to use a Datomic-based configuration, the alternative being something more semantically or ontologically descriptive such as RDF+OWL. 

Although we elected to use Datomic, Datomic does not itself offer much ontological modeling capacity. It has no built-in notion of types/classes, and its attribute specifications are limited to what is necessary for efficient storage and indexing, rather than expressive or validative power.

Ideally, we want modules to be able to communicate additional information about the structure and intent of their domain model, including:

- Types of entities which can exist
- Relationships between those types
- Logical constraints on the values of attributes:
    - more fine grained cardinality; optional/required attributes
    - valid value ranges
    - target entity type (for ref attributes)

This additional data could serve three purposes:

- Documentation about the intended purpose and structure of the configuration defined by a module.
- Deeper, more specific validation of user-supplied configuration values
- Machine-readable integration point for tools which consume and produce Arachne configurations."
"When starting a new project, it isn't practical to start completely from scratch, every time. We would like to have a varity of ""starting point"" projects, for different purposes.

### Lein templates
In the Clojure space, Leiningen Templates fill this purpose. These are sets of special string-interpolated files that are ""rendered"" into a working project using special tooling.

However, they have two major drawbacks:

- They only work when using Leiningen as a build tool.
- The template files are are not actually valid source files, which makes them difficult to maintain. Changes need to be manually copied over to the templates.

### Rails templates

Rails also provides a complete project templating solution. In Rails, the project template is a `template.rb` file which contains DSL forms that specify operations to perform on a fresh project. These operations include creating files, modifying a projects dependencies, adding Rake tasks, and running specific _generators_. 

Generators are particularly interesting, because the idea is that they can generate or modify stubs for files pertaining to a specific part of the application (e.g, a new model or a new controller), and they can be invoked _at any point_, not just initial project creation. ","To start with, Arachne templates will be standard git repositories containing an Arachne project. They will use no special syntax, and will be valid, runnable projects out of the box.

In order to allow users to create their own projects, these template projects will include a `rename` script. The `rename` script will recursively rename an entire project directory to something that the user chooses, and will delete `.git` and re-run `git init`, 

Therefore, the process to start a new Arachne project will be:

1. Choose an appropriate project template.
2. Clone its git repository from Github
3. Run the `rename` script to rename the project to whatever you wish
4. Start a repl, and begin editing.

### Maven Distribution

There are certain development environments where there is not full access to the open internet (particularly in certain governmental applications.) Therefore, accessing GitHub can prove difficult. However, in order to support developers, these organizations often run their own Maven mirrors.

As a convenience to users in these situations, when it is necessary, we can build a wrapper that can compress and install a project directory as a Maven artifact. Then, using standard Maven command line tooling, it will be possible to download and decompress the artifact into a local filesystem directory, and proceed as normal.

","Architectural Decision Record

## Context
When starting a new project, it isn't practical to start completely from scratch, every time. We would like to have a varity of ""starting point"" projects, for different purposes.

### Lein templates
In the Clojure space, Leiningen Templates fill this purpose. These are sets of special string-interpolated files that are ""rendered"" into a working project using special tooling.

However, they have two major drawbacks:

- They only work when using Leiningen as a build tool.
- The template files are are not actually valid source files, which makes them difficult to maintain. Changes need to be manually copied over to the templates.

### Rails templates

Rails also provides a complete project templating solution. In Rails, the project template is a `template.rb` file which contains DSL forms that specify operations to perform on a fresh project. These operations include creating files, modifying a projects dependencies, adding Rake tasks, and running specific _generators_. 

Generators are particularly interesting, because the idea is that they can generate or modify stubs for files pertaining to a specific part of the application (e.g, a new model or a new controller), and they can be invoked _at any point_, not just initial project creation. 

## Decision
","## Context

When starting a new project, it isn't practical to start completely from scratch, every time. We would like to have a varity of ""starting point"" projects, for different purposes.

### Lein templates
In the Clojure space, Leiningen Templates fill this purpose. These are sets of special string-interpolated files that are ""rendered"" into a working project using special tooling.

However, they have two major drawbacks:

- They only work when using Leiningen as a build tool.
- The template files are are not actually valid source files, which makes them difficult to maintain. Changes need to be manually copied over to the templates.

### Rails templates

Rails also provides a complete project templating solution. In Rails, the project template is a `template.rb` file which contains DSL forms that specify operations to perform on a fresh project. These operations include creating files, modifying a projects dependencies, adding Rake tasks, and running specific _generators_. 

Generators are particularly interesting, because the idea is that they can generate or modify stubs for files pertaining to a specific part of the application (e.g, a new model or a new controller), and they can be invoked _at any point_, not just initial project creation. ","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
When starting a new project, it isn't practical to start completely from scratch, every time. We would like to have a varity of ""starting point"" projects, for different purposes.

### Lein templates
In the Clojure space, Leiningen Templates fill this purpose. These are sets of special string-interpolated files that are ""rendered"" into a working project using special tooling.

However, they have two major drawbacks:

- They only work when using Leiningen as a build tool.
- The template files are are not actually valid source files, which makes them difficult to maintain. Changes need to be manually copied over to the templates.

### Rails templates

Rails also provides a complete project templating solution. In Rails, the project template is a `template.rb` file which contains DSL forms that specify operations to perform on a fresh project. These operations include creating files, modifying a projects dependencies, adding Rake tasks, and running specific _generators_. 

Generators are particularly interesting, because the idea is that they can generate or modify stubs for files pertaining to a specific part of the application (e.g, a new model or a new controller), and they can be invoked _at any point_, not just initial project creation. "
"A core part of the process of developing an application is making changes to its configuration. With its emphasis on configuration, this is even more true of Arachne than with most other web frameworks.

In a development context, developers will want to see these changes reflected in their running application as quickly as possible. Keeping the test/modify cycle short is an important goal.

However, accommodating change is a source of complexity. Extra code would be required to handle  ""update"" scenarios. Components are initialized with a particular configuration in hand. While it would be possible to require that every component support an `update` operation to receive an arbitrary new config, implementing this is non-trivial and would likely need to involve conditional logic to determine the ways in which the new configuration is different from the old. If any mistakes where made in the implementation of `update`, *for any component*, such that the result was not identical to a clean restart, it would be possible to put the system in an inconsistent, unreproducible state.

The ""simplest"" approach is to avoid the issue and completely discard and rebuild the Arachne runtime ([ADR-006](adr-006-core-runtime)) every time the configuration is updated. Every modification to the config would be applied via a clean start, guaranteeing reproducibility and a single code path.

However, this simple baseline approach has two major drawbacks:

1. The shutdown, initialization, and startup times of the entire set of components will be incurred every time the configuration is updated.
2. The developer will lose any application state stored in the components whenever the config is modified.

The startup and shutdown time issues are potentially problematic because of the general increase to cycle time. However, it might not be too bad depending on exactly how long it takes sub-components to start. Most commonly-used components take only a few milliseconds to rebuild and restart. This is a cost that most Component workflows absorb without too much trouble.

The second issue is more problematic. Not only is losing state a drain on overall cycle speed, it is a direct source of frustration, causing developers to repeat the same tasks over and over. It will mean that touching the configuration has a real cost, and will cause developers to be hesitant to do so.


### Prior Art

There is a library designed to solve the startup/shutdown problem, in conjunction with Component: [Suspendable](https://github.com/weavejester/suspendable). It is not an ideal fit for Arachne, since it focuses on suspending and resuming the same Component instances rather than rebuilding, but its approach may be instructive.","Whenever the configuration changes, we will use the simple approach of stopping and discarding the entire old Arachne runtime (and all its components), and starting a new one.

To mitigate the issue of lost state, Arachne will provide a new protocol called `Preservable` (name subject to change, pending a better one.) Components may optionally implement `Preservable`; it is not required. `Preservable` defines a single method, `preserve`. 

Whenever the configuration changes, the following procedure will be used:

1. Call `stop` on the old runtime.
2. Instantiate the new runtime.
3. For all components in the new runtime which implement `Preservable`, invoke the `preserve` function, passing it the corresponding component from the old runtime (if there is one).
4. The `preserve` function will selectively copy state out of the old, stopped component into the new, not-yet-started component. It should be careful not to copy any state that would be invalidated by a configuration change.
5. Call `start` on the new runtime.

Arachne will not provide a mitigation for avoiding the cost of stopping and starting individual components. If this becomes a pain point, we can explore solutions such as that offered by Suspendable.
 
","Architectural Decision Record

## Context
A core part of the process of developing an application is making changes to its configuration. With its emphasis on configuration, this is even more true of Arachne than with most other web frameworks.

In a development context, developers will want to see these changes reflected in their running application as quickly as possible. Keeping the test/modify cycle short is an important goal.

However, accommodating change is a source of complexity. Extra code would be required to handle  ""update"" scenarios. Components are initialized with a particular configuration in hand. While it would be possible to require that every component support an `update` operation to receive an arbitrary new config, implementing this is non-trivial and would likely need to involve conditional logic to determine the ways in which the new configuration is different from the old. If any mistakes where made in the implementation of `update`, *for any component*, such that the result was not identical to a clean restart, it would be possible to put the system in an inconsistent, unreproducible state.

The ""simplest"" approach is to avoid the issue and completely discard and rebuild the Arachne runtime ([ADR-006](adr-006-core-runtime)) every time the configuration is updated. Every modification to the config would be applied via a clean start, guaranteeing reproducibility and a single code path.

However, this simple baseline approach has two major drawbacks:

1. The shutdown, initialization, and startup times of the entire set of components will be incurred every time the configuration is updated.
2. The developer will lose any application state stored in the components whenever the config is modified.

The startup and shutdown time issues are potentially problematic because of the general increase to cycle time. However, it might not be too bad depending on exactly how long it takes sub-components to start. Most commonly-used components take only a few milliseconds to rebuild and restart. This is a cost that most Component workflows absorb without too much trouble.

The second issue is more problematic. Not only is losing state a drain on overall cycle speed, it is a direct source of frustration, causing developers to repeat the same tasks over and over. It will mean that touching the configuration has a real cost, and will cause developers to be hesitant to do so.


### Prior Art

There is a library designed to solve the startup/shutdown problem, in conjunction with Component: [Suspendable](https://github.com/weavejester/suspendable). It is not an ideal fit for Arachne, since it focuses on suspending and resuming the same Component instances rather than rebuilding, but its approach may be instructive.

## Decision
","## Context

A core part of the process of developing an application is making changes to its configuration. With its emphasis on configuration, this is even more true of Arachne than with most other web frameworks.

In a development context, developers will want to see these changes reflected in their running application as quickly as possible. Keeping the test/modify cycle short is an important goal.

However, accommodating change is a source of complexity. Extra code would be required to handle  ""update"" scenarios. Components are initialized with a particular configuration in hand. While it would be possible to require that every component support an `update` operation to receive an arbitrary new config, implementing this is non-trivial and would likely need to involve conditional logic to determine the ways in which the new configuration is different from the old. If any mistakes where made in the implementation of `update`, *for any component*, such that the result was not identical to a clean restart, it would be possible to put the system in an inconsistent, unreproducible state.

The ""simplest"" approach is to avoid the issue and completely discard and rebuild the Arachne runtime ([ADR-006](adr-006-core-runtime)) every time the configuration is updated. Every modification to the config would be applied via a clean start, guaranteeing reproducibility and a single code path.

However, this simple baseline approach has two major drawbacks:

1. The shutdown, initialization, and startup times of the entire set of components will be incurred every time the configuration is updated.
2. The developer will lose any application state stored in the components whenever the config is modified.

The startup and shutdown time issues are potentially problematic because of the general increase to cycle time. However, it might not be too bad depending on exactly how long it takes sub-components to start. Most commonly-used components take only a few milliseconds to rebuild and restart. This is a cost that most Component workflows absorb without too much trouble.

The second issue is more problematic. Not only is losing state a drain on overall cycle speed, it is a direct source of frustration, causing developers to repeat the same tasks over and over. It will mean that touching the configuration has a real cost, and will cause developers to be hesitant to do so.


### Prior Art

There is a library designed to solve the startup/shutdown problem, in conjunction with Component: [Suspendable](https://github.com/weavejester/suspendable). It is not an ideal fit for Arachne, since it focuses on suspending and resuming the same Component instances rather than rebuilding, but its approach may be instructive.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
A core part of the process of developing an application is making changes to its configuration. With its emphasis on configuration, this is even more true of Arachne than with most other web frameworks.

In a development context, developers will want to see these changes reflected in their running application as quickly as possible. Keeping the test/modify cycle short is an important goal.

However, accommodating change is a source of complexity. Extra code would be required to handle  ""update"" scenarios. Components are initialized with a particular configuration in hand. While it would be possible to require that every component support an `update` operation to receive an arbitrary new config, implementing this is non-trivial and would likely need to involve conditional logic to determine the ways in which the new configuration is different from the old. If any mistakes where made in the implementation of `update`, *for any component*, such that the result was not identical to a clean restart, it would be possible to put the system in an inconsistent, unreproducible state.

The ""simplest"" approach is to avoid the issue and completely discard and rebuild the Arachne runtime ([ADR-006](adr-006-core-runtime)) every time the configuration is updated. Every modification to the config would be applied via a clean start, guaranteeing reproducibility and a single code path.

However, this simple baseline approach has two major drawbacks:

1. The shutdown, initialization, and startup times of the entire set of components will be incurred every time the configuration is updated.
2. The developer will lose any application state stored in the components whenever the config is modified.

The startup and shutdown time issues are potentially problematic because of the general increase to cycle time. However, it might not be too bad depending on exactly how long it takes sub-components to start. Most commonly-used components take only a few milliseconds to rebuild and restart. This is a cost that most Component workflows absorb without too much trouble.

The second issue is more problematic. Not only is losing state a drain on overall cycle speed, it is a direct source of frustration, causing developers to repeat the same tasks over and over. It will mean that touching the configuration has a real cost, and will cause developers to be hesitant to do so.


### Prior Art

There is a library designed to solve the startup/shutdown problem, in conjunction with Component: [Suspendable](https://github.com/weavejester/suspendable). It is not an ideal fit for Arachne, since it focuses on suspending and resuming the same Component instances rather than rebuilding, but its approach may be instructive."
"The Chimera data model (as described in ADR-15 and ADR-16) includes the concepts of _entity types_ in the domain data model: a defined entity type may have supertypes, and inherits all the attributes of a given supertype 

This is quite expressive, and is a good fit for certain types of data stores (such as Datomic, graph databases, and some object stores.) It makes it possible to compose types, and re-use attributes effectively.

However, it leads to a number of conceptual problems, as well as implementation complexities. These issues include but are not limited to:

- There is a desire for some types to be ""abstract"", in that they exist purely to be extended and are not intented to be reified in the target database (e.g, as a table.) In the current model it is ambiguous whether this is the case or not.
- A singe `extend-type` migration operation may need to create multiple columns in multiple tables, which some databases do not support transactionally.
- When doing a lookup by attribute that exists in multiple types, it is ambiguous which type is intended.
- In a SQL database, how to best model an extended type becomes ambiguous: copying the column leads to ""denormalization"", which might not be desired. On the other hand, creating a separate table for the shared columns leads to more complex queries with more joins.

All of these issues can be resolved or worked around. But they add a variable amount of complexity cost to every Chimera adapter, and create a domain with large amounts of ambigous behavior that must be resolved (and which might not be discovered until writing a particular adapter.)","The concept of type extension and attribute inheritance does not provide benefits proportional to the cost.

We will remove all concept of supertypes, subtypes and attribute inheritance from Chimera's data model.

Chimera's data model will remain ""flat"". In order to achieve attribute reuse for data stores for which that is idiomatic (such as Datomic), multiple Chimera attributes can be mapped to a single DB-level attribute in the adapter mapping metadata.
   
","Architectural Decision Record

## Context
The Chimera data model (as described in ADR-15 and ADR-16) includes the concepts of _entity types_ in the domain data model: a defined entity type may have supertypes, and inherits all the attributes of a given supertype 

This is quite expressive, and is a good fit for certain types of data stores (such as Datomic, graph databases, and some object stores.) It makes it possible to compose types, and re-use attributes effectively.

However, it leads to a number of conceptual problems, as well as implementation complexities. These issues include but are not limited to:

- There is a desire for some types to be ""abstract"", in that they exist purely to be extended and are not intented to be reified in the target database (e.g, as a table.) In the current model it is ambiguous whether this is the case or not.
- A singe `extend-type` migration operation may need to create multiple columns in multiple tables, which some databases do not support transactionally.
- When doing a lookup by attribute that exists in multiple types, it is ambiguous which type is intended.
- In a SQL database, how to best model an extended type becomes ambiguous: copying the column leads to ""denormalization"", which might not be desired. On the other hand, creating a separate table for the shared columns leads to more complex queries with more joins.

All of these issues can be resolved or worked around. But they add a variable amount of complexity cost to every Chimera adapter, and create a domain with large amounts of ambigous behavior that must be resolved (and which might not be discovered until writing a particular adapter.)

## Decision
","## Context

The Chimera data model (as described in ADR-15 and ADR-16) includes the concepts of _entity types_ in the domain data model: a defined entity type may have supertypes, and inherits all the attributes of a given supertype 

This is quite expressive, and is a good fit for certain types of data stores (such as Datomic, graph databases, and some object stores.) It makes it possible to compose types, and re-use attributes effectively.

However, it leads to a number of conceptual problems, as well as implementation complexities. These issues include but are not limited to:

- There is a desire for some types to be ""abstract"", in that they exist purely to be extended and are not intented to be reified in the target database (e.g, as a table.) In the current model it is ambiguous whether this is the case or not.
- A singe `extend-type` migration operation may need to create multiple columns in multiple tables, which some databases do not support transactionally.
- When doing a lookup by attribute that exists in multiple types, it is ambiguous which type is intended.
- In a SQL database, how to best model an extended type becomes ambiguous: copying the column leads to ""denormalization"", which might not be desired. On the other hand, creating a separate table for the shared columns leads to more complex queries with more joins.

All of these issues can be resolved or worked around. But they add a variable amount of complexity cost to every Chimera adapter, and create a domain with large amounts of ambigous behavior that must be resolved (and which might not be discovered until writing a particular adapter.)","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
The Chimera data model (as described in ADR-15 and ADR-16) includes the concepts of _entity types_ in the domain data model: a defined entity type may have supertypes, and inherits all the attributes of a given supertype 

This is quite expressive, and is a good fit for certain types of data stores (such as Datomic, graph databases, and some object stores.) It makes it possible to compose types, and re-use attributes effectively.

However, it leads to a number of conceptual problems, as well as implementation complexities. These issues include but are not limited to:

- There is a desire for some types to be ""abstract"", in that they exist purely to be extended and are not intented to be reified in the target database (e.g, as a table.) In the current model it is ambiguous whether this is the case or not.
- A singe `extend-type` migration operation may need to create multiple columns in multiple tables, which some databases do not support transactionally.
- When doing a lookup by attribute that exists in multiple types, it is ambiguous which type is intended.
- In a SQL database, how to best model an extended type becomes ambiguous: copying the column leads to ""denormalization"", which might not be desired. On the other hand, creating a separate table for the shared columns leads to more complex queries with more joins.

All of these issues can be resolved or worked around. But they add a variable amount of complexity cost to every Chimera adapter, and create a domain with large amounts of ambigous behavior that must be resolved (and which might not be discovered until writing a particular adapter.)"
"While many Arachne applications will use a transient config which is rebuilt from its initialization scripts every time an instance is started, some users might wish instead to store their config persistently in a full Datomic instance.

There are a number of possible benefits to this approach:

- Deployments from the same configuration are highly reproducible
- Organizations can maintain an immutable persistent log of configuration changes over time.
- External tooling can be used to persistently build and define configurations, up to and including full ""drag and drop"" architecture or application design.

Doing this introduces a number of additional challenges:

- **Initialization Scripts**: Having a persistent configuration introduces the question of what role initialization scripts play in the setup. Merely having a persistent config does not make it easier to modify by hand - quite the opposite. While an init script could be used to create the configuration, it's not clear how they would be updated from that point (absent a full config editor UI.)
  
  Re-running a modified configuration script on an existing configuration poses challenges as well; it would require that all scripts be idempotent, so as not to create spurious objects on subsequent runs. Also, scripts would then need to support some concept of retraction.
- **Scope & Naming**: It is extremely convenient to use `:db.unique/identity` attributes to identify particular entities in a configuration and configuration init scripts. This is not only convenient, but *required* if init scripts are to be idempotent, since this is the only mechanism by which Datomic can determine that a new entity is ""the same"" as an older entity in the system.

  However, if there are multiple different configurations in the same database, there is the risk that some of these unique values might be unintentionally the same and ""collide"", causing inadvertent linkages between what ought to be logically distinct configurations.
 
  While this can be mitigated in the simple case by ensuring that every config uses its own unique namespace, it is still something to keep in mind.

- **Configuration Copying & Versioning** Although Datomic supports a full history, that history is linear. Datomic does not currently support ""forking"" or maintaining multiple concurrent versions of the same logical data set.

  This does introduce complexities when thinking about ""modifying"" a configuration, while still keeping the old one. This kind of ""fork"" would require a deep clone of all the entities in the config, *as well as* renaming all of the `:db.unique/identity` attrs.
  
  Renaming identity attributes compounds the complexity, since it implies that either idents cannot be hardcoded in initialization scripts, or the same init script cannot be used to generate or update two different configurations.

- **Environment-specific Configuration**: Some applications need slightly different configurations for different instances of the ""same"" application. For instance, some software needs to be told what its own IP address is. While it makes sense to put this data in the configuration, this means that there would no longer be a single configuration, but N distinct (yet 99% identical) configurations.

  One solution would be to not store this data in the configuration (instead picking it up at runtime from an environment variable or secondary config file), but multiplying the sources of configuration runs counter to Arachne's overriding philosophy of putting everything in the configuration to start with.
  
- **Relationship with module load process**: Would the stored configuration represent only the ""initial"" configuration, before being updated by the active modules? Or would it represent the complete configuration, after all the modules have completed their updates?

  Both alternatives present issues.
  
  If only the user-supplied, initial config is stored, then the usefulness of the stored config is diminished, since it does not provide a comprehensive, complete view of the configuration.
  
  On the other hand, if the complete, post-module config is persisted, it raises more questions. What happens if the user edits the configuration in ways that would cause modules to do something different with the config? Is it possible to run the module update process multiple times on the same config? If so, how would ""old"" or stale module-generated values be removed?

#### Goals

We need a technical approach with good answers to the challenges described above, that enables a clean user workflow. As such, it is useful to enumerate the specific activities that it would be useful for a persistent config implementation to support:

- Define a new configuration from an init script.
- Run an init script on an existing configuration, updating it.
- Edit an existing configuration using the REPL.
- Edit an existing configuration using a UI.
- Clone a configuration
- Deploy based on a specific configuration

At the same time, we need to be careful not to overly complicate things for the common case; most applications will still use the pattern of generating a configuration from an init script immediately before running an application using it.","We will not attempt to implement a concrete strategy for config persistence at this time; it runs the risk of becoming a quagmire that will halt forward momentum.

Instead, we will make a minimal set of choices and observations that will enable forward progress while preserving the ability to revisit the issue of persistent configuration at some point in the future.

1. The configuration schema itself should be compatible with having several configurations present in the same persistent database. Specifically:
  - Each logical configuration should have its own namespace, which will be used as the namespace of all `:db.unique/identity` values, ensuring their global uniqueness.
  - There is a 'configuration' entity that reifies a config, its possible root components, how it was constructed, etc.
  - The entities in a configuration must form a connected graph. That is, every entity in a configuration must be reachable from the base 'config' entity. This is required to have any ability to identify the config as a whole within for any purpose.

2. The current initial _tooling_ for building configurations (including the init scripts) will focus on building configurations from scratch. Tooling capable of ""editing"" an existing configuration is sufficiently different, with a different set of requirements and constraints, that it needs its own design process.

3. Any future tooling for storing, viewing and editing configurations will need to explicitly determine whether it wants to work with the configuration before or after processing by the modules, since there is a distinct set of tradeoffs.

","Architectural Decision Record

## Context
While many Arachne applications will use a transient config which is rebuilt from its initialization scripts every time an instance is started, some users might wish instead to store their config persistently in a full Datomic instance.

There are a number of possible benefits to this approach:

- Deployments from the same configuration are highly reproducible
- Organizations can maintain an immutable persistent log of configuration changes over time.
- External tooling can be used to persistently build and define configurations, up to and including full ""drag and drop"" architecture or application design.

Doing this introduces a number of additional challenges:

- **Initialization Scripts**: Having a persistent configuration introduces the question of what role initialization scripts play in the setup. Merely having a persistent config does not make it easier to modify by hand - quite the opposite. While an init script could be used to create the configuration, it's not clear how they would be updated from that point (absent a full config editor UI.)
  
  Re-running a modified configuration script on an existing configuration poses challenges as well; it would require that all scripts be idempotent, so as not to create spurious objects on subsequent runs. Also, scripts would then need to support some concept of retraction.
- **Scope & Naming**: It is extremely convenient to use `:db.unique/identity` attributes to identify particular entities in a configuration and configuration init scripts. This is not only convenient, but *required* if init scripts are to be idempotent, since this is the only mechanism by which Datomic can determine that a new entity is ""the same"" as an older entity in the system.

  However, if there are multiple different configurations in the same database, there is the risk that some of these unique values might be unintentionally the same and ""collide"", causing inadvertent linkages between what ought to be logically distinct configurations.
 
  While this can be mitigated in the simple case by ensuring that every config uses its own unique namespace, it is still something to keep in mind.

- **Configuration Copying & Versioning** Although Datomic supports a full history, that history is linear. Datomic does not currently support ""forking"" or maintaining multiple concurrent versions of the same logical data set.

  This does introduce complexities when thinking about ""modifying"" a configuration, while still keeping the old one. This kind of ""fork"" would require a deep clone of all the entities in the config, *as well as* renaming all of the `:db.unique/identity` attrs.
  
  Renaming identity attributes compounds the complexity, since it implies that either idents cannot be hardcoded in initialization scripts, or the same init script cannot be used to generate or update two different configurations.

- **Environment-specific Configuration**: Some applications need slightly different configurations for different instances of the ""same"" application. For instance, some software needs to be told what its own IP address is. While it makes sense to put this data in the configuration, this means that there would no longer be a single configuration, but N distinct (yet 99% identical) configurations.

  One solution would be to not store this data in the configuration (instead picking it up at runtime from an environment variable or secondary config file), but multiplying the sources of configuration runs counter to Arachne's overriding philosophy of putting everything in the configuration to start with.
  
- **Relationship with module load process**: Would the stored configuration represent only the ""initial"" configuration, before being updated by the active modules? Or would it represent the complete configuration, after all the modules have completed their updates?

  Both alternatives present issues.
  
  If only the user-supplied, initial config is stored, then the usefulness of the stored config is diminished, since it does not provide a comprehensive, complete view of the configuration.
  
  On the other hand, if the complete, post-module config is persisted, it raises more questions. What happens if the user edits the configuration in ways that would cause modules to do something different with the config? Is it possible to run the module update process multiple times on the same config? If so, how would ""old"" or stale module-generated values be removed?

#### Goals

We need a technical approach with good answers to the challenges described above, that enables a clean user workflow. As such, it is useful to enumerate the specific activities that it would be useful for a persistent config implementation to support:

- Define a new configuration from an init script.
- Run an init script on an existing configuration, updating it.
- Edit an existing configuration using the REPL.
- Edit an existing configuration using a UI.
- Clone a configuration
- Deploy based on a specific configuration

At the same time, we need to be careful not to overly complicate things for the common case; most applications will still use the pattern of generating a configuration from an init script immediately before running an application using it.

## Decision
","## Context

While many Arachne applications will use a transient config which is rebuilt from its initialization scripts every time an instance is started, some users might wish instead to store their config persistently in a full Datomic instance.

There are a number of possible benefits to this approach:

- Deployments from the same configuration are highly reproducible
- Organizations can maintain an immutable persistent log of configuration changes over time.
- External tooling can be used to persistently build and define configurations, up to and including full ""drag and drop"" architecture or application design.

Doing this introduces a number of additional challenges:

- **Initialization Scripts**: Having a persistent configuration introduces the question of what role initialization scripts play in the setup. Merely having a persistent config does not make it easier to modify by hand - quite the opposite. While an init script could be used to create the configuration, it's not clear how they would be updated from that point (absent a full config editor UI.)
  
  Re-running a modified configuration script on an existing configuration poses challenges as well; it would require that all scripts be idempotent, so as not to create spurious objects on subsequent runs. Also, scripts would then need to support some concept of retraction.
- **Scope & Naming**: It is extremely convenient to use `:db.unique/identity` attributes to identify particular entities in a configuration and configuration init scripts. This is not only convenient, but *required* if init scripts are to be idempotent, since this is the only mechanism by which Datomic can determine that a new entity is ""the same"" as an older entity in the system.

  However, if there are multiple different configurations in the same database, there is the risk that some of these unique values might be unintentionally the same and ""collide"", causing inadvertent linkages between what ought to be logically distinct configurations.
 
  While this can be mitigated in the simple case by ensuring that every config uses its own unique namespace, it is still something to keep in mind.

- **Configuration Copying & Versioning** Although Datomic supports a full history, that history is linear. Datomic does not currently support ""forking"" or maintaining multiple concurrent versions of the same logical data set.

  This does introduce complexities when thinking about ""modifying"" a configuration, while still keeping the old one. This kind of ""fork"" would require a deep clone of all the entities in the config, *as well as* renaming all of the `:db.unique/identity` attrs.
  
  Renaming identity attributes compounds the complexity, since it implies that either idents cannot be hardcoded in initialization scripts, or the same init script cannot be used to generate or update two different configurations.

- **Environment-specific Configuration**: Some applications need slightly different configurations for different instances of the ""same"" application. For instance, some software needs to be told what its own IP address is. While it makes sense to put this data in the configuration, this means that there would no longer be a single configuration, but N distinct (yet 99% identical) configurations.

  One solution would be to not store this data in the configuration (instead picking it up at runtime from an environment variable or secondary config file), but multiplying the sources of configuration runs counter to Arachne's overriding philosophy of putting everything in the configuration to start with.
  
- **Relationship with module load process**: Would the stored configuration represent only the ""initial"" configuration, before being updated by the active modules? Or would it represent the complete configuration, after all the modules have completed their updates?

  Both alternatives present issues.
  
  If only the user-supplied, initial config is stored, then the usefulness of the stored config is diminished, since it does not provide a comprehensive, complete view of the configuration.
  
  On the other hand, if the complete, post-module config is persisted, it raises more questions. What happens if the user edits the configuration in ways that would cause modules to do something different with the config? Is it possible to run the module update process multiple times on the same config? If so, how would ""old"" or stale module-generated values be removed?

#### Goals

We need a technical approach with good answers to the challenges described above, that enables a clean user workflow. As such, it is useful to enumerate the specific activities that it would be useful for a persistent config implementation to support:

- Define a new configuration from an init script.
- Run an init script on an existing configuration, updating it.
- Edit an existing configuration using the REPL.
- Edit an existing configuration using a UI.
- Clone a configuration
- Deploy based on a specific configuration

At the same time, we need to be careful not to overly complicate things for the common case; most applications will still use the pattern of generating a configuration from an init script immediately before running an application using it.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
While many Arachne applications will use a transient config which is rebuilt from its initialization scripts every time an instance is started, some users might wish instead to store their config persistently in a full Datomic instance.

There are a number of possible benefits to this approach:

- Deployments from the same configuration are highly reproducible
- Organizations can maintain an immutable persistent log of configuration changes over time.
- External tooling can be used to persistently build and define configurations, up to and including full ""drag and drop"" architecture or application design.

Doing this introduces a number of additional challenges:

- **Initialization Scripts**: Having a persistent configuration introduces the question of what role initialization scripts play in the setup. Merely having a persistent config does not make it easier to modify by hand - quite the opposite. While an init script could be used to create the configuration, it's not clear how they would be updated from that point (absent a full config editor UI.)
  
  Re-running a modified configuration script on an existing configuration poses challenges as well; it would require that all scripts be idempotent, so as not to create spurious objects on subsequent runs. Also, scripts would then need to support some concept of retraction.
- **Scope & Naming**: It is extremely convenient to use `:db.unique/identity` attributes to identify particular entities in a configuration and configuration init scripts. This is not only convenient, but *required* if init scripts are to be idempotent, since this is the only mechanism by which Datomic can determine that a new entity is ""the same"" as an older entity in the system.

  However, if there are multiple different configurations in the same database, there is the risk that some of these unique values might be unintentionally the same and ""collide"", causing inadvertent linkages between what ought to be logically distinct configurations.
 
  While this can be mitigated in the simple case by ensuring that every config uses its own unique namespace, it is still something to keep in mind.

- **Configuration Copying & Versioning** Although Datomic supports a full history, that history is linear. Datomic does not currently support ""forking"" or maintaining multiple concurrent versions of the same logical data set.

  This does introduce complexities when thinking about ""modifying"" a configuration, while still keeping the old one. This kind of ""fork"" would require a deep clone of all the entities in the config, *as well as* renaming all of the `:db.unique/identity` attrs.
  
  Renaming identity attributes compounds the complexity, since it implies that either idents cannot be hardcoded in initialization scripts, or the same init script cannot be used to generate or update two different configurations.

- **Environment-specific Configuration**: Some applications need slightly different configurations for different instances of the ""same"" application. For instance, some software needs to be told what its own IP address is. While it makes sense to put this data in the configuration, this means that there would no longer be a single configuration, but N distinct (yet 99% identical) configurations.

  One solution would be to not store this data in the configuration (instead picking it up at runtime from an environment variable or secondary config file), but multiplying the sources of configuration runs counter to Arachne's overriding philosophy of putting everything in the configuration to start with.
  
- **Relationship with module load process**: Would the stored configuration represent only the ""initial"" configuration, before being updated by the active modules? Or would it represent the complete configuration, after all the modules have completed their updates?

  Both alternatives present issues.
  
  If only the user-supplied, initial config is stored, then the usefulness of the stored config is diminished, since it does not provide a comprehensive, complete view of the configuration.
  
  On the other hand, if the complete, post-module config is persisted, it raises more questions. What happens if the user edits the configuration in ways that would cause modules to do something different with the config? Is it possible to run the module update process multiple times on the same config? If so, how would ""old"" or stale module-generated values be removed?

#### Goals

We need a technical approach with good answers to the challenges described above, that enables a clean user workflow. As such, it is useful to enumerate the specific activities that it would be useful for a persistent config implementation to support:

- Define a new configuration from an init script.
- Run an init script on an existing configuration, updating it.
- Edit an existing configuration using the REPL.
- Edit an existing configuration using a UI.
- Clone a configuration
- Deploy based on a specific configuration

At the same time, we need to be careful not to overly complicate things for the common case; most applications will still use the pattern of generating a configuration from an init script immediately before running an application using it."
"Arachne has a number of goals.

1. It needs to be *modular*. Different software packages, written by
   different developers, should be usable and swappable in the same
   application with a minimum of effort.

2. Arachne applications need to be *transparent* and
   *introspectable*. It should always be as clear as possible what is
   going on at any given moment, and why the application is behaving
   in the way it does.

3. As a general-purpose web framework, it needs to provide a strong
   set of default settings which are also highly overridable, and
   *configurable* to suit the unique needs of users.


Also, it is a good development practice (particularly in Clojure) to
code to a specific information model (that is, data) rather than to
particular functions or APIs. Along with other benefits, this helps
separate (avoids ""complecting"") the intended operation and its
implementation.

Documenting the full rationale for this ""data first"" philosophy is
beyond the scope of this document, but some resources that explain it (among other things) are:

- [Simple Made Easy](http://www.infoq.com/presentations/Simple-Made-Easy) - Rich Hickey
- [Narcissistic Design](https://vimeo.com/77199361) - Stuart Halloway
- [Data Beats Functions](https://malcolmsparks.com/posts/data-beats-functions.html) - Malcolm Sparks
- [Always Be Composing](https://www.youtube.com/watch?v=3oQTSP4FngY) - Zach Tellman
- [Data > Functions > Macros](http://www.lispcast.com/data-functions-macros-why) - Eric Normand

Finally, one weakness of many existing Clojure libraries, especially
web development libraries, is the way in which they overload the
Clojure runtime (particularly vars and reified namespaces) to store
information about the webapp. Because both the Clojure runtime and
many web application entities (e.g servers) are stateful, this causes
a variety of issues, particularly with reloading namespaces. Therefore,
as much as possible, we would like to avoid entangling information
about an Arachne application with the Clojure runtime itself.","Arachne will take the ""everything is data"" philosophy to its logical
extreme, and encode as much information about the application as
possible in a single, highly general data structure. This will include
not just data that is normally thought of as ""config"" data, but the
structure and definition of the application itself. Everything that
does not have to be arbitrary executable code will be
reflected in the application config value.

Some concrete examples include (but are not limited to):

- Dependency injection components
- Runtime entities (servers, caches, connections, pools, etc)
- HTTP routes and middleware
- Persistence schemas and migrations
- Locations of static and dynamic assets

This configuration value will have a *schema* that defines what types
of entities can exist in the configuration, and what their expected
properties are.

Each distinct module will have the ability to contribute to the schema
and define entity types specific to its own domain. Modules may
interact by referencing entity types and properties defined in other
modules.

Although it has much in common with a fully general in-memory
database, the configuration value will be a single immutable value,
not a stateful data store. This will avoid many of the complexities
of state and change, and will eliminate the temptation to use the
configuration itself as dynamic storage for runtime data.

","Architectural Decision Record

## Context
Arachne has a number of goals.

1. It needs to be *modular*. Different software packages, written by
   different developers, should be usable and swappable in the same
   application with a minimum of effort.

2. Arachne applications need to be *transparent* and
   *introspectable*. It should always be as clear as possible what is
   going on at any given moment, and why the application is behaving
   in the way it does.

3. As a general-purpose web framework, it needs to provide a strong
   set of default settings which are also highly overridable, and
   *configurable* to suit the unique needs of users.


Also, it is a good development practice (particularly in Clojure) to
code to a specific information model (that is, data) rather than to
particular functions or APIs. Along with other benefits, this helps
separate (avoids ""complecting"") the intended operation and its
implementation.

Documenting the full rationale for this ""data first"" philosophy is
beyond the scope of this document, but some resources that explain it (among other things) are:

- [Simple Made Easy](http://www.infoq.com/presentations/Simple-Made-Easy) - Rich Hickey
- [Narcissistic Design](https://vimeo.com/77199361) - Stuart Halloway
- [Data Beats Functions](https://malcolmsparks.com/posts/data-beats-functions.html) - Malcolm Sparks
- [Always Be Composing](https://www.youtube.com/watch?v=3oQTSP4FngY) - Zach Tellman
- [Data > Functions > Macros](http://www.lispcast.com/data-functions-macros-why) - Eric Normand

Finally, one weakness of many existing Clojure libraries, especially
web development libraries, is the way in which they overload the
Clojure runtime (particularly vars and reified namespaces) to store
information about the webapp. Because both the Clojure runtime and
many web application entities (e.g servers) are stateful, this causes
a variety of issues, particularly with reloading namespaces. Therefore,
as much as possible, we would like to avoid entangling information
about an Arachne application with the Clojure runtime itself.

## Decision
","## Context

Arachne has a number of goals.

1. It needs to be *modular*. Different software packages, written by
   different developers, should be usable and swappable in the same
   application with a minimum of effort.

2. Arachne applications need to be *transparent* and
   *introspectable*. It should always be as clear as possible what is
   going on at any given moment, and why the application is behaving
   in the way it does.

3. As a general-purpose web framework, it needs to provide a strong
   set of default settings which are also highly overridable, and
   *configurable* to suit the unique needs of users.


Also, it is a good development practice (particularly in Clojure) to
code to a specific information model (that is, data) rather than to
particular functions or APIs. Along with other benefits, this helps
separate (avoids ""complecting"") the intended operation and its
implementation.

Documenting the full rationale for this ""data first"" philosophy is
beyond the scope of this document, but some resources that explain it (among other things) are:

- [Simple Made Easy](http://www.infoq.com/presentations/Simple-Made-Easy) - Rich Hickey
- [Narcissistic Design](https://vimeo.com/77199361) - Stuart Halloway
- [Data Beats Functions](https://malcolmsparks.com/posts/data-beats-functions.html) - Malcolm Sparks
- [Always Be Composing](https://www.youtube.com/watch?v=3oQTSP4FngY) - Zach Tellman
- [Data > Functions > Macros](http://www.lispcast.com/data-functions-macros-why) - Eric Normand

Finally, one weakness of many existing Clojure libraries, especially
web development libraries, is the way in which they overload the
Clojure runtime (particularly vars and reified namespaces) to store
information about the webapp. Because both the Clojure runtime and
many web application entities (e.g servers) are stateful, this causes
a variety of issues, particularly with reloading namespaces. Therefore,
as much as possible, we would like to avoid entangling information
about an Arachne application with the Clojure runtime itself.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Arachne has a number of goals.

1. It needs to be *modular*. Different software packages, written by
   different developers, should be usable and swappable in the same
   application with a minimum of effort.

2. Arachne applications need to be *transparent* and
   *introspectable*. It should always be as clear as possible what is
   going on at any given moment, and why the application is behaving
   in the way it does.

3. As a general-purpose web framework, it needs to provide a strong
   set of default settings which are also highly overridable, and
   *configurable* to suit the unique needs of users.


Also, it is a good development practice (particularly in Clojure) to
code to a specific information model (that is, data) rather than to
particular functions or APIs. Along with other benefits, this helps
separate (avoids ""complecting"") the intended operation and its
implementation.

Documenting the full rationale for this ""data first"" philosophy is
beyond the scope of this document, but some resources that explain it (among other things) are:

- [Simple Made Easy](http://www.infoq.com/presentations/Simple-Made-Easy) - Rich Hickey
- [Narcissistic Design](https://vimeo.com/77199361) - Stuart Halloway
- [Data Beats Functions](https://malcolmsparks.com/posts/data-beats-functions.html) - Malcolm Sparks
- [Always Be Composing](https://www.youtube.com/watch?v=3oQTSP4FngY) - Zach Tellman
- [Data > Functions > Macros](http://www.lispcast.com/data-functions-macros-why) - Eric Normand

Finally, one weakness of many existing Clojure libraries, especially
web development libraries, is the way in which they overload the
Clojure runtime (particularly vars and reified namespaces) to store
information about the webapp. Because both the Clojure runtime and
many web application entities (e.g servers) are stateful, this causes
a variety of issues, particularly with reloading namespaces. Therefore,
as much as possible, we would like to avoid entangling information
about an Arachne application with the Clojure runtime itself."
"As much as possible, an Arachne application should be defined by its configuration. If something is wrong with the configuration, there is no way that an application can be expected to work correctly.

Therefore, it is desirable to validate that a configuration is correct to the greatest extent possible, at the earliest possible moment. This is important for two distinct reasons:

- Ease of use and developer friendliness. Config validation can return helpful errors that point out exactly what's wrong instead of deep failures with lengthy debug sessions.
- Program correctness. Some types of errors in configs might not be discovered at all during testing or development, and aggressively failing on invalid configs will prevent those issues from affecting end users in production.

There are two ""kinds"" of config validation.

The first is ensuring that a configuration as data is structurally correct; that it adheres to its own schema. This includes validating types and cardinalities as expressed by the Arachne's core ontology system.

The second is ensuring that the Arachne Runtime constructed from a given configuration is correct; that the runtime component instances returned by component constructors are of the correct type and likely to work.","Arachne will perform both kinds of validation. To disambiguate them (since they are logically distinct), we will term the structural/schema validation ""configuration validation"", while the validation of the runtime objects will be ""runtime validation.""

Both styles of validation should be extensible by modules, so modules can specify additional validations, where necessary.

#### Configuration Validation

Configuration validation is ensuring that an Arachne configuration object is consistent with itself and with its schema.

Because this is ultimately validating a set of Datomic style `eavt` tuples, the natural form for checking tuple data is Datalog queries and query rules, to search for and locate data that is ""incorrect."" 

Each logical validation will have its own ""validator"", a function which takes a config, queries it, and either returns or throws an exception. To validate a config, it is passed through every validator as the final step of building a module.

The set of validators is open, and defined in the configuration itself. To add new validators, a module can transact entities for them during its configuration building phase.

#### Runtime Validation

Runtime validation occurs after a runtime is instantiated, but before it is started. Validation happens on the component level; each component may be subject to validation.

Unlike Configuration validation, Runtime validation uses Spec. What specs should be applied to each component are defined in the configuration using a keyword-valued attribute. Specs may be defined on individual component entities, or to the *type* of a component entity. When a component is validated, it is validated using all the specs defined for it or any of its supertypes.

","Architectural Decision Record

## Context
As much as possible, an Arachne application should be defined by its configuration. If something is wrong with the configuration, there is no way that an application can be expected to work correctly.

Therefore, it is desirable to validate that a configuration is correct to the greatest extent possible, at the earliest possible moment. This is important for two distinct reasons:

- Ease of use and developer friendliness. Config validation can return helpful errors that point out exactly what's wrong instead of deep failures with lengthy debug sessions.
- Program correctness. Some types of errors in configs might not be discovered at all during testing or development, and aggressively failing on invalid configs will prevent those issues from affecting end users in production.

There are two ""kinds"" of config validation.

The first is ensuring that a configuration as data is structurally correct; that it adheres to its own schema. This includes validating types and cardinalities as expressed by the Arachne's core ontology system.

The second is ensuring that the Arachne Runtime constructed from a given configuration is correct; that the runtime component instances returned by component constructors are of the correct type and likely to work.

## Decision
","## Context

As much as possible, an Arachne application should be defined by its configuration. If something is wrong with the configuration, there is no way that an application can be expected to work correctly.

Therefore, it is desirable to validate that a configuration is correct to the greatest extent possible, at the earliest possible moment. This is important for two distinct reasons:

- Ease of use and developer friendliness. Config validation can return helpful errors that point out exactly what's wrong instead of deep failures with lengthy debug sessions.
- Program correctness. Some types of errors in configs might not be discovered at all during testing or development, and aggressively failing on invalid configs will prevent those issues from affecting end users in production.

There are two ""kinds"" of config validation.

The first is ensuring that a configuration as data is structurally correct; that it adheres to its own schema. This includes validating types and cardinalities as expressed by the Arachne's core ontology system.

The second is ensuring that the Arachne Runtime constructed from a given configuration is correct; that the runtime component instances returned by component constructors are of the correct type and likely to work.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
As much as possible, an Arachne application should be defined by its configuration. If something is wrong with the configuration, there is no way that an application can be expected to work correctly.

Therefore, it is desirable to validate that a configuration is correct to the greatest extent possible, at the earliest possible moment. This is important for two distinct reasons:

- Ease of use and developer friendliness. Config validation can return helpful errors that point out exactly what's wrong instead of deep failures with lengthy debug sessions.
- Program correctness. Some types of errors in configs might not be discovered at all during testing or development, and aggressively failing on invalid configs will prevent those issues from affecting end users in production.

There are two ""kinds"" of config validation.

The first is ensuring that a configuration as data is structurally correct; that it adheres to its own schema. This includes validating types and cardinalities as expressed by the Arachne's core ontology system.

The second is ensuring that the Arachne Runtime constructed from a given configuration is correct; that the runtime component instances returned by component constructors are of the correct type and likely to work."
"One design goal of Arachne is to have modules be relatively easily swappable. Users should not be permanently committed to particular technical choices, but instead should have some flexibility in choosing their preferred tech, as long as it exists in the form of an Arachne module.

Some examples of the alternative implementations that people might wish to use for various parts of their application:

- HTTP Server: Pedestal or Ring
- Database: Datomic, an RDBMS or one of many NoSQL options.
- HTML Templating: Hiccup, Enlive, StringTemplate, etc.
- Client-side code: ClojureScript, CoffeeScript, Elm, etc.
- Authentication: Password-based, OpenID, Facebook, Google, etc.
- Emailing: SMTP, one of many third-party services.

This is only a representative sample; the actual list is unbounded.

The need for this kind of flexibility raises some design concerns: 

**Capability**. Users should always be able to leverage the full power of their chosen technology. That is, they should not have to code to the ""least common denominator"" of capability. If they use Datomic Pro, for example, they should be able to write Datalog and fully utilize the in-process Peer model, not be restricted to an anemic ""ORM"" that is also compatible with RDBMSs.

**Uniformity**. At tension with capability is the desire for uniformity; where the feature set of two alternatives is *not* particularly distinct, it is desirable to use a common API, so that implementations can be swapped out with little or no effort. For example, the user-facing API for sending a single email should (probably) not care whether it is ultimately sent via a local Sendmail server or a third-party service.

**Composition**. Modules should also *compose* as much as possible, and they should be as general as possible in their dependencies to maximize the number of compatible modules. In this situation, it is actually desirable to have a ""least common denominator"" that modules can have a dependency on, rather than depending on specific implementations. For example, many modules will need to persist data and ultimately will need to work in projects that use Datomic or SQL. Rather than providing multiple versions, one for Datomic users and another for SQL, it would be ideal if they could code against a common persistence abstraction, and therefore be usable in *any* project with a persistence layer.

### What does it mean to use a module?

The following list enumerates the ways in which it is possible to ""use"" a module, either from a user application or from another module. (See [ADR-004](ADR-004-module-loading.md)).

1. You can call code that the module provides (the same as any Clojure library.)
2. You can extend a protocol that the module provides (the same as any Clojure library.)
3. You can read the attributes defined in the module from the configuration.
4. You can write configuration data using the attributes defined in the module.

These tools allow the definition of modules with many different kinds of relationships to each other. Speaking loosely, these relationships can correspond to other well-known patterns in software development including composition, mixins, interface/implementation, inheritance, etc.","In order to simultaneously meet the needs for capability, uniformity and composition, Arachne's core modules will (as appropriate) use the pattern of *abstract modules*.

Abstract modules define certain attributes (and possibly also corresponding init script DSLs) that describe entities in a particular domain, *without* providing any runtime implementation which uses them. Then, other modules can ""implement"" the abstract module, reading the abstract entities and doing something concrete with them at runtime, as well as defining their own more specific attributes.

In this way, user applications and dependent modules can rely either on the common, abstract module or the specific, concrete module as appropriate. Coding against the abstract module will yield a more generic ""least common denominator"" experience, while coding against a specific implementor will give more access to the unique distinguishing features of that particular technology, at the cost of generality.

Similar relationships should hold in the library code which modules expose (if any.) An abstract module, for example, would be free to define a protocol, intended to be implemented concretely by code in an implementing module.

This pattern is fully extensible; it isn't limited to a single level of abstraction. An abstract module could itself be a narrowing or refinement of another, even more general abstract module.

### Concrete Example

As mentioned above, Arachne would like to support both Ring and Pedestal as HTTP servers. Both systems have a number of things in common:

- The concept of a ""server"" running on a port.
- The concept of a URL path/route
- The concept of a terminal ""handler"" function which receives a request and returns a response.

They also have some key differences:

- Ring composes ""middleware"" functions, whereas Pedestal uses ""interceptor"" objects
- Asynchronous responses are handled differently

Therefore, it makes sense to define an abstract HTTP module which defines the basic domain concepts; servers, routes, handlers, etc. Many dependent modules and applications will be able to make real use of this subset.

Then, there will be the two modules which provide concrete implementations; one for Pedestal, one for Ring. These will contain the code that actually reads the configuration, and at runtime builds appropriate routing tables, starts server instances, etc. Applications which wish to make direct use of a specific feature like Pedestal interceptors may freely do so, using attributes defined by the Pedestal module.

","Architectural Decision Record

## Context
One design goal of Arachne is to have modules be relatively easily swappable. Users should not be permanently committed to particular technical choices, but instead should have some flexibility in choosing their preferred tech, as long as it exists in the form of an Arachne module.

Some examples of the alternative implementations that people might wish to use for various parts of their application:

- HTTP Server: Pedestal or Ring
- Database: Datomic, an RDBMS or one of many NoSQL options.
- HTML Templating: Hiccup, Enlive, StringTemplate, etc.
- Client-side code: ClojureScript, CoffeeScript, Elm, etc.
- Authentication: Password-based, OpenID, Facebook, Google, etc.
- Emailing: SMTP, one of many third-party services.

This is only a representative sample; the actual list is unbounded.

The need for this kind of flexibility raises some design concerns: 

**Capability**. Users should always be able to leverage the full power of their chosen technology. That is, they should not have to code to the ""least common denominator"" of capability. If they use Datomic Pro, for example, they should be able to write Datalog and fully utilize the in-process Peer model, not be restricted to an anemic ""ORM"" that is also compatible with RDBMSs.

**Uniformity**. At tension with capability is the desire for uniformity; where the feature set of two alternatives is *not* particularly distinct, it is desirable to use a common API, so that implementations can be swapped out with little or no effort. For example, the user-facing API for sending a single email should (probably) not care whether it is ultimately sent via a local Sendmail server or a third-party service.

**Composition**. Modules should also *compose* as much as possible, and they should be as general as possible in their dependencies to maximize the number of compatible modules. In this situation, it is actually desirable to have a ""least common denominator"" that modules can have a dependency on, rather than depending on specific implementations. For example, many modules will need to persist data and ultimately will need to work in projects that use Datomic or SQL. Rather than providing multiple versions, one for Datomic users and another for SQL, it would be ideal if they could code against a common persistence abstraction, and therefore be usable in *any* project with a persistence layer.

### What does it mean to use a module?

The following list enumerates the ways in which it is possible to ""use"" a module, either from a user application or from another module. (See [ADR-004](ADR-004-module-loading.md)).

1. You can call code that the module provides (the same as any Clojure library.)
2. You can extend a protocol that the module provides (the same as any Clojure library.)
3. You can read the attributes defined in the module from the configuration.
4. You can write configuration data using the attributes defined in the module.

These tools allow the definition of modules with many different kinds of relationships to each other. Speaking loosely, these relationships can correspond to other well-known patterns in software development including composition, mixins, interface/implementation, inheritance, etc.

## Decision
","## Context

One design goal of Arachne is to have modules be relatively easily swappable. Users should not be permanently committed to particular technical choices, but instead should have some flexibility in choosing their preferred tech, as long as it exists in the form of an Arachne module.

Some examples of the alternative implementations that people might wish to use for various parts of their application:

- HTTP Server: Pedestal or Ring
- Database: Datomic, an RDBMS or one of many NoSQL options.
- HTML Templating: Hiccup, Enlive, StringTemplate, etc.
- Client-side code: ClojureScript, CoffeeScript, Elm, etc.
- Authentication: Password-based, OpenID, Facebook, Google, etc.
- Emailing: SMTP, one of many third-party services.

This is only a representative sample; the actual list is unbounded.

The need for this kind of flexibility raises some design concerns: 

**Capability**. Users should always be able to leverage the full power of their chosen technology. That is, they should not have to code to the ""least common denominator"" of capability. If they use Datomic Pro, for example, they should be able to write Datalog and fully utilize the in-process Peer model, not be restricted to an anemic ""ORM"" that is also compatible with RDBMSs.

**Uniformity**. At tension with capability is the desire for uniformity; where the feature set of two alternatives is *not* particularly distinct, it is desirable to use a common API, so that implementations can be swapped out with little or no effort. For example, the user-facing API for sending a single email should (probably) not care whether it is ultimately sent via a local Sendmail server or a third-party service.

**Composition**. Modules should also *compose* as much as possible, and they should be as general as possible in their dependencies to maximize the number of compatible modules. In this situation, it is actually desirable to have a ""least common denominator"" that modules can have a dependency on, rather than depending on specific implementations. For example, many modules will need to persist data and ultimately will need to work in projects that use Datomic or SQL. Rather than providing multiple versions, one for Datomic users and another for SQL, it would be ideal if they could code against a common persistence abstraction, and therefore be usable in *any* project with a persistence layer.

### What does it mean to use a module?

The following list enumerates the ways in which it is possible to ""use"" a module, either from a user application or from another module. (See [ADR-004](ADR-004-module-loading.md)).

1. You can call code that the module provides (the same as any Clojure library.)
2. You can extend a protocol that the module provides (the same as any Clojure library.)
3. You can read the attributes defined in the module from the configuration.
4. You can write configuration data using the attributes defined in the module.

These tools allow the definition of modules with many different kinds of relationships to each other. Speaking loosely, these relationships can correspond to other well-known patterns in software development including composition, mixins, interface/implementation, inheritance, etc.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
One design goal of Arachne is to have modules be relatively easily swappable. Users should not be permanently committed to particular technical choices, but instead should have some flexibility in choosing their preferred tech, as long as it exists in the form of an Arachne module.

Some examples of the alternative implementations that people might wish to use for various parts of their application:

- HTTP Server: Pedestal or Ring
- Database: Datomic, an RDBMS or one of many NoSQL options.
- HTML Templating: Hiccup, Enlive, StringTemplate, etc.
- Client-side code: ClojureScript, CoffeeScript, Elm, etc.
- Authentication: Password-based, OpenID, Facebook, Google, etc.
- Emailing: SMTP, one of many third-party services.

This is only a representative sample; the actual list is unbounded.

The need for this kind of flexibility raises some design concerns: 

**Capability**. Users should always be able to leverage the full power of their chosen technology. That is, they should not have to code to the ""least common denominator"" of capability. If they use Datomic Pro, for example, they should be able to write Datalog and fully utilize the in-process Peer model, not be restricted to an anemic ""ORM"" that is also compatible with RDBMSs.

**Uniformity**. At tension with capability is the desire for uniformity; where the feature set of two alternatives is *not* particularly distinct, it is desirable to use a common API, so that implementations can be swapped out with little or no effort. For example, the user-facing API for sending a single email should (probably) not care whether it is ultimately sent via a local Sendmail server or a third-party service.

**Composition**. Modules should also *compose* as much as possible, and they should be as general as possible in their dependencies to maximize the number of compatible modules. In this situation, it is actually desirable to have a ""least common denominator"" that modules can have a dependency on, rather than depending on specific implementations. For example, many modules will need to persist data and ultimately will need to work in projects that use Datomic or SQL. Rather than providing multiple versions, one for Datomic users and another for SQL, it would be ideal if they could code against a common persistence abstraction, and therefore be usable in *any* project with a persistence layer.

### What does it mean to use a module?

The following list enumerates the ways in which it is possible to ""use"" a module, either from a user application or from another module. (See [ADR-004](ADR-004-module-loading.md)).

1. You can call code that the module provides (the same as any Clojure library.)
2. You can extend a protocol that the module provides (the same as any Clojure library.)
3. You can read the attributes defined in the module from the configuration.
4. You can write configuration data using the attributes defined in the module.

These tools allow the definition of modules with many different kinds of relationships to each other. Speaking loosely, these relationships can correspond to other well-known patterns in software development including composition, mixins, interface/implementation, inheritance, etc."
"Arachne needs to be as modular as possible. Not only do we want the
community to be able to contribute new abilities and features that
integrate well with the core and with eachother, we want some of the
basic functionality of Arachne to be swappable for alternatives as
well.

[ADR-002](adr-002-configuration.md) specifies that one role of modules
is to contribute schema to the application config. Other roles of
modules would include providing code (as any library does), and
querying and updating the config during the startup
process. Additionally, since modules can depend upon each other, they
must specify which modules they depend upon.

Ideally there will be as little overhead as possible for creating and
consuming modules.

Some of the general problems associated with plugin/module systems include:

- Finding and downloading the implementation of the module.
- Discovering and activating the correct set of installed modules.
- Managing module versions and dependencies.

There are some existing systems for modularity in the Java
ecosystem. The most notable is OSGi, which provides not only a module
system addressing the concerns above, but also service runtime with
classpath isolation, dynamic loading and unloading and lazy
activation.

OSGi (and other systems of comparable scope) are overkill for
Arachne. Although they come with benefits, they are very heavyweight
and carry a high complexity burden, not just for Arachne development
but also for end users. Specifically, Arachne applications will be
drastically simpler if (at runtime) they exist as a straightforward
codebase in a single classloader space. Features like lazy loading and
dynamic start-stop are likewise out of scope; the goal is for an
Arachne runtime itself to be lightweight enough that starting and
stopping when modules change is not an issue.","Arachne will not be responsible for packaging, distribution or
downloading of modules. These jobs will be delegated to an external
dependency management & packaging tool. Initially, that tool will be
Maven/Leiningen/Boot, or some other tool that works with Maven
artifact repositories, since that is currently the standard for JVM
projects.

Modules that have a dependency on another module must specify a
dependency using Maven (or other dependency management tool.)

Arachne will provide no versioning system beyond what the packaging
tool provides.

Each module JAR will contain a special `arachne-modules.edn` file at
the root of its classpath. This data file (when read) contains a
sequence of *module definition maps*.

Each module definition map contains the following information:

- The formal name of the module (as a namespaced symbol.)
- A list of dependencies of the module (as a set of namespaced
  symbols.) Module dependencies must form a directed acyclic graph;
  circular dependencies are not allowed.
- A namespace qualified symbol that resolves to the module's *schema
  function.* A schema function is a function with no arguments that
  returns transactable data containing the schema of the module.
- A namespace qualified symbol that resolves to the module's
  *configure function*. A configure function is a function that takes a
  configuration value and returns an updated configuration.

When an application is defined, the user must specify a set of module
names to use (exact mechanism TBD.) Only the specified modules (and
their dependencies) will be considered by Arachne. In other words,
merely including a module as a dependency in the package manager is
not sufficient to activate it and cause it to be used in an
application.

","Architectural Decision Record

## Context
Arachne needs to be as modular as possible. Not only do we want the
community to be able to contribute new abilities and features that
integrate well with the core and with eachother, we want some of the
basic functionality of Arachne to be swappable for alternatives as
well.

[ADR-002](adr-002-configuration.md) specifies that one role of modules
is to contribute schema to the application config. Other roles of
modules would include providing code (as any library does), and
querying and updating the config during the startup
process. Additionally, since modules can depend upon each other, they
must specify which modules they depend upon.

Ideally there will be as little overhead as possible for creating and
consuming modules.

Some of the general problems associated with plugin/module systems include:

- Finding and downloading the implementation of the module.
- Discovering and activating the correct set of installed modules.
- Managing module versions and dependencies.

There are some existing systems for modularity in the Java
ecosystem. The most notable is OSGi, which provides not only a module
system addressing the concerns above, but also service runtime with
classpath isolation, dynamic loading and unloading and lazy
activation.

OSGi (and other systems of comparable scope) are overkill for
Arachne. Although they come with benefits, they are very heavyweight
and carry a high complexity burden, not just for Arachne development
but also for end users. Specifically, Arachne applications will be
drastically simpler if (at runtime) they exist as a straightforward
codebase in a single classloader space. Features like lazy loading and
dynamic start-stop are likewise out of scope; the goal is for an
Arachne runtime itself to be lightweight enough that starting and
stopping when modules change is not an issue.

## Decision
","## Context

Arachne needs to be as modular as possible. Not only do we want the
community to be able to contribute new abilities and features that
integrate well with the core and with eachother, we want some of the
basic functionality of Arachne to be swappable for alternatives as
well.

[ADR-002](adr-002-configuration.md) specifies that one role of modules
is to contribute schema to the application config. Other roles of
modules would include providing code (as any library does), and
querying and updating the config during the startup
process. Additionally, since modules can depend upon each other, they
must specify which modules they depend upon.

Ideally there will be as little overhead as possible for creating and
consuming modules.

Some of the general problems associated with plugin/module systems include:

- Finding and downloading the implementation of the module.
- Discovering and activating the correct set of installed modules.
- Managing module versions and dependencies.

There are some existing systems for modularity in the Java
ecosystem. The most notable is OSGi, which provides not only a module
system addressing the concerns above, but also service runtime with
classpath isolation, dynamic loading and unloading and lazy
activation.

OSGi (and other systems of comparable scope) are overkill for
Arachne. Although they come with benefits, they are very heavyweight
and carry a high complexity burden, not just for Arachne development
but also for end users. Specifically, Arachne applications will be
drastically simpler if (at runtime) they exist as a straightforward
codebase in a single classloader space. Features like lazy loading and
dynamic start-stop are likewise out of scope; the goal is for an
Arachne runtime itself to be lightweight enough that starting and
stopping when modules change is not an issue.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Arachne needs to be as modular as possible. Not only do we want the
community to be able to contribute new abilities and features that
integrate well with the core and with eachother, we want some of the
basic functionality of Arachne to be swappable for alternatives as
well.

[ADR-002](adr-002-configuration.md) specifies that one role of modules
is to contribute schema to the application config. Other roles of
modules would include providing code (as any library does), and
querying and updating the config during the startup
process. Additionally, since modules can depend upon each other, they
must specify which modules they depend upon.

Ideally there will be as little overhead as possible for creating and
consuming modules.

Some of the general problems associated with plugin/module systems include:

- Finding and downloading the implementation of the module.
- Discovering and activating the correct set of installed modules.
- Managing module versions and dependencies.

There are some existing systems for modularity in the Java
ecosystem. The most notable is OSGi, which provides not only a module
system addressing the concerns above, but also service runtime with
classpath isolation, dynamic loading and unloading and lazy
activation.

OSGi (and other systems of comparable scope) are overkill for
Arachne. Although they come with benefits, they are very heavyweight
and carry a high complexity burden, not just for Arachne development
but also for end users. Specifically, Arachne applications will be
drastically simpler if (at runtime) they exist as a straightforward
codebase in a single classloader space. Features like lazy loading and
dynamic start-stop are likewise out of scope; the goal is for an
Arachne runtime itself to be lightweight enough that starting and
stopping when modules change is not an issue."
"Manual serialization of SnakeYAML

The TOSCA YAML files have to be read into a Java model (deserialized) and written from the Java model into files (serialized).

## Considered Alternatives

* Manual serialization
* [SnakeYAML](https://bitbucket.org/asomov/snakeyaml)
* [jackson-dataformat-yaml](https://github.com/FasterXML/jackson-dataformat-yaml)","* Chosen Alternative: *Manual serialization*
* SnakeYAML does not support annotations for serialization
* jackson-dataformat-yaml seems not to support annotations, such as [jackson-annotations](https://github.com/FasterXML/jackson-annotations)

","Architectural Decision Record

## Context
Manual serialization of SnakeYAML

The TOSCA YAML files have to be read into a Java model (deserialized) and written from the Java model into files (serialized).

## Considered Alternatives

* Manual serialization
* [SnakeYAML](https://bitbucket.org/asomov/snakeyaml)
* [jackson-dataformat-yaml](https://github.com/FasterXML/jackson-dataformat-yaml)

## Decision
","## Context

Manual serialization of SnakeYAML

The TOSCA YAML files have to be read into a Java model (deserialized) and written from the Java model into files (serialized).

## Considered Alternatives

* Manual serialization
* [SnakeYAML](https://bitbucket.org/asomov/snakeyaml)
* [jackson-dataformat-yaml](https://github.com/FasterXML/jackson-dataformat-yaml)","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Manual serialization of SnakeYAML

The TOSCA YAML files have to be read into a Java model (deserialized) and written from the Java model into files (serialized).

## Considered Alternatives

* Manual serialization
* [SnakeYAML](https://bitbucket.org/asomov/snakeyaml)
* [jackson-dataformat-yaml](https://github.com/FasterXML/jackson-dataformat-yaml)"
"Reflection test for TOSCA YAML builder

The TOSCA YAML builder converts Java Objects to instances of TOSCA YAML classes. To get clean an good instances validation is needed. Reflection test are Junit5 test which take yaml service templates with metadata that describes what assertions should be made for the resulting TOSCA YAML class instances. 

```
...
metadata:
  assert: |
    repositories.rp1.url = http://github.com/kleinech
    node_types.ntp1.requirements.0.rqr1.capability = cbt1
...
```
Each assert line contains a keyname and a value.
*[context and problem statement]*
*[decision drivers | forces]* <!-- optional -->

## Considered Alternatives

* *reflection tests*
* *manual test*","* Chosen Alternative: *reflection tests*
* Only alternative, which meets simplifies the effort to make complete tests 
","Architectural Decision Record

## Context
Reflection test for TOSCA YAML builder

The TOSCA YAML builder converts Java Objects to instances of TOSCA YAML classes. To get clean an good instances validation is needed. Reflection test are Junit5 test which take yaml service templates with metadata that describes what assertions should be made for the resulting TOSCA YAML class instances. 

```
...
metadata:
  assert: |
    repositories.rp1.url = http://github.com/kleinech
    node_types.ntp1.requirements.0.rqr1.capability = cbt1
...
```
Each assert line contains a keyname and a value.
*[context and problem statement]*
*[decision drivers | forces]* <!-- optional -->

## Considered Alternatives

* *reflection tests*
* *manual test*

## Decision
","## Context

Reflection test for TOSCA YAML builder

The TOSCA YAML builder converts Java Objects to instances of TOSCA YAML classes. To get clean an good instances validation is needed. Reflection test are Junit5 test which take yaml service templates with metadata that describes what assertions should be made for the resulting TOSCA YAML class instances. 

```
...
metadata:
  assert: |
    repositories.rp1.url = http://github.com/kleinech
    node_types.ntp1.requirements.0.rqr1.capability = cbt1
...
```
Each assert line contains a keyname and a value.
*[context and problem statement]*
*[decision drivers | forces]* <!-- optional -->

## Considered Alternatives

* *reflection tests*
* *manual test*","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Reflection test for TOSCA YAML builder

The TOSCA YAML builder converts Java Objects to instances of TOSCA YAML classes. To get clean an good instances validation is needed. Reflection test are Junit5 test which take yaml service templates with metadata that describes what assertions should be made for the resulting TOSCA YAML class instances. 

```
...
metadata:
  assert: |
    repositories.rp1.url = http://github.com/kleinech
    node_types.ntp1.requirements.0.rqr1.capability = cbt1
...
```
Each assert line contains a keyname and a value.
*[context and problem statement]*
*[decision drivers | forces]* <!-- optional -->

## Considered Alternatives

* *reflection tests*
* *manual test*"
"Double Encoded URLs

URLs should be human-readable, slashes are not allowed.
Encoded slahes are not enabled as default due to security reasons.

More information about encoding is available at [dev/Encoding.md](dev/Encoding).

## Considered Alternatives

* Using namespace prefixes in the URLs
* Single-encoded URLs and forcing the environment to be reconfigered
* Double-encoded URLs","* *Chosen Alternative: Double-encoded URLs*

","Architectural Decision Record

## Context
Double Encoded URLs

URLs should be human-readable, slashes are not allowed.
Encoded slahes are not enabled as default due to security reasons.

More information about encoding is available at [dev/Encoding.md](dev/Encoding).

## Considered Alternatives

* Using namespace prefixes in the URLs
* Single-encoded URLs and forcing the environment to be reconfigered
* Double-encoded URLs

## Decision
","## Context

Double Encoded URLs

URLs should be human-readable, slashes are not allowed.
Encoded slahes are not enabled as default due to security reasons.

More information about encoding is available at [dev/Encoding.md](dev/Encoding).

## Considered Alternatives

* Using namespace prefixes in the URLs
* Single-encoded URLs and forcing the environment to be reconfigered
* Double-encoded URLs","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Double Encoded URLs

URLs should be human-readable, slashes are not allowed.
Encoded slahes are not enabled as default due to security reasons.

More information about encoding is available at [dev/Encoding.md](dev/Encoding).

## Considered Alternatives

* Using namespace prefixes in the URLs
* Single-encoded URLs and forcing the environment to be reconfigered
* Double-encoded URLs"
"Provide Support for Custom Key-Value Properties

Most properties are key/value

## Considered Alternatives

* Provide support for custom key-value properties
* Support XML-only","* Chosen Alternative: Provide support for custom key-value properties
* Nice UI experience

","Architectural Decision Record

## Context
Provide Support for Custom Key-Value Properties

Most properties are key/value

## Considered Alternatives

* Provide support for custom key-value properties
* Support XML-only

## Decision
","## Context

Provide Support for Custom Key-Value Properties

Most properties are key/value

## Considered Alternatives

* Provide support for custom key-value properties
* Support XML-only","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Provide Support for Custom Key-Value Properties

Most properties are key/value

## Considered Alternatives

* Provide support for custom key-value properties
* Support XML-only"
"TOSCA YAML deserialization using SnakeYAML

The TOSCA YAML files have to be read into a Java model (deserialized) and written from the Java model into files (serialized).

## Considered Alternatives

* [SnakeYAML](https://bitbucket.org/asomov/snakeyaml) writing into intermediate model consisting of default Java types
* SnakeYAML writing into final Java model
* Manual reading","* Chosen Alternative: *SnakeYAML writing into intermediate model consisting of default Java types*

# Pros and Cons of the Alternatives
  
### SnakeYAML writing into intermediate model consisting of default Java types
  
* `+` Basic YAML parsing can be done using SnakeYAML
* `+` TOSCA Java classes can be filled directly
* `-` Programming effort for conversion

### SnakeYAML writing into final Java model

* `+` Established library
* `+` Less error prone 
* `-` SnakeYAML has to be adapted to be able to convert YAML into TOSCA models
* `-` SnakeYAML is not well-prepared for adaptions
* `-` SnakeYAML has issues to write into complex Java classes (which are not Java base types). E.g., List of Maps. - see <https://bitbucket.org/asomov/snakeyaml/issues/361/list-does-not-create-property-objects>
* `-` huge effort, first attempt did not result in a working converter

### Manual reading

* `+` Can write directly into Java model
* `-` Special cases of YAML have to be handled manually
* `-` Error prone

","Architectural Decision Record

## Context
TOSCA YAML deserialization using SnakeYAML

The TOSCA YAML files have to be read into a Java model (deserialized) and written from the Java model into files (serialized).

## Considered Alternatives

* [SnakeYAML](https://bitbucket.org/asomov/snakeyaml) writing into intermediate model consisting of default Java types
* SnakeYAML writing into final Java model
* Manual reading

## Decision
","## Context

TOSCA YAML deserialization using SnakeYAML

The TOSCA YAML files have to be read into a Java model (deserialized) and written from the Java model into files (serialized).

## Considered Alternatives

* [SnakeYAML](https://bitbucket.org/asomov/snakeyaml) writing into intermediate model consisting of default Java types
* SnakeYAML writing into final Java model
* Manual reading","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
TOSCA YAML deserialization using SnakeYAML

The TOSCA YAML files have to be read into a Java model (deserialized) and written from the Java model into files (serialized).

## Considered Alternatives

* [SnakeYAML](https://bitbucket.org/asomov/snakeyaml) writing into intermediate model consisting of default Java types
* SnakeYAML writing into final Java model
* Manual reading"
"Custom URI for lifecycle interface

Winery can generate a lifecycle interface.
That interface has to take a URI for a name

## Considered Alternatives

* `http://opentosca.org/interfaces/lifecycle`
* `http://www.example.com/interfaces/lifecycle` (from http://docs.oasis-open.org/tosca/tosca-primer/v1.0/tosca-primer-v1.0.html)
* `tosca.interfaces.node.lifecycle.Standard` (from http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.1/TOSCA-Simple-Profile-YAML-v1.1.html)  ","* Chosen Alternative: `http://www.example.com/interfaces/lifecycle`
* Although the alternative is not standardized, it is the one consistent with the primer for TOSCA 1.0.


","Architectural Decision Record

## Context
Custom URI for lifecycle interface

Winery can generate a lifecycle interface.
That interface has to take a URI for a name

## Considered Alternatives

* `http://opentosca.org/interfaces/lifecycle`
* `http://www.example.com/interfaces/lifecycle` (from http://docs.oasis-open.org/tosca/tosca-primer/v1.0/tosca-primer-v1.0.html)
* `tosca.interfaces.node.lifecycle.Standard` (from http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.1/TOSCA-Simple-Profile-YAML-v1.1.html)  

## Decision
","## Context

Custom URI for lifecycle interface

Winery can generate a lifecycle interface.
That interface has to take a URI for a name

## Considered Alternatives

* `http://opentosca.org/interfaces/lifecycle`
* `http://www.example.com/interfaces/lifecycle` (from http://docs.oasis-open.org/tosca/tosca-primer/v1.0/tosca-primer-v1.0.html)
* `tosca.interfaces.node.lifecycle.Standard` (from http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.1/TOSCA-Simple-Profile-YAML-v1.1.html)  ","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Custom URI for lifecycle interface

Winery can generate a lifecycle interface.
That interface has to take a URI for a name

## Considered Alternatives

* `http://opentosca.org/interfaces/lifecycle`
* `http://www.example.com/interfaces/lifecycle` (from http://docs.oasis-open.org/tosca/tosca-primer/v1.0/tosca-primer-v1.0.html)
* `tosca.interfaces.node.lifecycle.Standard` (from http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.1/TOSCA-Simple-Profile-YAML-v1.1.html)  "
"Use Builder Pattern for Model Classes

Model classes should be instantiable simple without using large constructors.

## Considered Alternatives

* [Builders]
* Setters, getters and default constructor 
* Large constructors
* Factories","* Chosen Alternative: *Builders*
* Flexible
* Simple for complex objects
* Extensions cause problems (solved with generic builders) 

### Generic Builders

Generic Builders are used to enable safe method chaining for Builders with extend other Builders.
Another discussion is made at [stackoverflow].

The method `self()` is necessary because all setter methods should return the Builder used for instantiation and not the builder that is extended. `self()` can not be replace by `this` because the expected type is `<T>` and casting to `<T>` results in warnings.

Builders which are not abstract and are extended by other builders are generic and implement the `self()` method by casting `this` to `<T>`. To reduce warnings this casting is only used in this case.

Example:
```java
// part of ExtensibleElements.Builder
public abstract static class Builder<T extends Builder<T>> {
    private List<TDocumentation> documentation;
    
    // setter returns generic <T> 
    public T setDocumentation(List<TDocumentation> documentation) {
        this.documentation = documentation;
        // return this; => IncompatibleType exception either cast with warnings or use self() method
        return self();
    }
    
    // overwritten method
    public abstract T self();
}

// part of TEntityType.Builder
public abstract static class Builder<T extends Builder<T>> extends TExtensibleElements.Builder<T> {
	
}

// part of TNodeType.Builder
public static class Builder extends TEntityType.Builder<Builder> {
    @Override
    public Builder self() {
        return this;
    }
}
```



[Builders]:(https://en.wikipedia.org/wiki/Builder_pattern)
[stackoverflow]: https://stackoverflow.com/a/5818701/8235252

","Architectural Decision Record

## Context
Use Builder Pattern for Model Classes

Model classes should be instantiable simple without using large constructors.

## Considered Alternatives

* [Builders]
* Setters, getters and default constructor 
* Large constructors
* Factories

## Decision
","## Context

Use Builder Pattern for Model Classes

Model classes should be instantiable simple without using large constructors.

## Considered Alternatives

* [Builders]
* Setters, getters and default constructor 
* Large constructors
* Factories","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Use Builder Pattern for Model Classes

Model classes should be instantiable simple without using large constructors.

## Considered Alternatives

* [Builders]
* Setters, getters and default constructor 
* Large constructors
* Factories"
"File system folder structure using type-namespace-id structure

Winery's data is stored in a a file system [ADR-0001](0001-use-filesystem-as-backend).
The contents of the repository should be

- human readable
- machine processable

## Considered Alternatives

* Folder structure using type-namespace-id
* Everything in one folder. Hash-based storing similar to git.","*Chosen Alternative: Folders subdivided in type-namespace-id*

The final file system layout itself is documented at [RepositoryLayout](../dev/RepositoryLayout).

### human readable

Everything in one directory causes many files listed and thus humans will have difficulties to find the right file.

The folders in the *top level* are the TOSCA ""components"", e.g., Node Type, Relationship Type, Service Template, ...

The *second structuring element* are namespaces.
[Namespaces are an established method to avoid naming conflicts](https://www.w3schools.com/xml/xml_namespaces.asp) and are a structuring element.
TOSCA is an open system and everyone can create Node Types.
One has no global control which names are given to Node Types.
Thus, there might be two different Node Types with the same name.
The namespaces provide a natural structuring and Winery reuses this idea.

The *third structuring element* are the ids of the respective definitions child (type, template, ...):
Each element has an id contained in the respective namespace.
This id can be directly used as folder name.

Within each folder, the component-specific information is stored.

### machine processable

Windows cannot create directories named `http://www.example.com`.
Therefore, the names have to be [encoded](https://en.wikipedia.org/wiki/Character_encoding) so that an appropriate folder can be generated.


","Architectural Decision Record

## Context
File system folder structure using type-namespace-id structure

Winery's data is stored in a a file system [ADR-0001](0001-use-filesystem-as-backend).
The contents of the repository should be

- human readable
- machine processable

## Considered Alternatives

* Folder structure using type-namespace-id
* Everything in one folder. Hash-based storing similar to git.

## Decision
","## Context

File system folder structure using type-namespace-id structure

Winery's data is stored in a a file system [ADR-0001](0001-use-filesystem-as-backend).
The contents of the repository should be

- human readable
- machine processable

## Considered Alternatives

* Folder structure using type-namespace-id
* Everything in one folder. Hash-based storing similar to git.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
File system folder structure using type-namespace-id structure

Winery's data is stored in a a file system [ADR-0001](0001-use-filesystem-as-backend).
The contents of the repository should be

- human readable
- machine processable

## Considered Alternatives

* Folder structure using type-namespace-id
* Everything in one folder. Hash-based storing similar to git."
"Use Markdown Architectural Decision Records

Should we record the architectural decisions made in this project?
And if we do, how to structure these recordings?

## Considered Alternatives

* [MADR](https://adr.github.io/madr/) - Markdown Architectural Decision Records
* [Michael Nygard's template](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) - The first incarnation of the term ""ADR"". Maintainable by [adr-tools](https://github.com/npryce/adr-tools).
* [Sustainable Architectural Decisions](https://www.infoq.com/articles/sustainable-architectural-design-decisions) - The Y-Statements
* [DecisionRecord](https://github.com/schubmat/DecisionCapture) - Agile records by [@schubmat](https://github.com/schubmat/)
* Other templates listed at <https://github.com/joelparkerhenderson/architecture_decision_record>
* No records","* Chosen Alternative: MADR
* Implicit assumptions should be made explicit.
  Design documentation is important to enable people understanding the decisions later on.
  See also [A rational design process: How and why to fake it](https://doi.org/10.1109/TSE.1986.6312940).
* The MADR template is lean and fits our development style.

<!-- Pros and cons of alternatives straight-forward to elicit and therefore not captured. -->

","Architectural Decision Record

## Context
Use Markdown Architectural Decision Records

Should we record the architectural decisions made in this project?
And if we do, how to structure these recordings?

## Considered Alternatives

* [MADR](https://adr.github.io/madr/) - Markdown Architectural Decision Records
* [Michael Nygard's template](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) - The first incarnation of the term ""ADR"". Maintainable by [adr-tools](https://github.com/npryce/adr-tools).
* [Sustainable Architectural Decisions](https://www.infoq.com/articles/sustainable-architectural-design-decisions) - The Y-Statements
* [DecisionRecord](https://github.com/schubmat/DecisionCapture) - Agile records by [@schubmat](https://github.com/schubmat/)
* Other templates listed at <https://github.com/joelparkerhenderson/architecture_decision_record>
* No records

## Decision
","## Context

Use Markdown Architectural Decision Records

Should we record the architectural decisions made in this project?
And if we do, how to structure these recordings?

## Considered Alternatives

* [MADR](https://adr.github.io/madr/) - Markdown Architectural Decision Records
* [Michael Nygard's template](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) - The first incarnation of the term ""ADR"". Maintainable by [adr-tools](https://github.com/npryce/adr-tools).
* [Sustainable Architectural Decisions](https://www.infoq.com/articles/sustainable-architectural-design-decisions) - The Y-Statements
* [DecisionRecord](https://github.com/schubmat/DecisionCapture) - Agile records by [@schubmat](https://github.com/schubmat/)
* Other templates listed at <https://github.com/joelparkerhenderson/architecture_decision_record>
* No records","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Use Markdown Architectural Decision Records

Should we record the architectural decisions made in this project?
And if we do, how to structure these recordings?

## Considered Alternatives

* [MADR](https://adr.github.io/madr/) - Markdown Architectural Decision Records
* [Michael Nygard's template](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) - The first incarnation of the term ""ADR"". Maintainable by [adr-tools](https://github.com/npryce/adr-tools).
* [Sustainable Architectural Decisions](https://www.infoq.com/articles/sustainable-architectural-design-decisions) - The Y-Statements
* [DecisionRecord](https://github.com/schubmat/DecisionCapture) - Agile records by [@schubmat](https://github.com/schubmat/)
* Other templates listed at <https://github.com/joelparkerhenderson/architecture_decision_record>
* No records"
"Offer copying files from the source to the files folder

Forces:

- Source code needs to be versioned in Winery
- Support for scripting languages do not need any processing and only need to be copied as they are
- Support for compiled languages need to be processed before copying

## Considered Alternatives

* Copying the sources to files as they are
* Require external tooling to go from source to files"," Chosen Alternative: Copying the sources as they are
* For supporting compiled languages, it is relied on an external IDE (see [ADR-0014]).
  This IDE stores the files in the ""source"" folder and manages the copying to the files folder.
  Thus, the only left support is for scripting languages.
  In that case, the source can be directly used as files (""binary"") in an artifact template.

","Architectural Decision Record

## Context
Offer copying files from the source to the files folder

Forces:

- Source code needs to be versioned in Winery
- Support for scripting languages do not need any processing and only need to be copied as they are
- Support for compiled languages need to be processed before copying

## Considered Alternatives

* Copying the sources to files as they are
* Require external tooling to go from source to files

## Decision
","## Context

Offer copying files from the source to the files folder

Forces:

- Source code needs to be versioned in Winery
- Support for scripting languages do not need any processing and only need to be copied as they are
- Support for compiled languages need to be processed before copying

## Considered Alternatives

* Copying the sources to files as they are
* Require external tooling to go from source to files","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Offer copying files from the source to the files folder

Forces:

- Source code needs to be versioned in Winery
- Support for scripting languages do not need any processing and only need to be copied as they are
- Support for compiled languages need to be processed before copying

## Considered Alternatives

* Copying the sources to files as they are
* Require external tooling to go from source to files"
"Use filesystem as backend

Winery needs to store its contents.
These contents need to be shared.

## Considered Alternatives

* Filesystem
* Database","* *Chosen Alternative: Filesystem*

","Architectural Decision Record

## Context
Use filesystem as backend

Winery needs to store its contents.
These contents need to be shared.

## Considered Alternatives

* Filesystem
* Database

## Decision
","## Context

Use filesystem as backend

Winery needs to store its contents.
These contents need to be shared.

## Considered Alternatives

* Filesystem
* Database","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Use filesystem as backend

Winery needs to store its contents.
These contents need to be shared.

## Considered Alternatives

* Filesystem
* Database"
"No support for local git source clones

A user wants to edit source files locally in his favourite IDE.
Therefore, he wants to use the usual ways to retrieve source files.
Typically, this is a `git clone` from a git repository having the respective source files.

A user does not want to clone the whole winery repository, as this might a) be too large b) not focused enough.
It would be beneficial to have the source of an artifact template available as git checkout.

The source files of an artifact implementation are currently directly editable in the winery once they are uploaded. 
The only way to edit sources locally is to download and upload them again.
The solution for the user should be:
- easy to use
- scalable in terms of storage required in Winery's repository


## Considered Alternatives

* No support for local clones
* Git repositories as submodules
* Using filter-branch (https://help.github.com/articles/splitting-a-subfolder-out-into-a-new-repository/)
* Using git sparse-checkout to create a local clone (https://gist.github.com/sumardi/5559896)","* Chosen Alternative: no support for local edit

Since all alternatives require either too many additional git repositories or are very inconvenient to apply for the user,
we decided to not support any clone/push functionality.


","Architectural Decision Record

## Context
No support for local git source clones

A user wants to edit source files locally in his favourite IDE.
Therefore, he wants to use the usual ways to retrieve source files.
Typically, this is a `git clone` from a git repository having the respective source files.

A user does not want to clone the whole winery repository, as this might a) be too large b) not focused enough.
It would be beneficial to have the source of an artifact template available as git checkout.

The source files of an artifact implementation are currently directly editable in the winery once they are uploaded. 
The only way to edit sources locally is to download and upload them again.
The solution for the user should be:
- easy to use
- scalable in terms of storage required in Winery's repository


## Considered Alternatives

* No support for local clones
* Git repositories as submodules
* Using filter-branch (https://help.github.com/articles/splitting-a-subfolder-out-into-a-new-repository/)
* Using git sparse-checkout to create a local clone (https://gist.github.com/sumardi/5559896)

## Decision
","## Context

No support for local git source clones

A user wants to edit source files locally in his favourite IDE.
Therefore, he wants to use the usual ways to retrieve source files.
Typically, this is a `git clone` from a git repository having the respective source files.

A user does not want to clone the whole winery repository, as this might a) be too large b) not focused enough.
It would be beneficial to have the source of an artifact template available as git checkout.

The source files of an artifact implementation are currently directly editable in the winery once they are uploaded. 
The only way to edit sources locally is to download and upload them again.
The solution for the user should be:
- easy to use
- scalable in terms of storage required in Winery's repository


## Considered Alternatives

* No support for local clones
* Git repositories as submodules
* Using filter-branch (https://help.github.com/articles/splitting-a-subfolder-out-into-a-new-repository/)
* Using git sparse-checkout to create a local clone (https://gist.github.com/sumardi/5559896)","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
No support for local git source clones

A user wants to edit source files locally in his favourite IDE.
Therefore, he wants to use the usual ways to retrieve source files.
Typically, this is a `git clone` from a git repository having the respective source files.

A user does not want to clone the whole winery repository, as this might a) be too large b) not focused enough.
It would be beneficial to have the source of an artifact template available as git checkout.

The source files of an artifact implementation are currently directly editable in the winery once they are uploaded. 
The only way to edit sources locally is to download and upload them again.
The solution for the user should be:
- easy to use
- scalable in terms of storage required in Winery's repository


## Considered Alternatives

* No support for local clones
* Git repositories as submodules
* Using filter-branch (https://help.github.com/articles/splitting-a-subfolder-out-into-a-new-repository/)
* Using git sparse-checkout to create a local clone (https://gist.github.com/sumardi/5559896)"
"Use Eclipse Orion as Editor

Editor should provide syntax highlighting and work in browser.

## Considered Alternatives

* [Eclipse Orion](https://wiki.eclipse.org/Orion)
* [Eclipse Che](https://www.eclipse.org/che/)
* [Codeanywhere](https://codeanywhere.com)
* [Cloud9](https://c9.io/)
* [Codiad](http://codiad.com/)
* [Theia](https://github.com/theia-ide/theia)","* Chosen Alternative: Eclipse Orion
* Reasons: 
  * Extension of other IDEs either too time consuming or not possible.
  * Communication between other IDEs and Winery unclear.
  * Eclipse Orion was already used in the project.
  
For more details see the [ultimate comparison](https://github.com/ultimate-comparisons/ultimate-webIDE-comparison).

","Architectural Decision Record

## Context
Use Eclipse Orion as Editor

Editor should provide syntax highlighting and work in browser.

## Considered Alternatives

* [Eclipse Orion](https://wiki.eclipse.org/Orion)
* [Eclipse Che](https://www.eclipse.org/che/)
* [Codeanywhere](https://codeanywhere.com)
* [Cloud9](https://c9.io/)
* [Codiad](http://codiad.com/)
* [Theia](https://github.com/theia-ide/theia)

## Decision
","## Context

Use Eclipse Orion as Editor

Editor should provide syntax highlighting and work in browser.

## Considered Alternatives

* [Eclipse Orion](https://wiki.eclipse.org/Orion)
* [Eclipse Che](https://www.eclipse.org/che/)
* [Codeanywhere](https://codeanywhere.com)
* [Cloud9](https://c9.io/)
* [Codiad](http://codiad.com/)
* [Theia](https://github.com/theia-ide/theia)","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Use Eclipse Orion as Editor

Editor should provide syntax highlighting and work in browser.

## Considered Alternatives

* [Eclipse Orion](https://wiki.eclipse.org/Orion)
* [Eclipse Che](https://www.eclipse.org/che/)
* [Codeanywhere](https://codeanywhere.com)
* [Cloud9](https://c9.io/)
* [Codiad](http://codiad.com/)
* [Theia](https://github.com/theia-ide/theia)"
"OAuth with GitHub

**UserStory:**

In order to use (private) repositories instead of uploading all files into a Artifact Template, it is necessary to log in into GitHub.
  Therefore, the OAuth flow is implented as follows:
  
1. By clicking the `Login with GitHub` button, the browser gets redirected to GitHub for authentication.
    1. Login and authorize the Application to access the private repositories.
    1. Or, if the application was already been authorized, GitHub automatically continues with step 2
1. GitHub answers with the `status` and `code` parameters which are parsed and send to our server to get the access token.
1. Our server responses with the access token.

![OAuth flow diagram](graphics/oauth-flow.png)  

## Considered Alternatives
* *[ALTERNATIVE 1]* Perform the whole login process in the browser
* *[ALTERNATIVE 2]* Perform the whole login process in the backend
* *[ALTERNATIVE 3]* Mix both, frontend and backend to get the access token"," *Chosen Alternative: [ALTERNATIVE 3]*
   because we can easily protect our client secret on the server, store the token safely in the clients local storage and 
   do not need to keep the state at the server. Further, it is possible to save additional user information in the local
   storage without the need for getting it every time from the server.

","Architectural Decision Record

## Context
OAuth with GitHub

**UserStory:**

In order to use (private) repositories instead of uploading all files into a Artifact Template, it is necessary to log in into GitHub.
  Therefore, the OAuth flow is implented as follows:
  
1. By clicking the `Login with GitHub` button, the browser gets redirected to GitHub for authentication.
    1. Login and authorize the Application to access the private repositories.
    1. Or, if the application was already been authorized, GitHub automatically continues with step 2
1. GitHub answers with the `status` and `code` parameters which are parsed and send to our server to get the access token.
1. Our server responses with the access token.

![OAuth flow diagram](graphics/oauth-flow.png)  

## Considered Alternatives
* *[ALTERNATIVE 1]* Perform the whole login process in the browser
* *[ALTERNATIVE 2]* Perform the whole login process in the backend
* *[ALTERNATIVE 3]* Mix both, frontend and backend to get the access token

## Decision
","## Context

OAuth with GitHub

**UserStory:**

In order to use (private) repositories instead of uploading all files into a Artifact Template, it is necessary to log in into GitHub.
  Therefore, the OAuth flow is implented as follows:
  
1. By clicking the `Login with GitHub` button, the browser gets redirected to GitHub for authentication.
    1. Login and authorize the Application to access the private repositories.
    1. Or, if the application was already been authorized, GitHub automatically continues with step 2
1. GitHub answers with the `status` and `code` parameters which are parsed and send to our server to get the access token.
1. Our server responses with the access token.

![OAuth flow diagram](graphics/oauth-flow.png)  

## Considered Alternatives
* *[ALTERNATIVE 1]* Perform the whole login process in the browser
* *[ALTERNATIVE 2]* Perform the whole login process in the backend
* *[ALTERNATIVE 3]* Mix both, frontend and backend to get the access token","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
OAuth with GitHub

**UserStory:**

In order to use (private) repositories instead of uploading all files into a Artifact Template, it is necessary to log in into GitHub.
  Therefore, the OAuth flow is implented as follows:
  
1. By clicking the `Login with GitHub` button, the browser gets redirected to GitHub for authentication.
    1. Login and authorize the Application to access the private repositories.
    1. Or, if the application was already been authorized, GitHub automatically continues with step 2
1. GitHub answers with the `status` and `code` parameters which are parsed and send to our server to get the access token.
1. Our server responses with the access token.

![OAuth flow diagram](graphics/oauth-flow.png)  

## Considered Alternatives
* *[ALTERNATIVE 1]* Perform the whole login process in the browser
* *[ALTERNATIVE 2]* Perform the whole login process in the backend
* *[ALTERNATIVE 3]* Mix both, frontend and backend to get the access token"
"XML editor does not enforce validation

**UserStory:** Winery offers editing the stored XML of the TOSCA definitions. What to do with validation?

## Considered Alternatives

* Winery never creates an non-schema-conforming XML. For instance, the user has to create a topology template first before he is allowed to save the service template
* Winery generate random data to gain schema-conforming XML
* Winery generates non-schema-conforming XML, but assumes that the user makes it eventually valid. In casea the user uses the XML tab, the user knows what he does. Winery forces the user to generate schema-conforming in the XML editor.
* Winery generates non-schema-conforming XML and warns the user when the user uses the XML editor. Winery does NOT force the user to generate schema-conforming XML in the XML editor.","* *Chosen Alternative: D*
* This is in line with other editors: They allow to save, but warn if the file has compile errors, validation errors, ...

","Architectural Decision Record

## Context
XML editor does not enforce validation

**UserStory:** Winery offers editing the stored XML of the TOSCA definitions. What to do with validation?

## Considered Alternatives

* Winery never creates an non-schema-conforming XML. For instance, the user has to create a topology template first before he is allowed to save the service template
* Winery generate random data to gain schema-conforming XML
* Winery generates non-schema-conforming XML, but assumes that the user makes it eventually valid. In casea the user uses the XML tab, the user knows what he does. Winery forces the user to generate schema-conforming in the XML editor.
* Winery generates non-schema-conforming XML and warns the user when the user uses the XML editor. Winery does NOT force the user to generate schema-conforming XML in the XML editor.

## Decision
","## Context

XML editor does not enforce validation

**UserStory:** Winery offers editing the stored XML of the TOSCA definitions. What to do with validation?

## Considered Alternatives

* Winery never creates an non-schema-conforming XML. For instance, the user has to create a topology template first before he is allowed to save the service template
* Winery generate random data to gain schema-conforming XML
* Winery generates non-schema-conforming XML, but assumes that the user makes it eventually valid. In casea the user uses the XML tab, the user knows what he does. Winery forces the user to generate schema-conforming in the XML editor.
* Winery generates non-schema-conforming XML and warns the user when the user uses the XML editor. Winery does NOT force the user to generate schema-conforming XML in the XML editor.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
XML editor does not enforce validation

**UserStory:** Winery offers editing the stored XML of the TOSCA definitions. What to do with validation?

## Considered Alternatives

* Winery never creates an non-schema-conforming XML. For instance, the user has to create a topology template first before he is allowed to save the service template
* Winery generate random data to gain schema-conforming XML
* Winery generates non-schema-conforming XML, but assumes that the user makes it eventually valid. In casea the user uses the XML tab, the user knows what he does. Winery forces the user to generate schema-conforming in the XML editor.
* Winery generates non-schema-conforming XML and warns the user when the user uses the XML editor. Winery does NOT force the user to generate schema-conforming XML in the XML editor."
"Routes in the Repository Angular App

## Considered Alternatives
* Using Wildcards for the `ToscaTypes`
* Explicitly Define the Routes for Each `ToscaType`"," Chosen Alternative: *Explicitly Define the Routes for Each `ToscaType`*
* By choosing this alternative, the whole project gets more type save by the (lightweight) trade-off
 of maintaining a list of all available `MainRoutes` in the `ToscaTypes` enum. It is now harder to add
 new main routes, because you need to add extra `Modules` and `RoutingModules` for each type. However,
 because of this decision, it is easier to define invalid routes which lead to a `404 - Not Found` error page.

","Architectural Decision Record

## Context
Routes in the Repository Angular App

## Considered Alternatives
* Using Wildcards for the `ToscaTypes`
* Explicitly Define the Routes for Each `ToscaType`

## Decision
","## Context

Routes in the Repository Angular App

## Considered Alternatives
* Using Wildcards for the `ToscaTypes`
* Explicitly Define the Routes for Each `ToscaType`","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Routes in the Repository Angular App

## Considered Alternatives
* Using Wildcards for the `ToscaTypes`
* Explicitly Define the Routes for Each `ToscaType`"
"Wrap properties in TOSCA properties element

When GETting/PUTting the properties of an entitty template, the content has to be serialized somehow.

## Considered Alternatives

* Wrap properties in TOSCA properties element
* Use nested XML element (`getAny()`)","* Chosen Alternative: Wrap properties in TOSCA properties element
* Receiving an XML element is not possible with JAX-B/JAX-RS as that setting relies on strong typing.

","Architectural Decision Record

## Context
Wrap properties in TOSCA properties element

When GETting/PUTting the properties of an entitty template, the content has to be serialized somehow.

## Considered Alternatives

* Wrap properties in TOSCA properties element
* Use nested XML element (`getAny()`)

## Decision
","## Context

Wrap properties in TOSCA properties element

When GETting/PUTting the properties of an entitty template, the content has to be serialized somehow.

## Considered Alternatives

* Wrap properties in TOSCA properties element
* Use nested XML element (`getAny()`)","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Wrap properties in TOSCA properties element

When GETting/PUTting the properties of an entitty template, the content has to be serialized somehow.

## Considered Alternatives

* Wrap properties in TOSCA properties element
* Use nested XML element (`getAny()`)"
"The Team has been considering the adoption of Agile software development methodology to increase productivity, improve efficiency, and better adapt to the rapidly changing software development industry.

Key Points:

- Agile emphasizes iterative development, frequent communication, and flexibility in requirements 

- Agile can lead to better collaboration among team members

- Agile can help anticipate and respond to changes in project scope and requirements

- Agile can lead to improved time-to-market and better overall project outcomes","After much deliberation and discussion, the Team has decided to adopt Agile software development methodology. ","Architectural Decision Record

## Context
The Team has been considering the adoption of Agile software development methodology to increase productivity, improve efficiency, and better adapt to the rapidly changing software development industry.

Key Points:

- Agile emphasizes iterative development, frequent communication, and flexibility in requirements 

- Agile can lead to better collaboration among team members

- Agile can help anticipate and respond to changes in project scope and requirements

- Agile can lead to improved time-to-market and better overall project outcomes

## Decision
","## Context

The Team has been considering the adoption of Agile software development methodology to increase productivity, improve efficiency, and better adapt to the rapidly changing software development industry.

Key Points:

- Agile emphasizes iterative development, frequent communication, and flexibility in requirements 

- Agile can lead to better collaboration among team members

- Agile can help anticipate and respond to changes in project scope and requirements

- Agile can lead to improved time-to-market and better overall project outcomes","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
The Team has been considering the adoption of Agile software development methodology to increase productivity, improve efficiency, and better adapt to the rapidly changing software development industry.

Key Points:

- Agile emphasizes iterative development, frequent communication, and flexibility in requirements 

- Agile can lead to better collaboration among team members

- Agile can help anticipate and respond to changes in project scope and requirements

- Agile can lead to improved time-to-market and better overall project outcomes"
"Our organization is growing rapidly and we need a reliable and scalable cloud infrastructure to meet the demands of our business. After considering a number of cloud infrastructure providers, it was concluded that AWS offers the most suitable range of services to meet our current and future requirements.
","After a thorough evaluation of various cloud infrastructure platforms, it has been decided that Amazon Web Services (AWS) will be adopted by our organization for our cloud infrastructure needs.","Architectural Decision Record

## Context
Our organization is growing rapidly and we need a reliable and scalable cloud infrastructure to meet the demands of our business. After considering a number of cloud infrastructure providers, it was concluded that AWS offers the most suitable range of services to meet our current and future requirements.


## Decision
","## Context

Our organization is growing rapidly and we need a reliable and scalable cloud infrastructure to meet the demands of our business. After considering a number of cloud infrastructure providers, it was concluded that AWS offers the most suitable range of services to meet our current and future requirements.
","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Our organization is growing rapidly and we need a reliable and scalable cloud infrastructure to meet the demands of our business. After considering a number of cloud infrastructure providers, it was concluded that AWS offers the most suitable range of services to meet our current and future requirements.
"
"We are considering different container orchestration tools to manage our microservices-based architecture. We have evaluated different solutions like Kubernetes, Docker Swarm, and Mesosphere DC/OS. However, we have decided to focus on Docker Swarm due to its simplicity, integration with Docker, and built-in load balancing.","We have decided to use Docker Swarm as our container orchestration tool. Docker Swarm provides a simple and intuitive way to manage containerized applications across a cluster of nodes. It also allows us to leverage our existing Docker-based workflows and infrastructure. With Docker Swarm, we can easily deploy, scale, and manage our applications, all while taking advantage of built-in load balancing.","Architectural Decision Record

## Context
We are considering different container orchestration tools to manage our microservices-based architecture. We have evaluated different solutions like Kubernetes, Docker Swarm, and Mesosphere DC/OS. However, we have decided to focus on Docker Swarm due to its simplicity, integration with Docker, and built-in load balancing.

## Decision
","## Context

We are considering different container orchestration tools to manage our microservices-based architecture. We have evaluated different solutions like Kubernetes, Docker Swarm, and Mesosphere DC/OS. However, we have decided to focus on Docker Swarm due to its simplicity, integration with Docker, and built-in load balancing.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
We are considering different container orchestration tools to manage our microservices-based architecture. We have evaluated different solutions like Kubernetes, Docker Swarm, and Mesosphere DC/OS. However, we have decided to focus on Docker Swarm due to its simplicity, integration with Docker, and built-in load balancing."
"We want our applications to be configurable beyond artifacts/binaries/source, such that one build can behave differently depending on its deployment environment.

  * To accomplish this, we want to use environment variable configuration.

  * We want to manage the configuration by using files that we can version control.

  * We want to provide some developer experience ergonomics, such as knowing what can be configured and any relevant defaults.
",Decided on .env files with related default file and schema file.,"Architectural Decision Record

## Context
We want our applications to be configurable beyond artifacts/binaries/source, such that one build can behave differently depending on its deployment environment.

  * To accomplish this, we want to use environment variable configuration.

  * We want to manage the configuration by using files that we can version control.

  * We want to provide some developer experience ergonomics, such as knowing what can be configured and any relevant defaults.


## Decision
","## Context

We want our applications to be configurable beyond artifacts/binaries/source, such that one build can behave differently depending on its deployment environment.

  * To accomplish this, we want to use environment variable configuration.

  * We want to manage the configuration by using files that we can version control.

  * We want to provide some developer experience ergonomics, such as knowing what can be configured and any relevant defaults.
","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
We want our applications to be configurable beyond artifacts/binaries/source, such that one build can behave differently depending on its deployment environment.

  * To accomplish this, we want to use environment variable configuration.

  * We want to manage the configuration by using files that we can version control.

  * We want to provide some developer experience ergonomics, such as knowing what can be configured and any relevant defaults.
"
"In naming conventions for REST APIs, there are two popular formats: snake_case and camelCase. The snake_case format is where each word in the name is separated by underscores, whereas camelCase is where the first word of the name is in lower case, and the subsequent words have their first letter capitalized. This decision will determine which naming convention should be used for a REST API.
","rivers

- Consistency with existing naming conventions in the project

- Readability and clarity for anyone who may be working on the API

- Alignment with industry best practices for REST API naming conventions

- Ease of implementation and maintenance

## Decision

The snake_case naming convention will be used for REST API endpoints. This choice is driven by the following factors:

1. **Consistency**: The project already uses snake_case naming convention for all endpoints, and it would be beneficial to maintain this convention to ensure consistency across the entire project.

2. **Readability and clarity**: The snake_case convention is more readable and easier to understand. The underscores provide a clear separation between words, making it easier to parse and understand the meaning of the name.

3. **Alignment with industry best practices**: The snake_case convention is widely used in the industry and is considered to be a best practice for REST APIs, making it a good choice for the project.

4. **Ease of implementation and maintenance**: Keeping with the existing naming convention is easier to implement and maintain as all existing code and documentation would need to be updated if a new convention was chosen.
","Architectural Decision Record

## Context
In naming conventions for REST APIs, there are two popular formats: snake_case and camelCase. The snake_case format is where each word in the name is separated by underscores, whereas camelCase is where the first word of the name is in lower case, and the subsequent words have their first letter capitalized. This decision will determine which naming convention should be used for a REST API.


## Decision
","## Context

In naming conventions for REST APIs, there are two popular formats: snake_case and camelCase. The snake_case format is where each word in the name is separated by underscores, whereas camelCase is where the first word of the name is in lower case, and the subsequent words have their first letter capitalized. This decision will determine which naming convention should be used for a REST API.
","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
In naming conventions for REST APIs, there are two popular formats: snake_case and camelCase. The snake_case format is where each word in the name is separated by underscores, whereas camelCase is where the first word of the name is in lower case, and the subsequent words have their first letter capitalized. This decision will determine which naming convention should be used for a REST API.
"
"We need to store secrets, such as passwords, private keys, authentication tokens, etc.

Some of the secrets are user-oriented. For example, our developer wants to be able to use their mobile phone to look up a password to a service.

Some of the secrets are system-oriented. For example, our continuous delivery pipeline needs to be able to look up the credentials for our cloud hosting.
","Bitwarden for user-oriented secrets

Vault by HashiCorp for system-oriented secrets.","Architectural Decision Record

## Context
We need to store secrets, such as passwords, private keys, authentication tokens, etc.

Some of the secrets are user-oriented. For example, our developer wants to be able to use their mobile phone to look up a password to a service.

Some of the secrets are system-oriented. For example, our continuous delivery pipeline needs to be able to look up the credentials for our cloud hosting.


## Decision
","## Context

We need to store secrets, such as passwords, private keys, authentication tokens, etc.

Some of the secrets are user-oriented. For example, our developer wants to be able to use their mobile phone to look up a password to a service.

Some of the secrets are system-oriented. For example, our continuous delivery pipeline needs to be able to look up the credentials for our cloud hosting.
","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
We need to store secrets, such as passwords, private keys, authentication tokens, etc.

Some of the secrets are user-oriented. For example, our developer wants to be able to use their mobile phone to look up a password to a service.

Some of the secrets are system-oriented. For example, our continuous delivery pipeline needs to be able to look up the credentials for our cloud hosting.
"
"We are building a web application for a client that requires a responsive and customizable user interface. The client has asked for consistent and modern styling. The development team has experience using custom CSS but wants to explore CSS frameworks that can save time and effort. We are looking for a framework that is easy to use, customizable, and has a large community for support.
","rivers

- **Easy to use:**  We want a framework that is simple to learn and can be integrated into our development workflow.

- **Customizable:**  We want a framework that can be customized to match the client's brand guidelines and theme.

- **Large community support:**  We want a framework that has an active community of developers and designers that can provide help and resources when needed.

- **Time-saving:**  We want a framework that can save time and effort in styling the UI.

## Considered Options

- **Bootstrap:**  A popular CSS framework with a lot of pre-built components and a large community of developers.

- **Foundation:**  Another popular CSS framework that was designed for mobile-first development.

- **Materialize:**  A CSS framework based on Google's Material Design language with pre-built components for UI development.

- **Tailwind CSS:**  A utility-first CSS framework with a focus on customization and efficiency.

## Decision

After considering the options, we decided to use Tailwind CSS for the following reasons:

- **Easy to use:**  Tailwind's approach is simple and easy to understand. Its utility classes make building a UI faster.

- **Customizable:**  Tailwind allows for customization by providing a set of configuration files that allow developers to modify the framework's default styles with ease.

- **Large community support:**  Tailwind has a very active community of developers and designers that are consistently creating new resources, plugins, and tools.

- **Time-saving:**  Tailwind can save time, enabling developers to focus on other areas of the project.

We will use Tailwind CSS's utility classes to build the UI for our project. The development team will use its atomic and modular design system to write efficient and scalable CSS code. We will leverage Tailwind's pre-built templates and visual components to speed up the development process while utilizing custom styles to match the branding and theme of the client.","Architectural Decision Record

## Context
We are building a web application for a client that requires a responsive and customizable user interface. The client has asked for consistent and modern styling. The development team has experience using custom CSS but wants to explore CSS frameworks that can save time and effort. We are looking for a framework that is easy to use, customizable, and has a large community for support.


## Decision
","## Context

We are building a web application for a client that requires a responsive and customizable user interface. The client has asked for consistent and modern styling. The development team has experience using custom CSS but wants to explore CSS frameworks that can save time and effort. We are looking for a framework that is easy to use, customizable, and has a large community for support.
","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
We are building a web application for a client that requires a responsive and customizable user interface. The client has asked for consistent and modern styling. The development team has experience using custom CSS but wants to explore CSS frameworks that can save time and effort. We are looking for a framework that is easy to use, customizable, and has a large community for support.
"
"Due to the ongoing COVID-19 pandemic, many employees have been working remotely from home. This arrangement has provided benefits such as increased flexibility, improved work-life balance and a decrease in the spread of the virus. Furthermore, numerous employees have expressed that they would like to continue working from home even after the pandemic has ended.","After weighing the pros and cons of allowing employees to work from home, the management team has decided to implement a permanent work from home policy for eligible employees. The policy applies to all full-time employees who can effectively perform their duties from home and have demonstrated success working remotely during the pandemic.

Eligible employees can choose to work from home up to three days per week, while in-office work will be required for the remaining two days. To ensure effective communication and collaboration, regular check-ins with team members and managers will be required. Additionally, employees will be provided with the necessary equipment and resources to perform their duties effectively from home.

The implementation of this policy will be reviewed and revisited regularly to ensure that it continues to benefit both the employees and the company.","Architectural Decision Record

## Context
Due to the ongoing COVID-19 pandemic, many employees have been working remotely from home. This arrangement has provided benefits such as increased flexibility, improved work-life balance and a decrease in the spread of the virus. Furthermore, numerous employees have expressed that they would like to continue working from home even after the pandemic has ended.

## Decision
","## Context

Due to the ongoing COVID-19 pandemic, many employees have been working remotely from home. This arrangement has provided benefits such as increased flexibility, improved work-life balance and a decrease in the spread of the virus. Furthermore, numerous employees have expressed that they would like to continue working from home even after the pandemic has ended.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Due to the ongoing COVID-19 pandemic, many employees have been working remotely from home. This arrangement has provided benefits such as increased flexibility, improved work-life balance and a decrease in the spread of the virus. Furthermore, numerous employees have expressed that they would like to continue working from home even after the pandemic has ended."
"Our web application requires a front-end JavaScript library to provide dynamic and responsive user interfaces. We have evaluated several popular libraries that include React, Angular, and Vue. Based on the evaluation, we have decided to use the Vue library as the primary front-end library for our web application.",We have decided to adopt the Vue front-end JavaScript library,"Architectural Decision Record

## Context
Our web application requires a front-end JavaScript library to provide dynamic and responsive user interfaces. We have evaluated several popular libraries that include React, Angular, and Vue. Based on the evaluation, we have decided to use the Vue library as the primary front-end library for our web application.

## Decision
","## Context

Our web application requires a front-end JavaScript library to provide dynamic and responsive user interfaces. We have evaluated several popular libraries that include React, Angular, and Vue. Based on the evaluation, we have decided to use the Vue library as the primary front-end library for our web application.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Our web application requires a front-end JavaScript library to provide dynamic and responsive user interfaces. We have evaluated several popular libraries that include React, Angular, and Vue. Based on the evaluation, we have decided to use the Vue library as the primary front-end library for our web application."
"We need to decide on whether to use Python as a programming language for our project. Our project involves data analysis, machine learning, and web development.",We have decided to use Python as our primary programming language for our project.,"Architectural Decision Record

## Context
We need to decide on whether to use Python as a programming language for our project. Our project involves data analysis, machine learning, and web development.

## Decision
","## Context

We need to decide on whether to use Python as a programming language for our project. Our project involves data analysis, machine learning, and web development.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
We need to decide on whether to use Python as a programming language for our project. Our project involves data analysis, machine learning, and web development."
"We need to select a container orchestration platform for our growing cloud-native application portfolio. Our current legacy platform deployment is too slow, and not agile enough to keep up with our growing needs. We’re looking for a system that will allow us to scale our services in the most efficient way possible without compromising on agility or ease of use.
","After conducting a thorough analysis of each container orchestration platform, we have decided to adopt Kubernetes as the best option for our enterprise needs.","Architectural Decision Record

## Context
We need to select a container orchestration platform for our growing cloud-native application portfolio. Our current legacy platform deployment is too slow, and not agile enough to keep up with our growing needs. We’re looking for a system that will allow us to scale our services in the most efficient way possible without compromising on agility or ease of use.


## Decision
","## Context

We need to select a container orchestration platform for our growing cloud-native application portfolio. Our current legacy platform deployment is too slow, and not agile enough to keep up with our growing needs. We’re looking for a system that will allow us to scale our services in the most efficient way possible without compromising on agility or ease of use.
","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
We need to select a container orchestration platform for our growing cloud-native application portfolio. Our current legacy platform deployment is too slow, and not agile enough to keep up with our growing needs. We’re looking for a system that will allow us to scale our services in the most efficient way possible without compromising on agility or ease of use.
"
"We are building a new web application that requires a scalable and efficient front-end architecture. After conducting a thorough analysis of various front-end frameworks, we have chosen SvelteKit as our preferred framework. ","We have decided to implement the SvelteKit front-end architecture for our web application. SvelteKit provides an advanced front-end development system that is fast, efficient, and flexible. It can be used for optimizing large and small applications.","Architectural Decision Record

## Context
We are building a new web application that requires a scalable and efficient front-end architecture. After conducting a thorough analysis of various front-end frameworks, we have chosen SvelteKit as our preferred framework. 

## Decision
","## Context

We are building a new web application that requires a scalable and efficient front-end architecture. After conducting a thorough analysis of various front-end frameworks, we have chosen SvelteKit as our preferred framework. ","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
We are building a new web application that requires a scalable and efficient front-end architecture. After conducting a thorough analysis of various front-end frameworks, we have chosen SvelteKit as our preferred framework. "
"Google Cloud Platform (GCP) is a prominent cloud computing platform that offers various cloud services, including computing, storage, and networking solutions. This ADR aims to document the architectural decisions made for developing and implementing a GCP-based infrastructure for our organization.","Our organization has decided to use Google Cloud Platform as the cloud infrastructure for our application. The primary considerations for this decision are:

   - Cost-effectiveness

   - Scalability

   - Reliability

   - Flexibility","Architectural Decision Record

## Context
Google Cloud Platform (GCP) is a prominent cloud computing platform that offers various cloud services, including computing, storage, and networking solutions. This ADR aims to document the architectural decisions made for developing and implementing a GCP-based infrastructure for our organization.

## Decision
","## Context

Google Cloud Platform (GCP) is a prominent cloud computing platform that offers various cloud services, including computing, storage, and networking solutions. This ADR aims to document the architectural decisions made for developing and implementing a GCP-based infrastructure for our organization.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Google Cloud Platform (GCP) is a prominent cloud computing platform that offers various cloud services, including computing, storage, and networking solutions. This ADR aims to document the architectural decisions made for developing and implementing a GCP-based infrastructure for our organization."
"Our project involves developing three major categories of software:

  * Front-end GUIs
  * Middleware services
  * Back-end servers

When we develop, our source code management (SCM) version control system (VCS) is git.

We need to choose how we use git to organize our code.

The top-level choice is to organize as a ""monorepo"" or ""polyrepo"" or ""hybrid"":

  * Monorepo means we put all pieces into one big repo
  * Polyrepo means we put each piece in its own repo
  * Hybrid means some mix of monorepo and polyrepo

For more please see https://github.com/joelparkerhenderson/monorepo-vs-polyrepo
","Monorepo when an organization/team/project is relatively small, and rapid iteration is higher priority than sustaining stability.

Polyrepo when an organization/team/project is relatively large, and sustaining stability is higher priority than rapid iteration.","Architectural Decision Record

## Context
Our project involves developing three major categories of software:

  * Front-end GUIs
  * Middleware services
  * Back-end servers

When we develop, our source code management (SCM) version control system (VCS) is git.

We need to choose how we use git to organize our code.

The top-level choice is to organize as a ""monorepo"" or ""polyrepo"" or ""hybrid"":

  * Monorepo means we put all pieces into one big repo
  * Polyrepo means we put each piece in its own repo
  * Hybrid means some mix of monorepo and polyrepo

For more please see https://github.com/joelparkerhenderson/monorepo-vs-polyrepo


## Decision
","## Context

Our project involves developing three major categories of software:

  * Front-end GUIs
  * Middleware services
  * Back-end servers

When we develop, our source code management (SCM) version control system (VCS) is git.

We need to choose how we use git to organize our code.

The top-level choice is to organize as a ""monorepo"" or ""polyrepo"" or ""hybrid"":

  * Monorepo means we put all pieces into one big repo
  * Polyrepo means we put each piece in its own repo
  * Hybrid means some mix of monorepo and polyrepo

For more please see https://github.com/joelparkerhenderson/monorepo-vs-polyrepo
","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Our project involves developing three major categories of software:

  * Front-end GUIs
  * Middleware services
  * Back-end servers

When we develop, our source code management (SCM) version control system (VCS) is git.

We need to choose how we use git to organize our code.

The top-level choice is to organize as a ""monorepo"" or ""polyrepo"" or ""hybrid"":

  * Monorepo means we put all pieces into one big repo
  * Polyrepo means we put each piece in its own repo
  * Hybrid means some mix of monorepo and polyrepo

For more please see https://github.com/joelparkerhenderson/monorepo-vs-polyrepo
"
"We want to use devops to build, integrate, deploy, and host our projects. We are considering Microsoft Azure DevOps.

  * We want developer experience to be fast and reliable, for the setup of the devops e.g. configuring as well as ongoing use e.g. fast build times.
  
  * We want to consider using Microsoft Azure as whole, for hosting the project apps, databases, etc.
",Decided against Microsoft Azure DevOps.,"Architectural Decision Record

## Context
We want to use devops to build, integrate, deploy, and host our projects. We are considering Microsoft Azure DevOps.

  * We want developer experience to be fast and reliable, for the setup of the devops e.g. configuring as well as ongoing use e.g. fast build times.
  
  * We want to consider using Microsoft Azure as whole, for hosting the project apps, databases, etc.


## Decision
","## Context

We want to use devops to build, integrate, deploy, and host our projects. We are considering Microsoft Azure DevOps.

  * We want developer experience to be fast and reliable, for the setup of the devops e.g. configuring as well as ongoing use e.g. fast build times.
  
  * We want to consider using Microsoft Azure as whole, for hosting the project apps, databases, etc.
","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
We want to use devops to build, integrate, deploy, and host our projects. We are considering Microsoft Azure DevOps.

  * We want developer experience to be fast and reliable, for the setup of the devops e.g. configuring as well as ongoing use e.g. fast build times.
  
  * We want to consider using Microsoft Azure as whole, for hosting the project apps, databases, etc.
"
"We need to decide on which database management system (DBMS) to use for Project X. The database will be used to store and manage large amounts of data from multiple sources. We need a DBMS that can handle transactions, offer scalability, and provide high reliability and security. Among various options available, we are considering MySQL as a possible choice.
","onsiderations

- Ease of use and maintenance

- Community support and resources

- Performance and scalability

- Security and reliability

- Cost and licensing

- Compatibility with our technology stack

### Considered Options

- MySQL

- PostgreSQL

- Oracle

- Microsoft SQL Server

- MongoDB

## Decision

After evaluating the above options based on our decision considerations, we have decided to choose MySQL as our DBMS for Project X.

MySQL is a popular open-source system with a strong development community and a large pool of resources for problem-solving and knowledge sharing. It is well-known for its excellent performance and scalability capabilities, making it ideal for handling vast amounts of data with high levels of efficiency. The platform is secure, reliable, and has a wide range of features that are essential for our project, including ACID compliance for transactions, flexible data model, and support for various programming languages and frameworks.

MySQL is also compatible with the majority of our technology stack, including our web development framework, hosting solutions, and other essential tools. Plus, its cost and licensing terms are competitive compared to other proprietary systems like Oracle and Microsoft SQL Server.","Architectural Decision Record

## Context
We need to decide on which database management system (DBMS) to use for Project X. The database will be used to store and manage large amounts of data from multiple sources. We need a DBMS that can handle transactions, offer scalability, and provide high reliability and security. Among various options available, we are considering MySQL as a possible choice.


## Decision
","## Context

We need to decide on which database management system (DBMS) to use for Project X. The database will be used to store and manage large amounts of data from multiple sources. We need a DBMS that can handle transactions, offer scalability, and provide high reliability and security. Among various options available, we are considering MySQL as a possible choice.
","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
We need to decide on which database management system (DBMS) to use for Project X. The database will be used to store and manage large amounts of data from multiple sources. We need a DBMS that can handle transactions, offer scalability, and provide high reliability and security. Among various options available, we are considering MySQL as a possible choice.
"
Our organization is planning to develop a web application that manages customer data. We have chosen Python as the programming language and are considering Django as the web framework for the development of the application.,We have decided to use the Django web framework for the development of the web application. Django provides a robust set of tools and features for building web applications quickly and efficiently.,"Architectural Decision Record

## Context
Our organization is planning to develop a web application that manages customer data. We have chosen Python as the programming language and are considering Django as the web framework for the development of the application.

## Decision
","## Context

Our organization is planning to develop a web application that manages customer data. We have chosen Python as the programming language and are considering Django as the web framework for the development of the application.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Our organization is planning to develop a web application that manages customer data. We have chosen Python as the programming language and are considering Django as the web framework for the development of the application."
"We want to use a CSS framework to create our web applications:

  * We want user experience to be fast and reliable, on all popular browsers and screen sizes.

  * We want rapid iteration on design, layout, UI/UX, etc.

  * We want responsive applications, especially for smaller screens such as on mobile devices, larger screens such as on 4K widescreens, and dynamic screens such as rotatable displays.  
",Decided on Bulma.,"Architectural Decision Record

## Context
We want to use a CSS framework to create our web applications:

  * We want user experience to be fast and reliable, on all popular browsers and screen sizes.

  * We want rapid iteration on design, layout, UI/UX, etc.

  * We want responsive applications, especially for smaller screens such as on mobile devices, larger screens such as on 4K widescreens, and dynamic screens such as rotatable displays.  


## Decision
","## Context

We want to use a CSS framework to create our web applications:

  * We want user experience to be fast and reliable, on all popular browsers and screen sizes.

  * We want rapid iteration on design, layout, UI/UX, etc.

  * We want responsive applications, especially for smaller screens such as on mobile devices, larger screens such as on 4K widescreens, and dynamic screens such as rotatable displays.  
","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
We want to use a CSS framework to create our web applications:

  * We want user experience to be fast and reliable, on all popular browsers and screen sizes.

  * We want rapid iteration on design, layout, UI/UX, etc.

  * We want responsive applications, especially for smaller screens such as on mobile devices, larger screens such as on 4K widescreens, and dynamic screens such as rotatable displays.  
"
"The company has been experiencing challenges with the development process as a result of manual testing, build processes, and deployments. This has led to frequent delays, errors and inconsistencies in the quality and delivery of the software.",To implement continuous integration as a solution to the challenges experienced in the development process.,"Architectural Decision Record

## Context
The company has been experiencing challenges with the development process as a result of manual testing, build processes, and deployments. This has led to frequent delays, errors and inconsistencies in the quality and delivery of the software.

## Decision
","## Context

The company has been experiencing challenges with the development process as a result of manual testing, build processes, and deployments. This has led to frequent delays, errors and inconsistencies in the quality and delivery of the software.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
The company has been experiencing challenges with the development process as a result of manual testing, build processes, and deployments. This has led to frequent delays, errors and inconsistencies in the quality and delivery of the software."
"We need to choose programming languages for our software. We have two major needs: a front-end programming language suitable for web applications, and a back-end programming language suitable for server applications.
","We are choosing TypeScript for the front-end.

We are choosing Rust for the back-end.","Architectural Decision Record

## Context
We need to choose programming languages for our software. We have two major needs: a front-end programming language suitable for web applications, and a back-end programming language suitable for server applications.


## Decision
","## Context

We need to choose programming languages for our software. We have two major needs: a front-end programming language suitable for web applications, and a back-end programming language suitable for server applications.
","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
We need to choose programming languages for our software. We have two major needs: a front-end programming language suitable for web applications, and a back-end programming language suitable for server applications.
"
"We are designing an API for a new service that will be used by multiple clients. We have been considering two options for implementing the API: using JSON over HTTP or using gRPC.

JSON over HTTP is a widely-used approach for building APIs, and it is supported by many programming languages and frameworks. This approach is simple, lightweight, and easy to understand, making it a good choice for many projects. However, it can be less efficient than other options, especially when it comes to handling large amounts of data.

gRPC, on the other hand, is a newer technology that offers a more efficient way of building APIs. It uses binary serialization to transfer data, which can be faster and more compact than using JSON. gRPC also supports bidirectional streaming, making it a good choice for real-time applications.","After considering the pros and cons of both options, we have decided to use gRPC for our API. Although JSON over HTTP is a simpler option, we believe that gRPC will provide a more efficient and scalable solution for our service. We also anticipate that our API will handle a large amount of data, and gRPC's binary serialization will be more efficient for this use case.

In addition, we believe that gRPC's support for bidirectional streaming will be beneficial for real-time applications that we may develop in the future.","Architectural Decision Record

## Context
We are designing an API for a new service that will be used by multiple clients. We have been considering two options for implementing the API: using JSON over HTTP or using gRPC.

JSON over HTTP is a widely-used approach for building APIs, and it is supported by many programming languages and frameworks. This approach is simple, lightweight, and easy to understand, making it a good choice for many projects. However, it can be less efficient than other options, especially when it comes to handling large amounts of data.

gRPC, on the other hand, is a newer technology that offers a more efficient way of building APIs. It uses binary serialization to transfer data, which can be faster and more compact than using JSON. gRPC also supports bidirectional streaming, making it a good choice for real-time applications.

## Decision
","## Context

We are designing an API for a new service that will be used by multiple clients. We have been considering two options for implementing the API: using JSON over HTTP or using gRPC.

JSON over HTTP is a widely-used approach for building APIs, and it is supported by many programming languages and frameworks. This approach is simple, lightweight, and easy to understand, making it a good choice for many projects. However, it can be less efficient than other options, especially when it comes to handling large amounts of data.

gRPC, on the other hand, is a newer technology that offers a more efficient way of building APIs. It uses binary serialization to transfer data, which can be faster and more compact than using JSON. gRPC also supports bidirectional streaming, making it a good choice for real-time applications.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
We are designing an API for a new service that will be used by multiple clients. We have been considering two options for implementing the API: using JSON over HTTP or using gRPC.

JSON over HTTP is a widely-used approach for building APIs, and it is supported by many programming languages and frameworks. This approach is simple, lightweight, and easy to understand, making it a good choice for many projects. However, it can be less efficient than other options, especially when it comes to handling large amounts of data.

gRPC, on the other hand, is a newer technology that offers a more efficient way of building APIs. It uses binary serialization to transfer data, which can be faster and more compact than using JSON. gRPC also supports bidirectional streaming, making it a good choice for real-time applications."
"Our organization is currently evaluating options for the most suitable database management system to be used in our upcoming project. We have narrowed our search down to two options – PostgreSQL and MySQL. After careful consideration, we have decided to use PostgreSQL due to its advanced features and performance capabilities.",We will use PostgreSQL as the preferred database management system for our organization's upcoming project.,"Architectural Decision Record

## Context
Our organization is currently evaluating options for the most suitable database management system to be used in our upcoming project. We have narrowed our search down to two options – PostgreSQL and MySQL. After careful consideration, we have decided to use PostgreSQL due to its advanced features and performance capabilities.

## Decision
","## Context

Our organization is currently evaluating options for the most suitable database management system to be used in our upcoming project. We have narrowed our search down to two options – PostgreSQL and MySQL. After careful consideration, we have decided to use PostgreSQL due to its advanced features and performance capabilities.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Our organization is currently evaluating options for the most suitable database management system to be used in our upcoming project. We have narrowed our search down to two options – PostgreSQL and MySQL. After careful consideration, we have decided to use PostgreSQL due to its advanced features and performance capabilities."
"We want to be able to track when things happen by using timestamps and by using a consistent timestamp format that works well across all our systems and third-party systems.

We interact with systems that have different timestamp formats:

* JSON messages do not have a native timestamp format, so we need to choose how to convert a timestamp to a string, and convert a string to a timestamp, i.e. how to serialize/deserialize.

* Some applications are set to use local time, rather than UTC time. This can be convenient for projects that must adjust to local time, such as projects that trigger events that are based on local time.

* Some systems have different time precision needs and capabilities, such as using a time resolution of seconds vs. milliseconds vs. nanoseconds. For example, the Linux operating system `date` command uses a default time precision of seconds, whereas the Nasdaq stock exchange wants a default time precision of nanoseconds.
","We choose the timestamp standard format ISO 8601 with nanosecond precision, specifically ""YYYY-MM-DDTHH:MM:SS.NNNNNNNNNZ"".

The format shows the year, month, day, hour, minute, second, nanoseconds, and Zulu time zone a.k.a. UTC, GMT.","Architectural Decision Record

## Context
We want to be able to track when things happen by using timestamps and by using a consistent timestamp format that works well across all our systems and third-party systems.

We interact with systems that have different timestamp formats:

* JSON messages do not have a native timestamp format, so we need to choose how to convert a timestamp to a string, and convert a string to a timestamp, i.e. how to serialize/deserialize.

* Some applications are set to use local time, rather than UTC time. This can be convenient for projects that must adjust to local time, such as projects that trigger events that are based on local time.

* Some systems have different time precision needs and capabilities, such as using a time resolution of seconds vs. milliseconds vs. nanoseconds. For example, the Linux operating system `date` command uses a default time precision of seconds, whereas the Nasdaq stock exchange wants a default time precision of nanoseconds.


## Decision
","## Context

We want to be able to track when things happen by using timestamps and by using a consistent timestamp format that works well across all our systems and third-party systems.

We interact with systems that have different timestamp formats:

* JSON messages do not have a native timestamp format, so we need to choose how to convert a timestamp to a string, and convert a string to a timestamp, i.e. how to serialize/deserialize.

* Some applications are set to use local time, rather than UTC time. This can be convenient for projects that must adjust to local time, such as projects that trigger events that are based on local time.

* Some systems have different time precision needs and capabilities, such as using a time resolution of seconds vs. milliseconds vs. nanoseconds. For example, the Linux operating system `date` command uses a default time precision of seconds, whereas the Nasdaq stock exchange wants a default time precision of nanoseconds.
","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
We want to be able to track when things happen by using timestamps and by using a consistent timestamp format that works well across all our systems and third-party systems.

We interact with systems that have different timestamp formats:

* JSON messages do not have a native timestamp format, so we need to choose how to convert a timestamp to a string, and convert a string to a timestamp, i.e. how to serialize/deserialize.

* Some applications are set to use local time, rather than UTC time. This can be convenient for projects that must adjust to local time, such as projects that trigger events that are based on local time.

* Some systems have different time precision needs and capabilities, such as using a time resolution of seconds vs. milliseconds vs. nanoseconds. For example, the Linux operating system `date` command uses a default time precision of seconds, whereas the Nasdaq stock exchange wants a default time precision of nanoseconds.
"
"We are designing a new application that requires storing and retrieving data in a scalable and performant manner. We have identified three types of database technologies that are commonly used: relational databases, document databases, and event databases.

Relational databases store data in tables with fixed schemas and enforce strict data integrity constraints. They are suitable for applications that require complex data relationships and transactions. Examples include MySQL, PostgreSQL, and Oracle.

Document databases store data in JSON-like documents and are schema-less. They are well-suited for applications that require flexible data models and scaling horizontally. Examples include MongoDB, Couchbase, and Amazon DynamoDB.

Event databases store data as a series of events, capturing every change to the data. They are suitable for applications that require auditing, event sourcing, and complex data processing. Examples include Apache Kafka, Apache Pulsar, and AWS Kinesis.
","After carefully evaluating the requirements and constraints of our application, we have decided to use a document database.","Architectural Decision Record

## Context
We are designing a new application that requires storing and retrieving data in a scalable and performant manner. We have identified three types of database technologies that are commonly used: relational databases, document databases, and event databases.

Relational databases store data in tables with fixed schemas and enforce strict data integrity constraints. They are suitable for applications that require complex data relationships and transactions. Examples include MySQL, PostgreSQL, and Oracle.

Document databases store data in JSON-like documents and are schema-less. They are well-suited for applications that require flexible data models and scaling horizontally. Examples include MongoDB, Couchbase, and Amazon DynamoDB.

Event databases store data as a series of events, capturing every change to the data. They are suitable for applications that require auditing, event sourcing, and complex data processing. Examples include Apache Kafka, Apache Pulsar, and AWS Kinesis.


## Decision
","## Context

We are designing a new application that requires storing and retrieving data in a scalable and performant manner. We have identified three types of database technologies that are commonly used: relational databases, document databases, and event databases.

Relational databases store data in tables with fixed schemas and enforce strict data integrity constraints. They are suitable for applications that require complex data relationships and transactions. Examples include MySQL, PostgreSQL, and Oracle.

Document databases store data in JSON-like documents and are schema-less. They are well-suited for applications that require flexible data models and scaling horizontally. Examples include MongoDB, Couchbase, and Amazon DynamoDB.

Event databases store data as a series of events, capturing every change to the data. They are suitable for applications that require auditing, event sourcing, and complex data processing. Examples include Apache Kafka, Apache Pulsar, and AWS Kinesis.
","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
We are designing a new application that requires storing and retrieving data in a scalable and performant manner. We have identified three types of database technologies that are commonly used: relational databases, document databases, and event databases.

Relational databases store data in tables with fixed schemas and enforce strict data integrity constraints. They are suitable for applications that require complex data relationships and transactions. Examples include MySQL, PostgreSQL, and Oracle.

Document databases store data in JSON-like documents and are schema-less. They are well-suited for applications that require flexible data models and scaling horizontally. Examples include MongoDB, Couchbase, and Amazon DynamoDB.

Event databases store data as a series of events, capturing every change to the data. They are suitable for applications that require auditing, event sourcing, and complex data processing. Examples include Apache Kafka, Apache Pulsar, and AWS Kinesis.
"
"The organization is planning to migrate from traditional on-premises infrastructure to cloud infrastructure. In order to achieve this, the organization has evaluated multiple cloud service providers such as Amazon Web Services (AWS), Google Cloud Platform (GCP), Microsoft Azure, and IBM Cloud. Each provider has its own set of features, benefits, and pricing structure. After a detailed analysis, it was concluded that Microsoft Azure is the most suitable option for our organization.",Adopt Microsoft Azure as the cloud infrastructure for our organization.,"Architectural Decision Record

## Context
The organization is planning to migrate from traditional on-premises infrastructure to cloud infrastructure. In order to achieve this, the organization has evaluated multiple cloud service providers such as Amazon Web Services (AWS), Google Cloud Platform (GCP), Microsoft Azure, and IBM Cloud. Each provider has its own set of features, benefits, and pricing structure. After a detailed analysis, it was concluded that Microsoft Azure is the most suitable option for our organization.

## Decision
","## Context

The organization is planning to migrate from traditional on-premises infrastructure to cloud infrastructure. In order to achieve this, the organization has evaluated multiple cloud service providers such as Amazon Web Services (AWS), Google Cloud Platform (GCP), Microsoft Azure, and IBM Cloud. Each provider has its own set of features, benefits, and pricing structure. After a detailed analysis, it was concluded that Microsoft Azure is the most suitable option for our organization.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
The organization is planning to migrate from traditional on-premises infrastructure to cloud infrastructure. In order to achieve this, the organization has evaluated multiple cloud service providers such as Amazon Web Services (AWS), Google Cloud Platform (GCP), Microsoft Azure, and IBM Cloud. Each provider has its own set of features, benefits, and pricing structure. After a detailed analysis, it was concluded that Microsoft Azure is the most suitable option for our organization."
"Our organization has been primarily using Java programming language for building web applications. However, we have been experiencing some challenges due to Java's verbosity and the complexity of some frameworks. Also, our team has shown interest in exploring modern programming languages for more efficient and robust development. After conducting research, we have identified Go as a potential solution to the problems we face with Java.",We will adopt Go programming language for building web applications going forward.,"Architectural Decision Record

## Context
Our organization has been primarily using Java programming language for building web applications. However, we have been experiencing some challenges due to Java's verbosity and the complexity of some frameworks. Also, our team has shown interest in exploring modern programming languages for more efficient and robust development. After conducting research, we have identified Go as a potential solution to the problems we face with Java.

## Decision
","## Context

Our organization has been primarily using Java programming language for building web applications. However, we have been experiencing some challenges due to Java's verbosity and the complexity of some frameworks. Also, our team has shown interest in exploring modern programming languages for more efficient and robust development. After conducting research, we have identified Go as a potential solution to the problems we face with Java.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Our organization has been primarily using Java programming language for building web applications. However, we have been experiencing some challenges due to Java's verbosity and the complexity of some frameworks. Also, our team has shown interest in exploring modern programming languages for more efficient and robust development. After conducting research, we have identified Go as a potential solution to the problems we face with Java."
"We are developing a web application using a modern front-end JavaScript framework. We want to choose a library that is lightweight, fast, and easy to use. We have evaluated several libraries, including React, Vue, and Angular. We have also heard about Svelte, which claims to be a new kind of lightweight framework. We need to evaluate Svelte and decide whether it is a good fit for our application.","After considering the benefits and drawbacks of each library, we have decided to use Svelte as our front-end JavaScript library for the following reasons:

1. **Lightweight:**  Svelte is one of the lightest libraries available, which will help us keep our application size small and improve page load times.

2. **Fast:**  Svelte is known for its fast rendering speed, which will make our application feel responsive and snappy.

3. **Easy to use:**  Svelte's syntax is simple and easy to understand, which will make it easier for our team to adopt it and write clean code.

4. **Optimized for UI rendering:**  Svelte is optimized for rendering UI components, which will make it easier for us to create dynamic and reusable UI elements.

5. **Good documentation and community support:**  Svelte has a growing community and good documentation, which will help us get started quickly and resolve any issues that arise.","Architectural Decision Record

## Context
We are developing a web application using a modern front-end JavaScript framework. We want to choose a library that is lightweight, fast, and easy to use. We have evaluated several libraries, including React, Vue, and Angular. We have also heard about Svelte, which claims to be a new kind of lightweight framework. We need to evaluate Svelte and decide whether it is a good fit for our application.

## Decision
","## Context

We are developing a web application using a modern front-end JavaScript framework. We want to choose a library that is lightweight, fast, and easy to use. We have evaluated several libraries, including React, Vue, and Angular. We have also heard about Svelte, which claims to be a new kind of lightweight framework. We need to evaluate Svelte and decide whether it is a good fit for our application.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
We are developing a web application using a modern front-end JavaScript framework. We want to choose a library that is lightweight, fast, and easy to use. We have evaluated several libraries, including React, Vue, and Angular. We have also heard about Svelte, which claims to be a new kind of lightweight framework. We need to evaluate Svelte and decide whether it is a good fit for our application."
"- The organization is developing a web application with complex business logic and multiple integrations with third-party services.

- The team has experience in building web applications with Ruby on Rails.

- There is a need to choose a web framework that promotes rapid development, scalability, and maintainability.",- Use Ruby on Rails as the framework for developing the web application.,"Architectural Decision Record

## Context
- The organization is developing a web application with complex business logic and multiple integrations with third-party services.

- The team has experience in building web applications with Ruby on Rails.

- There is a need to choose a web framework that promotes rapid development, scalability, and maintainability.

## Decision
","## Context

- The organization is developing a web application with complex business logic and multiple integrations with third-party services.

- The team has experience in building web applications with Ruby on Rails.

- There is a need to choose a web framework that promotes rapid development, scalability, and maintainability.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
- The organization is developing a web application with complex business logic and multiple integrations with third-party services.

- The team has experience in building web applications with Ruby on Rails.

- There is a need to choose a web framework that promotes rapid development, scalability, and maintainability."
"We intended to build a web-based application that could run on multiple operating systems. Our project required a language that could provide strong security features, handle concurrency, support large code bases, and be versatile enough to meet our future growth requirements. We evaluated several languages, considering their strengths and limitations, before deciding on Java.

### Considerations

1. **Security:**  Java provides excellent security features through its well-defined security policies and access control. It also incorporates features like bytecode verification, which helps prevent malicious software from running on a system.

2. **Concurrency:**  Java has built-in support for multithreading, which allows applications to perform several tasks simultaneously. This feature makes Java an excellent choice for developing large, complex applications with multiple features and functionalities.

3. **Large codebases:**  Java supports object-oriented programming, which makes it a suitable choice for developing large codebases. Its modular nature and use of encapsulation and abstraction patterns further add to the software development process's ease.

4. **Versatility:**  Java is versatile and provides the ability to use a wide range of libraries and frameworks, making it an excellent choice for both web-based and enterprise-level applications.","We have decided to use the Java programming language for our project, considering its security features, concurrency support, ability to handle large codebases, and versatility. This decision aligns with our project requirements, and we believe that choosing Java will ensure its success. ","Architectural Decision Record

## Context
We intended to build a web-based application that could run on multiple operating systems. Our project required a language that could provide strong security features, handle concurrency, support large code bases, and be versatile enough to meet our future growth requirements. We evaluated several languages, considering their strengths and limitations, before deciding on Java.

### Considerations

1. **Security:**  Java provides excellent security features through its well-defined security policies and access control. It also incorporates features like bytecode verification, which helps prevent malicious software from running on a system.

2. **Concurrency:**  Java has built-in support for multithreading, which allows applications to perform several tasks simultaneously. This feature makes Java an excellent choice for developing large, complex applications with multiple features and functionalities.

3. **Large codebases:**  Java supports object-oriented programming, which makes it a suitable choice for developing large codebases. Its modular nature and use of encapsulation and abstraction patterns further add to the software development process's ease.

4. **Versatility:**  Java is versatile and provides the ability to use a wide range of libraries and frameworks, making it an excellent choice for both web-based and enterprise-level applications.

## Decision
","## Context

We intended to build a web-based application that could run on multiple operating systems. Our project required a language that could provide strong security features, handle concurrency, support large code bases, and be versatile enough to meet our future growth requirements. We evaluated several languages, considering their strengths and limitations, before deciding on Java.

### Considerations

1. **Security:**  Java provides excellent security features through its well-defined security policies and access control. It also incorporates features like bytecode verification, which helps prevent malicious software from running on a system.

2. **Concurrency:**  Java has built-in support for multithreading, which allows applications to perform several tasks simultaneously. This feature makes Java an excellent choice for developing large, complex applications with multiple features and functionalities.

3. **Large codebases:**  Java supports object-oriented programming, which makes it a suitable choice for developing large codebases. Its modular nature and use of encapsulation and abstraction patterns further add to the software development process's ease.

4. **Versatility:**  Java is versatile and provides the ability to use a wide range of libraries and frameworks, making it an excellent choice for both web-based and enterprise-level applications.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
We intended to build a web-based application that could run on multiple operating systems. Our project required a language that could provide strong security features, handle concurrency, support large code bases, and be versatile enough to meet our future growth requirements. We evaluated several languages, considering their strengths and limitations, before deciding on Java.

### Considerations

1. **Security:**  Java provides excellent security features through its well-defined security policies and access control. It also incorporates features like bytecode verification, which helps prevent malicious software from running on a system.

2. **Concurrency:**  Java has built-in support for multithreading, which allows applications to perform several tasks simultaneously. This feature makes Java an excellent choice for developing large, complex applications with multiple features and functionalities.

3. **Large codebases:**  Java supports object-oriented programming, which makes it a suitable choice for developing large codebases. Its modular nature and use of encapsulation and abstraction patterns further add to the software development process's ease.

4. **Versatility:**  Java is versatile and provides the ability to use a wide range of libraries and frameworks, making it an excellent choice for both web-based and enterprise-level applications."
"We need to choose a front-end JavaScript library for our application to develop a responsive and interactive user interface.
  
## Option Considered

1. React
  
2. Vue

3. Svelte
  ",We will use React as our front-end JavaScript library.,"Architectural Decision Record

## Context
We need to choose a front-end JavaScript library for our application to develop a responsive and interactive user interface.
  
## Option Considered

1. React
  
2. Vue

3. Svelte
  

## Decision
","## Context

We need to choose a front-end JavaScript library for our application to develop a responsive and interactive user interface.
  
## Option Considered

1. React
  
2. Vue

3. Svelte
  ","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
We need to choose a front-end JavaScript library for our application to develop a responsive and interactive user interface.
  
## Option Considered

1. React
  
2. Vue

3. Svelte
  "
"Programming code editors are an essential tool for developers to write and edit code. There are numerous code editors available, each with its own set of features, advantages, and disadvantages. The purpose of this ADR is to document the architectural decisions made for programming code editors.

## Priorities

The architecture for programming code editors should prioritize the following:

* **Modularity**: The code editor should be designed in a modular way, allowing developers to customize and extend it as needed. This allows for a flexible architecture that can adapt to the needs of different developers and teams.

* **Performance**: The code editor should be performant and responsive, allowing developers to work efficiently without being slowed down by the tool they are using.

* **User Interface**: The user interface should be intuitive and easy to use, allowing developers to focus on their code rather than struggling with the editor.

* **Extensibility**: The code editor should be designed to allow for easy extension with third-party plugins and integrations.

* **Compatibility**: The code editor should be compatible with a wide range of programming languages and technologies, making it a useful tool for a broad range of developers.","Based on these priorities, the architecture for programming code editors should be designed with the following components:

* **Core**: This component provides the basic functionality of the code editor, such as syntax highlighting, text editing, and file management.

* **UI**: This component provides the user interface for the code editor, including menus, toolbars, and keyboard shortcuts.

* **Plugins**: This component allows developers to extend the functionality of the code editor by installing third-party plugins. Plugins can provide additional features, such as code completion, linting, or debugging.

* **Integrations**: This component allows the code editor to integrate with other tools and technologies, such as version control systems, build systems, or debugging tools.","Architectural Decision Record

## Context
Programming code editors are an essential tool for developers to write and edit code. There are numerous code editors available, each with its own set of features, advantages, and disadvantages. The purpose of this ADR is to document the architectural decisions made for programming code editors.

## Priorities

The architecture for programming code editors should prioritize the following:

* **Modularity**: The code editor should be designed in a modular way, allowing developers to customize and extend it as needed. This allows for a flexible architecture that can adapt to the needs of different developers and teams.

* **Performance**: The code editor should be performant and responsive, allowing developers to work efficiently without being slowed down by the tool they are using.

* **User Interface**: The user interface should be intuitive and easy to use, allowing developers to focus on their code rather than struggling with the editor.

* **Extensibility**: The code editor should be designed to allow for easy extension with third-party plugins and integrations.

* **Compatibility**: The code editor should be compatible with a wide range of programming languages and technologies, making it a useful tool for a broad range of developers.

## Decision
","## Context

Programming code editors are an essential tool for developers to write and edit code. There are numerous code editors available, each with its own set of features, advantages, and disadvantages. The purpose of this ADR is to document the architectural decisions made for programming code editors.

## Priorities

The architecture for programming code editors should prioritize the following:

* **Modularity**: The code editor should be designed in a modular way, allowing developers to customize and extend it as needed. This allows for a flexible architecture that can adapt to the needs of different developers and teams.

* **Performance**: The code editor should be performant and responsive, allowing developers to work efficiently without being slowed down by the tool they are using.

* **User Interface**: The user interface should be intuitive and easy to use, allowing developers to focus on their code rather than struggling with the editor.

* **Extensibility**: The code editor should be designed to allow for easy extension with third-party plugins and integrations.

* **Compatibility**: The code editor should be compatible with a wide range of programming languages and technologies, making it a useful tool for a broad range of developers.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Programming code editors are an essential tool for developers to write and edit code. There are numerous code editors available, each with its own set of features, advantages, and disadvantages. The purpose of this ADR is to document the architectural decisions made for programming code editors.

## Priorities

The architecture for programming code editors should prioritize the following:

* **Modularity**: The code editor should be designed in a modular way, allowing developers to customize and extend it as needed. This allows for a flexible architecture that can adapt to the needs of different developers and teams.

* **Performance**: The code editor should be performant and responsive, allowing developers to work efficiently without being slowed down by the tool they are using.

* **User Interface**: The user interface should be intuitive and easy to use, allowing developers to focus on their code rather than struggling with the editor.

* **Extensibility**: The code editor should be designed to allow for easy extension with third-party plugins and integrations.

* **Compatibility**: The code editor should be compatible with a wide range of programming languages and technologies, making it a useful tool for a broad range of developers."
"Our organization has been considering a switch to a 4-day workweek. The current work schedule of 5 days per week, 8 hours per day, has resulted in low productivity, employee burnout, and high turnover rates. The proposed change aims to increase employee satisfaction, engagement, and retention, and ultimately improve organizational performance.",We have decided to implement a 4-day workweek starting from the next quarter for all employees across the organization.,"Architectural Decision Record

## Context
Our organization has been considering a switch to a 4-day workweek. The current work schedule of 5 days per week, 8 hours per day, has resulted in low productivity, employee burnout, and high turnover rates. The proposed change aims to increase employee satisfaction, engagement, and retention, and ultimately improve organizational performance.

## Decision
","## Context

Our organization has been considering a switch to a 4-day workweek. The current work schedule of 5 days per week, 8 hours per day, has resulted in low productivity, employee burnout, and high turnover rates. The proposed change aims to increase employee satisfaction, engagement, and retention, and ultimately improve organizational performance.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Our organization has been considering a switch to a 4-day workweek. The current work schedule of 5 days per week, 8 hours per day, has resulted in low productivity, employee burnout, and high turnover rates. The proposed change aims to increase employee satisfaction, engagement, and retention, and ultimately improve organizational performance."
"In Babbage era (available after the Vasil HF), the Cardano blockchain will support inline datums by changing the TxOut data type.

In Alonzo era, a TxOut was able to store arbitrary data called the datum. However, only the hash of the datum was stored, not the actual datum.

With inline datums available in Babbage era, transaction outputs can either contain the hash of the datum or the actual datum. Thus, we need to adapt our transaction constraint data type (TxConstraints) to support this new feature.","We will replace the Datum parameter in TxConstraints’s data constructor MustPayToPubKeyAddress with Plutus.V2.Ledger.Api.OutputDatum. In the offchain implementation of the constraint, we will use this new data constructor parameter to support either adding the datum in the datum witness set (by using the datum lookups to resolve the hash) or inline it in the transaction output. In the PlutusV1 on-chain implementation of the constraint, we will return False if the datum value matches OutputDatum Datum because the ledger forbids using Babbage era features with PlutusV1. The PlutusV2 on-chain implementation of the constraint is trivial.

    We will modify the data constructor interface, on-chain implementation and off-chain implementation of MustPayToOtherScript similarly to MustPayToPubKeyAddress.

    We will modify the off-chain implementation of the data constructor MustSpendScriptOutput in order to support inline datums. Currently, the script output’s datum is added in the transaction’s datum witness set. However, if the datum is inlined in the script output, then it is already witnessed. Therefore, we don’t need to add it in the datum witness set.","Architectural Decision Record

## Context
In Babbage era (available after the Vasil HF), the Cardano blockchain will support inline datums by changing the TxOut data type.

In Alonzo era, a TxOut was able to store arbitrary data called the datum. However, only the hash of the datum was stored, not the actual datum.

With inline datums available in Babbage era, transaction outputs can either contain the hash of the datum or the actual datum. Thus, we need to adapt our transaction constraint data type (TxConstraints) to support this new feature.

## Decision
","## Context

In Babbage era (available after the Vasil HF), the Cardano blockchain will support inline datums by changing the TxOut data type.

In Alonzo era, a TxOut was able to store arbitrary data called the datum. However, only the hash of the datum was stored, not the actual datum.

With inline datums available in Babbage era, transaction outputs can either contain the hash of the datum or the actual datum. Thus, we need to adapt our transaction constraint data type (TxConstraints) to support this new feature.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
In Babbage era (available after the Vasil HF), the Cardano blockchain will support inline datums by changing the TxOut data type.

In Alonzo era, a TxOut was able to store arbitrary data called the datum. However, only the hash of the datum was stored, not the actual datum.

With inline datums available in Babbage era, transaction outputs can either contain the hash of the datum or the actual datum. Thus, we need to adapt our transaction constraint data type (TxConstraints) to support this new feature."
"The following code samples were executed with cabal repl plutus-ledger on the plutus-apps commit hash 172873e87789d8aac623e014eff9a39364c719ae.

Currently, the plutus-ledger-constraint library has the MustValidateIn constraint which

    validates that a given POSIXTimeRange` contains the TxInfo’s validity range

    creates a transaction with the provided POSIXTimeRange

The implementation of 1) is trivial. However, a major issue arises for the implementation of 2). Setting the validity interval of a Cardano transaction is done by specifing the slot of the lower bound and the slot of the upper bound. Therefore, the MustValidateIn constraint needs to convert the provided POSIXTimeRange to essentially a (Maybe Slot, Maybe Slot). The problem is that there are many ways to convert a POSIXTime to a Slot.

Currently, provided a POSIXTimeRange, plutus-contract does the following:

    convert the time range to a slot range with Ledger.TimeSlot.posixTimeRangeToContainedSlotRange :: POSIXTimeRange -> SlotRange

    convert the SlotRange to (Cardano.Api.TxValidityLowerBound, Cardano.Api.TxValidityUpperBound) (essentially a (Maybe Slot, Maybe Slot))

The issue with these conversion is that the POSIXTimeRange and SlotRange intervals are type synonyms of the Plutus.V1.Ledger.Api.Interval.Interval a datatype which has has a “Closure” flag for each of the bounds.

Therefore, the conversions yields a discrepency when cardano-ledger converts the (Cardano.Api.TxValidityLowerBound, Cardano.Api.TxValidityUpperBound) to a POSIXTimeRange when creating the TxInfo.

Let’s show some examples to showcase the issue.

> let sc = SlotConfig 1000 0
> let interval = (Interval (LowerBound (Finite 999) False) (UpperBound PosInf True))
> let r = posixTimeRangeToContainedSlotRange sc interval
> r
Interval {ivFrom = LowerBound (Finite (Slot {getSlot = 0})) False, ivTo = UpperBound PosInf True}
> let txValidRange = toCardanoValidityRange r
> txValidRange
Right (TxValidityLowerBound ValidityLowerBoundInBabbageEra (SlotNo 1),TxValidityNoUpperBound ValidityNoUpperBoundInBabbageEra)

When creating the TxInfo, cardano-ledger will convert the previous cardano-api validity slot range to:

(Interval (LowerBound (Finite 1000) True) (UpperBound PosInf True))

In practical reasoning, LowerBound (Finite 999) False and LowerBound (Finite 1000) True are equal considering the precision of 1000 milliseconds per slot. However, given Interval semantics, these are not the same values. Therefore, if the constraint mustValidateIn interval is used both to create a transaction and inside a Plutus script (corresponds to the check interval `contains` txInfoValidRange scriptContextTxInfo), then the Plutus script will yield False.

We can identify a similar behavior with the upper bound.

> let sc = SlotConfig 1000 0
> let interval = (Interval (LowerBound NegInf True) (UpperBound (Finite 999) True))
> let r = posixTimeRangeToContainedSlotRange sc interval
> r
Interval {ivFrom = LowerBound NegInf True, ivTo = UpperBound (Finite (Slot {getSlot = 0})) True}
> let txValidRange = toCardanoValidityRange r
> txValidRange
Right (TxValidityNoLowerBound,TxValidityUpperBound ValidityUpperBoundInBabbageEra (SlotNo 1))

When creating the TxInfo, cardano-ledger will convert the previous cardano-api validity slot range to:

(Interval (LowerBound NegInf True) (UpperBound (Finite 1000) False))

Again, a Plutus script with interval `contains` txInfoValidRange scriptContextTxInfo will yield False.

Additionnaly, the current behavior makes it hard to reason about how a POSIXTime gets translated into a Slot when creating a transaction. Ultimately, a DApp developer should have control over how his POSIXTime gets translated to a Slot.","We will create the following datatype:

    -- | ValidityInterval is a half open interval. Closed (inclusive) on the bottom, open
    -- (exclusive) on the top. A 'Nothing' on the bottom is negative infinity, and a 'Nothing'
    -- on the top is positive infinity.
    data ValidityInterval a = ValidityInterval
      { invalidBefore :: !(Maybe a) -- ^ Inclusive lower bound or negative infinity
      , invalidHereafter :: !(Maybe a) -- ^ Exclusive upper bound or positive infinity
      }

    We will add the following constraint and smart constructor:

    data TxConstraint =
      ...
      MustValidateInTimeRange !(ValidityInterval POSIXTime)

    mustValidateInTimeRange :: !(ValidityInterval POSIXTime) -> TxConstraints

    We will remove the MustValidateIn constraint and deprecate the the mustValidateIn smart constructor which will be replaced by mustValidateInTimeRange.

    We will create the smart constructor

    mustValidateInSlotRange :: !(ValidityInterval Slot) -> TxConstraints

    which will translate the provide validity slot range into a POSIXTimeRange using Ledger.TimeSlot.posixTimeRangeToContainedSlotRange.","Architectural Decision Record

## Context
The following code samples were executed with cabal repl plutus-ledger on the plutus-apps commit hash 172873e87789d8aac623e014eff9a39364c719ae.

Currently, the plutus-ledger-constraint library has the MustValidateIn constraint which

    validates that a given POSIXTimeRange` contains the TxInfo’s validity range

    creates a transaction with the provided POSIXTimeRange

The implementation of 1) is trivial. However, a major issue arises for the implementation of 2). Setting the validity interval of a Cardano transaction is done by specifing the slot of the lower bound and the slot of the upper bound. Therefore, the MustValidateIn constraint needs to convert the provided POSIXTimeRange to essentially a (Maybe Slot, Maybe Slot). The problem is that there are many ways to convert a POSIXTime to a Slot.

Currently, provided a POSIXTimeRange, plutus-contract does the following:

    convert the time range to a slot range with Ledger.TimeSlot.posixTimeRangeToContainedSlotRange :: POSIXTimeRange -> SlotRange

    convert the SlotRange to (Cardano.Api.TxValidityLowerBound, Cardano.Api.TxValidityUpperBound) (essentially a (Maybe Slot, Maybe Slot))

The issue with these conversion is that the POSIXTimeRange and SlotRange intervals are type synonyms of the Plutus.V1.Ledger.Api.Interval.Interval a datatype which has has a “Closure” flag for each of the bounds.

Therefore, the conversions yields a discrepency when cardano-ledger converts the (Cardano.Api.TxValidityLowerBound, Cardano.Api.TxValidityUpperBound) to a POSIXTimeRange when creating the TxInfo.

Let’s show some examples to showcase the issue.

> let sc = SlotConfig 1000 0
> let interval = (Interval (LowerBound (Finite 999) False) (UpperBound PosInf True))
> let r = posixTimeRangeToContainedSlotRange sc interval
> r
Interval {ivFrom = LowerBound (Finite (Slot {getSlot = 0})) False, ivTo = UpperBound PosInf True}
> let txValidRange = toCardanoValidityRange r
> txValidRange
Right (TxValidityLowerBound ValidityLowerBoundInBabbageEra (SlotNo 1),TxValidityNoUpperBound ValidityNoUpperBoundInBabbageEra)

When creating the TxInfo, cardano-ledger will convert the previous cardano-api validity slot range to:

(Interval (LowerBound (Finite 1000) True) (UpperBound PosInf True))

In practical reasoning, LowerBound (Finite 999) False and LowerBound (Finite 1000) True are equal considering the precision of 1000 milliseconds per slot. However, given Interval semantics, these are not the same values. Therefore, if the constraint mustValidateIn interval is used both to create a transaction and inside a Plutus script (corresponds to the check interval `contains` txInfoValidRange scriptContextTxInfo), then the Plutus script will yield False.

We can identify a similar behavior with the upper bound.

> let sc = SlotConfig 1000 0
> let interval = (Interval (LowerBound NegInf True) (UpperBound (Finite 999) True))
> let r = posixTimeRangeToContainedSlotRange sc interval
> r
Interval {ivFrom = LowerBound NegInf True, ivTo = UpperBound (Finite (Slot {getSlot = 0})) True}
> let txValidRange = toCardanoValidityRange r
> txValidRange
Right (TxValidityNoLowerBound,TxValidityUpperBound ValidityUpperBoundInBabbageEra (SlotNo 1))

When creating the TxInfo, cardano-ledger will convert the previous cardano-api validity slot range to:

(Interval (LowerBound NegInf True) (UpperBound (Finite 1000) False))

Again, a Plutus script with interval `contains` txInfoValidRange scriptContextTxInfo will yield False.

Additionnaly, the current behavior makes it hard to reason about how a POSIXTime gets translated into a Slot when creating a transaction. Ultimately, a DApp developer should have control over how his POSIXTime gets translated to a Slot.

## Decision
","## Context

The following code samples were executed with cabal repl plutus-ledger on the plutus-apps commit hash 172873e87789d8aac623e014eff9a39364c719ae.

Currently, the plutus-ledger-constraint library has the MustValidateIn constraint which

    validates that a given POSIXTimeRange` contains the TxInfo’s validity range

    creates a transaction with the provided POSIXTimeRange

The implementation of 1) is trivial. However, a major issue arises for the implementation of 2). Setting the validity interval of a Cardano transaction is done by specifing the slot of the lower bound and the slot of the upper bound. Therefore, the MustValidateIn constraint needs to convert the provided POSIXTimeRange to essentially a (Maybe Slot, Maybe Slot). The problem is that there are many ways to convert a POSIXTime to a Slot.

Currently, provided a POSIXTimeRange, plutus-contract does the following:

    convert the time range to a slot range with Ledger.TimeSlot.posixTimeRangeToContainedSlotRange :: POSIXTimeRange -> SlotRange

    convert the SlotRange to (Cardano.Api.TxValidityLowerBound, Cardano.Api.TxValidityUpperBound) (essentially a (Maybe Slot, Maybe Slot))

The issue with these conversion is that the POSIXTimeRange and SlotRange intervals are type synonyms of the Plutus.V1.Ledger.Api.Interval.Interval a datatype which has has a “Closure” flag for each of the bounds.

Therefore, the conversions yields a discrepency when cardano-ledger converts the (Cardano.Api.TxValidityLowerBound, Cardano.Api.TxValidityUpperBound) to a POSIXTimeRange when creating the TxInfo.

Let’s show some examples to showcase the issue.

> let sc = SlotConfig 1000 0
> let interval = (Interval (LowerBound (Finite 999) False) (UpperBound PosInf True))
> let r = posixTimeRangeToContainedSlotRange sc interval
> r
Interval {ivFrom = LowerBound (Finite (Slot {getSlot = 0})) False, ivTo = UpperBound PosInf True}
> let txValidRange = toCardanoValidityRange r
> txValidRange
Right (TxValidityLowerBound ValidityLowerBoundInBabbageEra (SlotNo 1),TxValidityNoUpperBound ValidityNoUpperBoundInBabbageEra)

When creating the TxInfo, cardano-ledger will convert the previous cardano-api validity slot range to:

(Interval (LowerBound (Finite 1000) True) (UpperBound PosInf True))

In practical reasoning, LowerBound (Finite 999) False and LowerBound (Finite 1000) True are equal considering the precision of 1000 milliseconds per slot. However, given Interval semantics, these are not the same values. Therefore, if the constraint mustValidateIn interval is used both to create a transaction and inside a Plutus script (corresponds to the check interval `contains` txInfoValidRange scriptContextTxInfo), then the Plutus script will yield False.

We can identify a similar behavior with the upper bound.

> let sc = SlotConfig 1000 0
> let interval = (Interval (LowerBound NegInf True) (UpperBound (Finite 999) True))
> let r = posixTimeRangeToContainedSlotRange sc interval
> r
Interval {ivFrom = LowerBound NegInf True, ivTo = UpperBound (Finite (Slot {getSlot = 0})) True}
> let txValidRange = toCardanoValidityRange r
> txValidRange
Right (TxValidityNoLowerBound,TxValidityUpperBound ValidityUpperBoundInBabbageEra (SlotNo 1))

When creating the TxInfo, cardano-ledger will convert the previous cardano-api validity slot range to:

(Interval (LowerBound NegInf True) (UpperBound (Finite 1000) False))

Again, a Plutus script with interval `contains` txInfoValidRange scriptContextTxInfo will yield False.

Additionnaly, the current behavior makes it hard to reason about how a POSIXTime gets translated into a Slot when creating a transaction. Ultimately, a DApp developer should have control over how his POSIXTime gets translated to a Slot.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
The following code samples were executed with cabal repl plutus-ledger on the plutus-apps commit hash 172873e87789d8aac623e014eff9a39364c719ae.

Currently, the plutus-ledger-constraint library has the MustValidateIn constraint which

    validates that a given POSIXTimeRange` contains the TxInfo’s validity range

    creates a transaction with the provided POSIXTimeRange

The implementation of 1) is trivial. However, a major issue arises for the implementation of 2). Setting the validity interval of a Cardano transaction is done by specifing the slot of the lower bound and the slot of the upper bound. Therefore, the MustValidateIn constraint needs to convert the provided POSIXTimeRange to essentially a (Maybe Slot, Maybe Slot). The problem is that there are many ways to convert a POSIXTime to a Slot.

Currently, provided a POSIXTimeRange, plutus-contract does the following:

    convert the time range to a slot range with Ledger.TimeSlot.posixTimeRangeToContainedSlotRange :: POSIXTimeRange -> SlotRange

    convert the SlotRange to (Cardano.Api.TxValidityLowerBound, Cardano.Api.TxValidityUpperBound) (essentially a (Maybe Slot, Maybe Slot))

The issue with these conversion is that the POSIXTimeRange and SlotRange intervals are type synonyms of the Plutus.V1.Ledger.Api.Interval.Interval a datatype which has has a “Closure” flag for each of the bounds.

Therefore, the conversions yields a discrepency when cardano-ledger converts the (Cardano.Api.TxValidityLowerBound, Cardano.Api.TxValidityUpperBound) to a POSIXTimeRange when creating the TxInfo.

Let’s show some examples to showcase the issue.

> let sc = SlotConfig 1000 0
> let interval = (Interval (LowerBound (Finite 999) False) (UpperBound PosInf True))
> let r = posixTimeRangeToContainedSlotRange sc interval
> r
Interval {ivFrom = LowerBound (Finite (Slot {getSlot = 0})) False, ivTo = UpperBound PosInf True}
> let txValidRange = toCardanoValidityRange r
> txValidRange
Right (TxValidityLowerBound ValidityLowerBoundInBabbageEra (SlotNo 1),TxValidityNoUpperBound ValidityNoUpperBoundInBabbageEra)

When creating the TxInfo, cardano-ledger will convert the previous cardano-api validity slot range to:

(Interval (LowerBound (Finite 1000) True) (UpperBound PosInf True))

In practical reasoning, LowerBound (Finite 999) False and LowerBound (Finite 1000) True are equal considering the precision of 1000 milliseconds per slot. However, given Interval semantics, these are not the same values. Therefore, if the constraint mustValidateIn interval is used both to create a transaction and inside a Plutus script (corresponds to the check interval `contains` txInfoValidRange scriptContextTxInfo), then the Plutus script will yield False.

We can identify a similar behavior with the upper bound.

> let sc = SlotConfig 1000 0
> let interval = (Interval (LowerBound NegInf True) (UpperBound (Finite 999) True))
> let r = posixTimeRangeToContainedSlotRange sc interval
> r
Interval {ivFrom = LowerBound NegInf True, ivTo = UpperBound (Finite (Slot {getSlot = 0})) True}
> let txValidRange = toCardanoValidityRange r
> txValidRange
Right (TxValidityNoLowerBound,TxValidityUpperBound ValidityUpperBoundInBabbageEra (SlotNo 1))

When creating the TxInfo, cardano-ledger will convert the previous cardano-api validity slot range to:

(Interval (LowerBound NegInf True) (UpperBound (Finite 1000) False))

Again, a Plutus script with interval `contains` txInfoValidRange scriptContextTxInfo will yield False.

Additionnaly, the current behavior makes it hard to reason about how a POSIXTime gets translated into a Slot when creating a transaction. Ultimately, a DApp developer should have control over how his POSIXTime gets translated to a Slot."
"Marconi is a Haskell executable and library that lives in plutus-chain-index.

It is desirable to move it into a separate repository for the following reasons:

    Better visibility and easier to discover

    It wants to update the version of its cardano-api dependency independently of the version used by plutus-apps

    It is a farily independent component, therefore it warrants its own repository

However, creating a separate repository would be rather costly. It would involve a great deal of duplication, due to the way our current nix code is structured, not to mention the added complexity and overhead inherent in maintaining a separate codebase.","We will put Marconi in a separate Github repository

    Until we resolve the issues with creating a separate Github repository (see Context), we will keep Marconi as a separate project in plutus-apps","Architectural Decision Record

## Context
Marconi is a Haskell executable and library that lives in plutus-chain-index.

It is desirable to move it into a separate repository for the following reasons:

    Better visibility and easier to discover

    It wants to update the version of its cardano-api dependency independently of the version used by plutus-apps

    It is a farily independent component, therefore it warrants its own repository

However, creating a separate repository would be rather costly. It would involve a great deal of duplication, due to the way our current nix code is structured, not to mention the added complexity and overhead inherent in maintaining a separate codebase.

## Decision
","## Context

Marconi is a Haskell executable and library that lives in plutus-chain-index.

It is desirable to move it into a separate repository for the following reasons:

    Better visibility and easier to discover

    It wants to update the version of its cardano-api dependency independently of the version used by plutus-apps

    It is a farily independent component, therefore it warrants its own repository

However, creating a separate repository would be rather costly. It would involve a great deal of duplication, due to the way our current nix code is structured, not to mention the added complexity and overhead inherent in maintaining a separate codebase.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Marconi is a Haskell executable and library that lives in plutus-chain-index.

It is desirable to move it into a separate repository for the following reasons:

    Better visibility and easier to discover

    It wants to update the version of its cardano-api dependency independently of the version used by plutus-apps

    It is a farily independent component, therefore it warrants its own repository

However, creating a separate repository would be rather costly. It would involve a great deal of duplication, due to the way our current nix code is structured, not to mention the added complexity and overhead inherent in maintaining a separate codebase."
"Since the genesis of the plutus-apps repository, the components have been historically using the types in the plutus-ledger-api package (which is now part of the plutus repository) in the off-chain part of a Plutus application.

This was desirable in order to start designing a way to build Plutus applications before the cardano-ledger actually supported the Alonzo-era features. Of course, this resulted in the unintended consequence that we used TxInfo types (types that are designed to be used in Plutus scripts) in off-chain code. This wouldn’t have been a problem if there was a 1:1 relationship between on-chain and off-chain types. However, that presumption is wrong.

Let’s take the example of the TxOut representation of plutus-ledger-api for PlutusV2.

data TxOut = TxOut {
  ...
  txOutReferenceScript :: Maybe ScriptHash
  }

As we can see, the TxOut can optionally store the ScriptHash of the referenced script. However, that is not the adequate representation of a TxOut in a transaction given the cardano-ledger specification. The off-chain TxOut should instead be:

data TxOut = TxOut {
  ...
  txOutReferenceScript :: Maybe Script
  }

where the reference script field can store that actual script, not just the hash.

This proved that we need to start moving away from plutus-ledger-api types in the off-chain part of Plutus applications, especially in components like the emulator and the chain indexer.","We will create a cardano-api-extended cabal project, which will contain features and utilities on top of the cardano-api package. A similar idea has emerged with hydra-cardano-api. This package will contain:

            type synonyms for working with the latest era

            a simplified and working transaction balancing function (mainly the Ledger.Fee.makeAutoBalancedTransaction in plutus-apps)

            validation rules (most of what’s in the current Ledger.Validation)

    The cardano-api-extended package will re-export the modules from the hydra-cardano-api package which contain type synonyms for working with the latest era.

    We will remove our data type representation of a Cardano transaction (Ledger.Tx.Internal.Tx in plutus-ledger) and fully commit to Cardano.Api.Tx.Tx era (or Cardano.Api.Tx.TxLatestEra) in the codebase.

    We will replace any use of plutus-ledger-api types by cardano-api and cardano-api-extended types whenever we work with the off-chain part of Plutus applications. For instance, the plutus-contract emulator and types in the Plutus.Contract.Request module of plutus-contract will be updated to use cardano-api types. However, the data types in Ledger.Tx.Constraints.TxConstraints will continue to use plutus-ledger-api types because the constraints are used to generate both Plutus scripts and transactions. Therefore, there should be no breaking change on the API for writing Plutus applications.

    We will improve cardano-api through cardano-api-extended and regularly push changes upstream when possible.

    We will restructure the Ledger.Tx.CardanoApi module in plutus-ledger and move functions in cardano-api-extended.

    We will enhance the plutus-contract emulator by being able to balance and submit cardano-api transactions.

    We will modify the plutus-contract emulator to fully use the cardano-ledger transaction validation rules, and we will remove our custom validation rules (module Ledger.Index in plutus-ledger).","Architectural Decision Record

## Context
Since the genesis of the plutus-apps repository, the components have been historically using the types in the plutus-ledger-api package (which is now part of the plutus repository) in the off-chain part of a Plutus application.

This was desirable in order to start designing a way to build Plutus applications before the cardano-ledger actually supported the Alonzo-era features. Of course, this resulted in the unintended consequence that we used TxInfo types (types that are designed to be used in Plutus scripts) in off-chain code. This wouldn’t have been a problem if there was a 1:1 relationship between on-chain and off-chain types. However, that presumption is wrong.

Let’s take the example of the TxOut representation of plutus-ledger-api for PlutusV2.

data TxOut = TxOut {
  ...
  txOutReferenceScript :: Maybe ScriptHash
  }

As we can see, the TxOut can optionally store the ScriptHash of the referenced script. However, that is not the adequate representation of a TxOut in a transaction given the cardano-ledger specification. The off-chain TxOut should instead be:

data TxOut = TxOut {
  ...
  txOutReferenceScript :: Maybe Script
  }

where the reference script field can store that actual script, not just the hash.

This proved that we need to start moving away from plutus-ledger-api types in the off-chain part of Plutus applications, especially in components like the emulator and the chain indexer.

## Decision
","## Context

Since the genesis of the plutus-apps repository, the components have been historically using the types in the plutus-ledger-api package (which is now part of the plutus repository) in the off-chain part of a Plutus application.

This was desirable in order to start designing a way to build Plutus applications before the cardano-ledger actually supported the Alonzo-era features. Of course, this resulted in the unintended consequence that we used TxInfo types (types that are designed to be used in Plutus scripts) in off-chain code. This wouldn’t have been a problem if there was a 1:1 relationship between on-chain and off-chain types. However, that presumption is wrong.

Let’s take the example of the TxOut representation of plutus-ledger-api for PlutusV2.

data TxOut = TxOut {
  ...
  txOutReferenceScript :: Maybe ScriptHash
  }

As we can see, the TxOut can optionally store the ScriptHash of the referenced script. However, that is not the adequate representation of a TxOut in a transaction given the cardano-ledger specification. The off-chain TxOut should instead be:

data TxOut = TxOut {
  ...
  txOutReferenceScript :: Maybe Script
  }

where the reference script field can store that actual script, not just the hash.

This proved that we need to start moving away from plutus-ledger-api types in the off-chain part of Plutus applications, especially in components like the emulator and the chain indexer.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Since the genesis of the plutus-apps repository, the components have been historically using the types in the plutus-ledger-api package (which is now part of the plutus repository) in the off-chain part of a Plutus application.

This was desirable in order to start designing a way to build Plutus applications before the cardano-ledger actually supported the Alonzo-era features. Of course, this resulted in the unintended consequence that we used TxInfo types (types that are designed to be used in Plutus scripts) in off-chain code. This wouldn’t have been a problem if there was a 1:1 relationship between on-chain and off-chain types. However, that presumption is wrong.

Let’s take the example of the TxOut representation of plutus-ledger-api for PlutusV2.

data TxOut = TxOut {
  ...
  txOutReferenceScript :: Maybe ScriptHash
  }

As we can see, the TxOut can optionally store the ScriptHash of the referenced script. However, that is not the adequate representation of a TxOut in a transaction given the cardano-ledger specification. The off-chain TxOut should instead be:

data TxOut = TxOut {
  ...
  txOutReferenceScript :: Maybe Script
  }

where the reference script field can store that actual script, not just the hash.

This proved that we need to start moving away from plutus-ledger-api types in the off-chain part of Plutus applications, especially in components like the emulator and the chain indexer."
"The current plutus-contract Haskell package allows developers to write Plutus applications using the Contract Monad. On top of that, the package contains a Plutus application contract emulator, a way to run those contracts on a simulated environment so that they can be tested. The emulator in question contains multiple components like the testing framework (including the ContractModel), the wallet emulator, a chain-index emulator and the node emulator.:

 +-------------------+     +-------------------+     +-----------------+
 | Testing framework |<----+ Contract Emulator +---->| Wallet emulator |
 +-------------------+     +-----+--------+----+     +--------+--------+
                                 |        |                   |
                                 |        |                   v
+----------------------+         |        |           +---------------+
| Chain index emulator |<--------+        +---------->| Node emulator |
+----------------------+                              +---------------+

The main reason we can’t use a real wallet or node backend is because they are not fast enough to be able to run many test cases using property tests with the ContractModel.

Now, we believe the node emulator to be a useful separate component that other testing framework can leverage for being able to write fast test cases.","We will create a new Haskell package named cardano-node-emulator.

    We will move node related functionality into this new package. Here are some modules (or parts of a module) that will need to be moved over.

        The Ledger.Validation module which validates transactions using cardano-ledger should only be used by cardano-node-emulator and should not be exposed.

        The Ledger.Fee module, which calculates the fees for a given transaction, should be internal to cardano-node-emulator.

        The Ledger.Generators module, which contains generators for constructing blockchains and transactions for use in property-based testing, should be internal to cardano-node-emulator.

        The Ledger.TimeSlot.SlotConfig datatype should only be used by the cardano-node-emulator. An end user should not use this representation in a real world scenario. See ADR 12: Time conversion semantic change for more details.

        The Wallet.Emulator.Chain module in plutus-contract should be moved in cardano-node-emulator.

        The Ledger.Params which allows to configuration the network parameters should be moved over to cardano-node-emulator.","Architectural Decision Record

## Context
The current plutus-contract Haskell package allows developers to write Plutus applications using the Contract Monad. On top of that, the package contains a Plutus application contract emulator, a way to run those contracts on a simulated environment so that they can be tested. The emulator in question contains multiple components like the testing framework (including the ContractModel), the wallet emulator, a chain-index emulator and the node emulator.:

 +-------------------+     +-------------------+     +-----------------+
 | Testing framework |<----+ Contract Emulator +---->| Wallet emulator |
 +-------------------+     +-----+--------+----+     +--------+--------+
                                 |        |                   |
                                 |        |                   v
+----------------------+         |        |           +---------------+
| Chain index emulator |<--------+        +---------->| Node emulator |
+----------------------+                              +---------------+

The main reason we can’t use a real wallet or node backend is because they are not fast enough to be able to run many test cases using property tests with the ContractModel.

Now, we believe the node emulator to be a useful separate component that other testing framework can leverage for being able to write fast test cases.

## Decision
","## Context

The current plutus-contract Haskell package allows developers to write Plutus applications using the Contract Monad. On top of that, the package contains a Plutus application contract emulator, a way to run those contracts on a simulated environment so that they can be tested. The emulator in question contains multiple components like the testing framework (including the ContractModel), the wallet emulator, a chain-index emulator and the node emulator.:

 +-------------------+     +-------------------+     +-----------------+
 | Testing framework |<----+ Contract Emulator +---->| Wallet emulator |
 +-------------------+     +-----+--------+----+     +--------+--------+
                                 |        |                   |
                                 |        |                   v
+----------------------+         |        |           +---------------+
| Chain index emulator |<--------+        +---------->| Node emulator |
+----------------------+                              +---------------+

The main reason we can’t use a real wallet or node backend is because they are not fast enough to be able to run many test cases using property tests with the ContractModel.

Now, we believe the node emulator to be a useful separate component that other testing framework can leverage for being able to write fast test cases.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
The current plutus-contract Haskell package allows developers to write Plutus applications using the Contract Monad. On top of that, the package contains a Plutus application contract emulator, a way to run those contracts on a simulated environment so that they can be tested. The emulator in question contains multiple components like the testing framework (including the ContractModel), the wallet emulator, a chain-index emulator and the node emulator.:

 +-------------------+     +-------------------+     +-----------------+
 | Testing framework |<----+ Contract Emulator +---->| Wallet emulator |
 +-------------------+     +-----+--------+----+     +--------+--------+
                                 |        |                   |
                                 |        |                   v
+----------------------+         |        |           +---------------+
| Chain index emulator |<--------+        +---------->| Node emulator |
+----------------------+                              +---------------+

The main reason we can’t use a real wallet or node backend is because they are not fast enough to be able to run many test cases using property tests with the ContractModel.

Now, we believe the node emulator to be a useful separate component that other testing framework can leverage for being able to write fast test cases."
"After the Vasil HF, the Cardano blockchain will support reference inputs by adding a new field in the transaction data type. With reference inputs, transactions can take a look at UTXOs without actually spending them.

Thus, we need to adapt our transaction constraint data type (TxConstraints) to support referencing UTXOs.","We will add the data constuctor MustReferenceOutput TxOutRef to the TxConstraints data type.

    The PlutusV1 on-chain implementation of this new constraint will simply return False. However, cardano-ledger throws a phase-1 validation error if transactions that use the some of the new features (reference inputs, inline datums and reference scripts) try to execute PlutusV1 scripts. See the Babbage era ledger specification. Therefore, the only way to get a phase-2 validation error would be to use this constraint on-chain in a PlutusV1 script, without using any of the new Babbage era features off-chain.

    The PlutusV2 on-chain implementation of this new constraint will check that the provided TxOutRef is part of the ScriptContext’s reference inputs.","Architectural Decision Record

## Context
After the Vasil HF, the Cardano blockchain will support reference inputs by adding a new field in the transaction data type. With reference inputs, transactions can take a look at UTXOs without actually spending them.

Thus, we need to adapt our transaction constraint data type (TxConstraints) to support referencing UTXOs.

## Decision
","## Context

After the Vasil HF, the Cardano blockchain will support reference inputs by adding a new field in the transaction data type. With reference inputs, transactions can take a look at UTXOs without actually spending them.

Thus, we need to adapt our transaction constraint data type (TxConstraints) to support referencing UTXOs.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
After the Vasil HF, the Cardano blockchain will support reference inputs by adding a new field in the transaction data type. With reference inputs, transactions can take a look at UTXOs without actually spending them.

Thus, we need to adapt our transaction constraint data type (TxConstraints) to support referencing UTXOs."
"In Babbage era (available after the Vasil HF), the Cardano blockchain will support “reference scripts” by changing the TxOut data type. Reference scripts are used to attach arbitrary scripts to transaction outputs and are used to satisfy script requirements during validation, rather than requiring the spending transaction to do so. Thus, we need to adapt our transaction constraint data type (TxConstraints) to support this new feature.","We will add Maybe ScriptHash as a new data constructor parameter for the constraints MustPayToPubKeyAddress, MustPayToOtherScript, ScriptOutputConstraint in TxConstraints. In the off-chain implementation of those constraints, if a reference script hash is provided, we will need to find the actual script in the lookups table so that we can include it in the transaction output. In the PlutusV1 on-chain implementation of the constraint, we will return False if a reference script is provided because the ledger forbids using Babbage era features with PlutusV1. The PlutusV2 on-chain implementation of the constraint is trivial.

    We will modify the off-chain implementation of MustSpendScriptOutput and ScriptInputConstraint in order to add support for witnessing a script by actually providing it, or by pointing to the reference input which contains the script.","Architectural Decision Record

## Context
In Babbage era (available after the Vasil HF), the Cardano blockchain will support “reference scripts” by changing the TxOut data type. Reference scripts are used to attach arbitrary scripts to transaction outputs and are used to satisfy script requirements during validation, rather than requiring the spending transaction to do so. Thus, we need to adapt our transaction constraint data type (TxConstraints) to support this new feature.

## Decision
","## Context

In Babbage era (available after the Vasil HF), the Cardano blockchain will support “reference scripts” by changing the TxOut data type. Reference scripts are used to attach arbitrary scripts to transaction outputs and are used to satisfy script requirements during validation, rather than requiring the spending transaction to do so. Thus, we need to adapt our transaction constraint data type (TxConstraints) to support this new feature.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
In Babbage era (available after the Vasil HF), the Cardano blockchain will support “reference scripts” by changing the TxOut data type. Reference scripts are used to attach arbitrary scripts to transaction outputs and are used to satisfy script requirements during validation, rather than requiring the spending transaction to do so. Thus, we need to adapt our transaction constraint data type (TxConstraints) to support this new feature."
"IOG is undertaking a company-wide effort to restructure and standardize its repositories, favoring mono-repos and enforcing shared GitOps and DevOps processes. Parallel to this, a new CI infrastructure is being developed.

Examples of this are:

    input-output-hk/cardano-world

    input-output-hk/ci-world

    input-output-hk/atala-world

This initiative appears to be championed by the SRE team who are the creators of divnix/std. Indeed std is at the heart of the standardization dream.","Standardization of the repositories has been deemed a worthwhile endeavour, though of very low priority.

    Phase 1 of the standardization process will be carried out in parallel with Move Marconi to a separate repository. A separate repository will be created for Marconi, and from the very beginning it will use std. This way the benefits, limitations and integration costs of std can be experienced and measured, and an informed, definitive decision on standardizing plutus-core and plutus-apps themselves can be made.","Architectural Decision Record

## Context
IOG is undertaking a company-wide effort to restructure and standardize its repositories, favoring mono-repos and enforcing shared GitOps and DevOps processes. Parallel to this, a new CI infrastructure is being developed.

Examples of this are:

    input-output-hk/cardano-world

    input-output-hk/ci-world

    input-output-hk/atala-world

This initiative appears to be championed by the SRE team who are the creators of divnix/std. Indeed std is at the heart of the standardization dream.

## Decision
","## Context

IOG is undertaking a company-wide effort to restructure and standardize its repositories, favoring mono-repos and enforcing shared GitOps and DevOps processes. Parallel to this, a new CI infrastructure is being developed.

Examples of this are:

    input-output-hk/cardano-world

    input-output-hk/ci-world

    input-output-hk/atala-world

This initiative appears to be championed by the SRE team who are the creators of divnix/std. Indeed std is at the heart of the standardization dream.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
IOG is undertaking a company-wide effort to restructure and standardize its repositories, favoring mono-repos and enforcing shared GitOps and DevOps processes. Parallel to this, a new CI infrastructure is being developed.

Examples of this are:

    input-output-hk/cardano-world

    input-output-hk/ci-world

    input-output-hk/atala-world

This initiative appears to be championed by the SRE team who are the creators of divnix/std. Indeed std is at the heart of the standardization dream."
"In Babbage era (available after the Vasil HF), Cardano transactions will contain new collateral related fields: “return collateral” and “total collateral” collateral. Return collateral (also called “collateral output”) and total collateral are detailed in CIP-40.

In summary, return collateral is a special output (basically of type TxOut) that becomes available in case there is a failed phase-2 validation. In addition, we have the new total collateral field which explicitly says how much collateral (in lovelace) is going to be actually consumed in the case of phase-2 validation failure.","We will add the txReturnCollateral and the txTotalCollateral fields in the Ledger.Tx.Internal.Tx data type.

    We will modify the Wallet.Emulator.Wallet.handleBalance function in plutus-contract to set the correct return and total collateral for an UnbalancedTx (of type Either CardanoBuildTx EmulatorTx). In either type of transaction, we would compute the txTotalCollateral while estimating the fee with the formula quot (txfee txb * (collateralPercent pp)) * 100 and then set txReturnCollateral with the formula sum collateralInputs - txTotalCollateral.","Architectural Decision Record

## Context
In Babbage era (available after the Vasil HF), Cardano transactions will contain new collateral related fields: “return collateral” and “total collateral” collateral. Return collateral (also called “collateral output”) and total collateral are detailed in CIP-40.

In summary, return collateral is a special output (basically of type TxOut) that becomes available in case there is a failed phase-2 validation. In addition, we have the new total collateral field which explicitly says how much collateral (in lovelace) is going to be actually consumed in the case of phase-2 validation failure.

## Decision
","## Context

In Babbage era (available after the Vasil HF), Cardano transactions will contain new collateral related fields: “return collateral” and “total collateral” collateral. Return collateral (also called “collateral output”) and total collateral are detailed in CIP-40.

In summary, return collateral is a special output (basically of type TxOut) that becomes available in case there is a failed phase-2 validation. In addition, we have the new total collateral field which explicitly says how much collateral (in lovelace) is going to be actually consumed in the case of phase-2 validation failure.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
In Babbage era (available after the Vasil HF), Cardano transactions will contain new collateral related fields: “return collateral” and “total collateral” collateral. Return collateral (also called “collateral output”) and total collateral are detailed in CIP-40.

In summary, return collateral is a special output (basically of type TxOut) that becomes available in case there is a failed phase-2 validation. In addition, we have the new total collateral field which explicitly says how much collateral (in lovelace) is going to be actually consumed in the case of phase-2 validation failure."
"Currently, PAB users need to provide the SlotConfig in the configuration file, which is passed through to the Contract API, which users can use to convert between a Slot and a POSIXTime. However, the current SlotConfig representation supposes that the slot length is the same for all epochs in the Cardano blockchain, which is not the case. For example, during the Byron era, the slot length was 20s, while from Shelley era and onwards, the slot length is 1s. Therefore, the functions from the Ledger.TimeSlot module in plutus-ledger do not compute the conversion between Slot and POSIXTime the right way. The current easiest way to compute the time conversions is to query the local Cardano node on the consensus layer, which requires the ouroboros-consensus dependency.","We will deprecate the Ledger.TimeSlot.SlotConfig type and all functions in the Ledger.TimeSlot module using the SlotConfig. The only viable functions are the ones that convert between Data.Time types and plutus types (types related to TxInfo).

    We will copy the Ledger.TimeSlot module in the emulator (ideally rename it) and keep it as an internal module. Any functions not used by the emulator will be removed.

    We will move the Ledger.Params module inside the emulator as an internal module and modify the Params datatype name to EmulatorParams.

    We will modify the Plutus.Contract.Request.getParams function to Plutus.Contract.Request.getProtocolParameters. This implies modifying the name of Contract effect GetParamsReq/GetParamsResp.

    We will create two pairs of effects in Plutus.Contract.Effect:

    data PABReq =
      ...
      | SlotToUTCTimeIntervalReq SlotNo
      | UTCTimeToSlotReq UTCTime
      ...

     data PABResp =
      ...
      | SlotToUTCTimeIntervalResp (UTCTime, SlotLength) -- An alternative can be (UTCTime, UTCTime)
      | UTCTimeToSlotResp SlotNo
      ...

    We will implement the emulator effect interpreter by simply using the SlotConfig for the conversions.

    We will implement the PAB effect interpreter by using the local node. There are multiple steps to implement this:

        At startup, the PAB will query the EraHistory from the local node and store it in its local environment.

        We will implement the PAB interpreter by using the EraHistory alongside the consensus functions wallclockToSlot and slotToWallclock. Here’s an example function of how to use them:

        -- Calculate slot number which contains a given timestamp
        utcTimeToSlotNo
          :: SystemStart
          -> EraHistory CardanoMode
          -> Time.UTCTime
          -> Either PastHorizonException SlotNo
        utcTimeToSlotNo systemStart (EraHistory _ interpreter) time = do
          let relativeTime = toRelativeTime systemStart time
          (slotNo, _, _) <- interpretQuery interpreter $ wallclockToSlot relativeTime
          pure slotNo

        slotStart
          :: SystemStart
          -> EraHistory CardanoMode
          -> SlotNo
          -> Either PastHorizonException Time.UTCTime
        slotStart systemStart (EraHistory _ interpreter) slotNo = do
          (relativeTime, _) <- interpretQuery interpreter $ slotToWallclock slotNo
          pure $ fromRelativeTime systemStart relativeTime

        However, we will also add an additional step. If the conversion returns PastHorizonException, then there is a good probability that the EraHistory is out of date. The reason is that EraHistory only encodes era information from the moment the user ran the query and it cannot predict the future. In that case, if the PastHorizonException is returned, we will re-query the EraHistory of the local node, replace the old value in the PAB environment, and retry the conversion. If it fails again, we return the error message.","Architectural Decision Record

## Context
Currently, PAB users need to provide the SlotConfig in the configuration file, which is passed through to the Contract API, which users can use to convert between a Slot and a POSIXTime. However, the current SlotConfig representation supposes that the slot length is the same for all epochs in the Cardano blockchain, which is not the case. For example, during the Byron era, the slot length was 20s, while from Shelley era and onwards, the slot length is 1s. Therefore, the functions from the Ledger.TimeSlot module in plutus-ledger do not compute the conversion between Slot and POSIXTime the right way. The current easiest way to compute the time conversions is to query the local Cardano node on the consensus layer, which requires the ouroboros-consensus dependency.

## Decision
","## Context

Currently, PAB users need to provide the SlotConfig in the configuration file, which is passed through to the Contract API, which users can use to convert between a Slot and a POSIXTime. However, the current SlotConfig representation supposes that the slot length is the same for all epochs in the Cardano blockchain, which is not the case. For example, during the Byron era, the slot length was 20s, while from Shelley era and onwards, the slot length is 1s. Therefore, the functions from the Ledger.TimeSlot module in plutus-ledger do not compute the conversion between Slot and POSIXTime the right way. The current easiest way to compute the time conversions is to query the local Cardano node on the consensus layer, which requires the ouroboros-consensus dependency.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Currently, PAB users need to provide the SlotConfig in the configuration file, which is passed through to the Contract API, which users can use to convert between a Slot and a POSIXTime. However, the current SlotConfig representation supposes that the slot length is the same for all epochs in the Cardano blockchain, which is not the case. For example, during the Byron era, the slot length was 20s, while from Shelley era and onwards, the slot length is 1s. Therefore, the functions from the Ledger.TimeSlot module in plutus-ledger do not compute the conversion between Slot and POSIXTime the right way. The current easiest way to compute the time conversions is to query the local Cardano node on the consensus layer, which requires the ouroboros-consensus dependency."
"Let’s start with the problematic example (copy-paste of the current PubKey contract in plutus-use-cases).

-- | Lock some funds in a 'PayToPubKey' contract, returning the output's address
--   and a 'TxIn' transaction input that can spend it.
pubKeyContract
    :: forall w s e.
    ( AsPubKeyError e
    )
    => PaymentPubKeyHash
    -> Value
    -> Contract w s e (TxOutRef, Maybe ChainIndexTxOut, TypedValidator PubKeyContract)
pubKeyContract pk vl = mapError (review _PubKeyError   ) $ do
    -- Step 1
    let inst = typedValidator pk
        address = Scripts.validatorAddress inst
        tx = Constraints.mustPayToTheScriptWithDatumHash () vl
    ledgerTx <- mkTxConstraints (Constraints.typedValidatorLookups inst) tx
               >>= submitUnbalancedTx . Constraints.adjustUnbalancedTx

    -- Step 2
    _ <- awaitTxConfirmed (getCardanoTxId ledgerTx)

    -- Step 3
    let refs = Map.keys
               $ Map.filter ((==) address . txOutAddress)
               $ getCardanoTxProducedOutputs ledgerTx
    case refs of
        []                   -> throwing _ScriptOutputMissing pk
        [outRef] -> do
            -- Step 4
            ciTxOut <- unspentTxOutFromRef outRef
            pure (outRef, ciTxOut, inst)
        _                    -> throwing _MultipleScriptOutputs pk

Here’s an outline of the contract’s steps:

    Creates a transaction and submits it to the node

    Waits for transaction to be confirmed

    Finds the first UTXO of that transaction (return type TxOutRef)

    Queries the plutus-chain-index to get the ChainIndexTxOut out of that TxOutRef

The problem is that the ciTxOut variable in step 4 will almost always result in Nothing.

Why? Here’s some context.

The PAB listens to the local node and stores blockchain information in memory such as the status of transactions, the status of transaction outputs, the last synced slot, the current slot, etc., in a variable of type BlockchainEnv. The awaitTxConfirmed is actually querying the state of BlockchainEnv and waits until the status of the transaction transitions to Confirmed.

Meanwhile, plutus-chain-index (our main indexing component at the time of this writing) is also listening to incoming blocks from the local node and indexes them into a database. The indexed data can be queried using the REST API interface.

This brings up the main issue: the PAB and plutus-chain-index each listen to the same source of information (a local Cardano node), but each index the information at different speeds. For a dApp developer writing off-chain code using the Contract API, there is no abstraction for handling multiple sources of truth.

Currently, in the best case scenario (fully synced PAB and plutus-chain-index), plutus-chain-index will always trail behind the in-memory storage of the PAB by a few seconds. Therefore, even in this scenario, querying the plutus-chain-index with unspentTxOutFromRef in the above contract has a high probability of returning Nothing.","The best solution is probably a combination of the Alternative solutions described below. However, we will mainly choose the Query functions should interact with a single source of truth solution.

    We will replace plutus-chain-index with Marconi as PAB’s indexing component

    We will move out the blockchain information indexed by PAB in Marconi

    We will add new indexers in Marconi in order to replicate the information indexed by plutus-chain-index

    We will adapt the architecture of Marconi (which will become our new indexing component) to support waiting queries

    Since we suppose that indexing component should be in the same machine as the PAB, then we will use Marconi as a library to index and query the indexed blockchain information without relying on an HTTP API","Architectural Decision Record

## Context
Let’s start with the problematic example (copy-paste of the current PubKey contract in plutus-use-cases).

-- | Lock some funds in a 'PayToPubKey' contract, returning the output's address
--   and a 'TxIn' transaction input that can spend it.
pubKeyContract
    :: forall w s e.
    ( AsPubKeyError e
    )
    => PaymentPubKeyHash
    -> Value
    -> Contract w s e (TxOutRef, Maybe ChainIndexTxOut, TypedValidator PubKeyContract)
pubKeyContract pk vl = mapError (review _PubKeyError   ) $ do
    -- Step 1
    let inst = typedValidator pk
        address = Scripts.validatorAddress inst
        tx = Constraints.mustPayToTheScriptWithDatumHash () vl
    ledgerTx <- mkTxConstraints (Constraints.typedValidatorLookups inst) tx
               >>= submitUnbalancedTx . Constraints.adjustUnbalancedTx

    -- Step 2
    _ <- awaitTxConfirmed (getCardanoTxId ledgerTx)

    -- Step 3
    let refs = Map.keys
               $ Map.filter ((==) address . txOutAddress)
               $ getCardanoTxProducedOutputs ledgerTx
    case refs of
        []                   -> throwing _ScriptOutputMissing pk
        [outRef] -> do
            -- Step 4
            ciTxOut <- unspentTxOutFromRef outRef
            pure (outRef, ciTxOut, inst)
        _                    -> throwing _MultipleScriptOutputs pk

Here’s an outline of the contract’s steps:

    Creates a transaction and submits it to the node

    Waits for transaction to be confirmed

    Finds the first UTXO of that transaction (return type TxOutRef)

    Queries the plutus-chain-index to get the ChainIndexTxOut out of that TxOutRef

The problem is that the ciTxOut variable in step 4 will almost always result in Nothing.

Why? Here’s some context.

The PAB listens to the local node and stores blockchain information in memory such as the status of transactions, the status of transaction outputs, the last synced slot, the current slot, etc., in a variable of type BlockchainEnv. The awaitTxConfirmed is actually querying the state of BlockchainEnv and waits until the status of the transaction transitions to Confirmed.

Meanwhile, plutus-chain-index (our main indexing component at the time of this writing) is also listening to incoming blocks from the local node and indexes them into a database. The indexed data can be queried using the REST API interface.

This brings up the main issue: the PAB and plutus-chain-index each listen to the same source of information (a local Cardano node), but each index the information at different speeds. For a dApp developer writing off-chain code using the Contract API, there is no abstraction for handling multiple sources of truth.

Currently, in the best case scenario (fully synced PAB and plutus-chain-index), plutus-chain-index will always trail behind the in-memory storage of the PAB by a few seconds. Therefore, even in this scenario, querying the plutus-chain-index with unspentTxOutFromRef in the above contract has a high probability of returning Nothing.

## Decision
","## Context

Let’s start with the problematic example (copy-paste of the current PubKey contract in plutus-use-cases).

-- | Lock some funds in a 'PayToPubKey' contract, returning the output's address
--   and a 'TxIn' transaction input that can spend it.
pubKeyContract
    :: forall w s e.
    ( AsPubKeyError e
    )
    => PaymentPubKeyHash
    -> Value
    -> Contract w s e (TxOutRef, Maybe ChainIndexTxOut, TypedValidator PubKeyContract)
pubKeyContract pk vl = mapError (review _PubKeyError   ) $ do
    -- Step 1
    let inst = typedValidator pk
        address = Scripts.validatorAddress inst
        tx = Constraints.mustPayToTheScriptWithDatumHash () vl
    ledgerTx <- mkTxConstraints (Constraints.typedValidatorLookups inst) tx
               >>= submitUnbalancedTx . Constraints.adjustUnbalancedTx

    -- Step 2
    _ <- awaitTxConfirmed (getCardanoTxId ledgerTx)

    -- Step 3
    let refs = Map.keys
               $ Map.filter ((==) address . txOutAddress)
               $ getCardanoTxProducedOutputs ledgerTx
    case refs of
        []                   -> throwing _ScriptOutputMissing pk
        [outRef] -> do
            -- Step 4
            ciTxOut <- unspentTxOutFromRef outRef
            pure (outRef, ciTxOut, inst)
        _                    -> throwing _MultipleScriptOutputs pk

Here’s an outline of the contract’s steps:

    Creates a transaction and submits it to the node

    Waits for transaction to be confirmed

    Finds the first UTXO of that transaction (return type TxOutRef)

    Queries the plutus-chain-index to get the ChainIndexTxOut out of that TxOutRef

The problem is that the ciTxOut variable in step 4 will almost always result in Nothing.

Why? Here’s some context.

The PAB listens to the local node and stores blockchain information in memory such as the status of transactions, the status of transaction outputs, the last synced slot, the current slot, etc., in a variable of type BlockchainEnv. The awaitTxConfirmed is actually querying the state of BlockchainEnv and waits until the status of the transaction transitions to Confirmed.

Meanwhile, plutus-chain-index (our main indexing component at the time of this writing) is also listening to incoming blocks from the local node and indexes them into a database. The indexed data can be queried using the REST API interface.

This brings up the main issue: the PAB and plutus-chain-index each listen to the same source of information (a local Cardano node), but each index the information at different speeds. For a dApp developer writing off-chain code using the Contract API, there is no abstraction for handling multiple sources of truth.

Currently, in the best case scenario (fully synced PAB and plutus-chain-index), plutus-chain-index will always trail behind the in-memory storage of the PAB by a few seconds. Therefore, even in this scenario, querying the plutus-chain-index with unspentTxOutFromRef in the above contract has a high probability of returning Nothing.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Let’s start with the problematic example (copy-paste of the current PubKey contract in plutus-use-cases).

-- | Lock some funds in a 'PayToPubKey' contract, returning the output's address
--   and a 'TxIn' transaction input that can spend it.
pubKeyContract
    :: forall w s e.
    ( AsPubKeyError e
    )
    => PaymentPubKeyHash
    -> Value
    -> Contract w s e (TxOutRef, Maybe ChainIndexTxOut, TypedValidator PubKeyContract)
pubKeyContract pk vl = mapError (review _PubKeyError   ) $ do
    -- Step 1
    let inst = typedValidator pk
        address = Scripts.validatorAddress inst
        tx = Constraints.mustPayToTheScriptWithDatumHash () vl
    ledgerTx <- mkTxConstraints (Constraints.typedValidatorLookups inst) tx
               >>= submitUnbalancedTx . Constraints.adjustUnbalancedTx

    -- Step 2
    _ <- awaitTxConfirmed (getCardanoTxId ledgerTx)

    -- Step 3
    let refs = Map.keys
               $ Map.filter ((==) address . txOutAddress)
               $ getCardanoTxProducedOutputs ledgerTx
    case refs of
        []                   -> throwing _ScriptOutputMissing pk
        [outRef] -> do
            -- Step 4
            ciTxOut <- unspentTxOutFromRef outRef
            pure (outRef, ciTxOut, inst)
        _                    -> throwing _MultipleScriptOutputs pk

Here’s an outline of the contract’s steps:

    Creates a transaction and submits it to the node

    Waits for transaction to be confirmed

    Finds the first UTXO of that transaction (return type TxOutRef)

    Queries the plutus-chain-index to get the ChainIndexTxOut out of that TxOutRef

The problem is that the ciTxOut variable in step 4 will almost always result in Nothing.

Why? Here’s some context.

The PAB listens to the local node and stores blockchain information in memory such as the status of transactions, the status of transaction outputs, the last synced slot, the current slot, etc., in a variable of type BlockchainEnv. The awaitTxConfirmed is actually querying the state of BlockchainEnv and waits until the status of the transaction transitions to Confirmed.

Meanwhile, plutus-chain-index (our main indexing component at the time of this writing) is also listening to incoming blocks from the local node and indexes them into a database. The indexed data can be queried using the REST API interface.

This brings up the main issue: the PAB and plutus-chain-index each listen to the same source of information (a local Cardano node), but each index the information at different speeds. For a dApp developer writing off-chain code using the Contract API, there is no abstraction for handling multiple sources of truth.

Currently, in the best case scenario (fully synced PAB and plutus-chain-index), plutus-chain-index will always trail behind the in-memory storage of the PAB by a few seconds. Therefore, even in this scenario, querying the plutus-chain-index with unspentTxOutFromRef in the above contract has a high probability of returning Nothing."
"End-to-end testing of Plutus Core functionality is currently performed by a combination of automation and exploratory approaches, both are performed on public preview and pre-prod testnets using a real node. Automation test scenarios for Plutus are currently being run as part of the wider cardano-node-tests test suite, which uses a Python wrapper for cardano-cli. Those tests focus on general ledger/node/cli functionality and only cover a few key scenarios for Plutus functionality, such as TxInfo and SECP256k1 builtins.

There is also ongoing development work to separate the functionality of cardano-api out into two packages:

    cardano-ledger-api handles the building and balancing of transactions.

    cardano-node-client will live in cardano-node and handle the submitting of balanced transactions and querying the ledger state.

Both of these packages are in early stage development and will require end-to-end test coverage.

This document outlines the decisions and arguments for an additional approach to end-to-end test automation using a framework written in Haskell.

The exploratory testing approach is not in the scope of this document.","We will create a new end-to-end testing framework written in Haskell called plutus-e2e-tests that will initially be a package in plutus-apps, see argument 1.

    We will use cardano-testnet for configuring and initialising local test network environments, see argument 2.

    We will initially use cardano-api for building and balancing transactions, and to submit balanced transactions and for querying the ledger, see argument 3.

    When available, we will use cardano-ledger-api instead of cardano-api for building and balancing transactions, see argument 4.

    When available, we will use cardano-node-client instead of cardano-api to submit balanced transactions and for querying the ledger state to make test assertions, see argument 5.

    We will prioritise Plutus test coverage over cardano-node, see argument 6.

    We will start by creating a few tests with the node/ledger apis without depending on plutus-apps and then assess whether we want to use the Contract API and other off-chain tooling going forwards, see argument 7.

    We will continue adding a subset of Plutus tests to cardano-node-tests, see argument 8.","Architectural Decision Record

## Context
End-to-end testing of Plutus Core functionality is currently performed by a combination of automation and exploratory approaches, both are performed on public preview and pre-prod testnets using a real node. Automation test scenarios for Plutus are currently being run as part of the wider cardano-node-tests test suite, which uses a Python wrapper for cardano-cli. Those tests focus on general ledger/node/cli functionality and only cover a few key scenarios for Plutus functionality, such as TxInfo and SECP256k1 builtins.

There is also ongoing development work to separate the functionality of cardano-api out into two packages:

    cardano-ledger-api handles the building and balancing of transactions.

    cardano-node-client will live in cardano-node and handle the submitting of balanced transactions and querying the ledger state.

Both of these packages are in early stage development and will require end-to-end test coverage.

This document outlines the decisions and arguments for an additional approach to end-to-end test automation using a framework written in Haskell.

The exploratory testing approach is not in the scope of this document.

## Decision
","## Context

End-to-end testing of Plutus Core functionality is currently performed by a combination of automation and exploratory approaches, both are performed on public preview and pre-prod testnets using a real node. Automation test scenarios for Plutus are currently being run as part of the wider cardano-node-tests test suite, which uses a Python wrapper for cardano-cli. Those tests focus on general ledger/node/cli functionality and only cover a few key scenarios for Plutus functionality, such as TxInfo and SECP256k1 builtins.

There is also ongoing development work to separate the functionality of cardano-api out into two packages:

    cardano-ledger-api handles the building and balancing of transactions.

    cardano-node-client will live in cardano-node and handle the submitting of balanced transactions and querying the ledger state.

Both of these packages are in early stage development and will require end-to-end test coverage.

This document outlines the decisions and arguments for an additional approach to end-to-end test automation using a framework written in Haskell.

The exploratory testing approach is not in the scope of this document.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
End-to-end testing of Plutus Core functionality is currently performed by a combination of automation and exploratory approaches, both are performed on public preview and pre-prod testnets using a real node. Automation test scenarios for Plutus are currently being run as part of the wider cardano-node-tests test suite, which uses a Python wrapper for cardano-cli. Those tests focus on general ledger/node/cli functionality and only cover a few key scenarios for Plutus functionality, such as TxInfo and SECP256k1 builtins.

There is also ongoing development work to separate the functionality of cardano-api out into two packages:

    cardano-ledger-api handles the building and balancing of transactions.

    cardano-node-client will live in cardano-node and handle the submitting of balanced transactions and querying the ledger state.

Both of these packages are in early stage development and will require end-to-end test coverage.

This document outlines the decisions and arguments for an additional approach to end-to-end test automation using a framework written in Haskell.

The exploratory testing approach is not in the scope of this document."
"There are multiple implementations of a Plutus Application Backend (PAB) external of IO Global, and also other tools related to Plutus smart contracts. Some of them are using the same contract interface as the official implementation, but some of them use a different interface. However, as the ecosystem evolves, it would be beneficial to create a well defined standard, that other off-chain tools can use as a reference, or as an interface to implement.

Currently, as we are getting close to the Vasil hardfork, testing tools and Plutus Application backend tools are at a hurry to update their dependencies and get to a Vasil compliant/compatible state. However, tools that are depending on plutus-apps are blocked by the PAB development. This initiative was born out of this context, but could solve other problems as well.

The Contract API (defined in plutus-apps/plutus-contract) is using the freer-simple effect system to define all the contract effects. This already allows us to separate the interface from the implementation, and to have multiple implementations/interpreters for one interface. Currently, there are two implementations for the Contract API:

    one for the plutus-apps emulator (inside plutus-apps/plutus-contract)

    one for plutus-apps’ Plutus Application Backend (inside plutus-apps/plutus-pab)

Therefore, we can leverage this separatation of interface and implementation in order to move the interface out of plutus-apps.","We will split the plutus-apps/plutus-contract package into two parts: the Contract API (plutus-contract) and the emulator (plutus-contract-emulator).

    We will create effects for the constraints-based transaction builder library (plutus-apps/plutus-ledger-constraints) in the Contract API. Currently, the interface and the implementation in the transaction builder library are tightly coupled. Therefore, we need to decouple them.

    We will create a separate repository with the contract effects and types (the splitted plutus-contract). By moving the Contract API out of the plutus-apps monorepository, any tool could update to newer version to their discretion. Without many dependencies, many tools could utilize the Contract API without having to depend on the whole plutus-apps monorepo.

    We (the Plutus Tools at IO Global) will continue to be the main maintainers of this new repository. However, a new ADR will need to be created if we ever decide to make this a community driven project.

    TODO: What about governance? How do we decide which interface changes are accepted? ADRs? Who ultimately accepts and rejects them?","Architectural Decision Record

## Context
There are multiple implementations of a Plutus Application Backend (PAB) external of IO Global, and also other tools related to Plutus smart contracts. Some of them are using the same contract interface as the official implementation, but some of them use a different interface. However, as the ecosystem evolves, it would be beneficial to create a well defined standard, that other off-chain tools can use as a reference, or as an interface to implement.

Currently, as we are getting close to the Vasil hardfork, testing tools and Plutus Application backend tools are at a hurry to update their dependencies and get to a Vasil compliant/compatible state. However, tools that are depending on plutus-apps are blocked by the PAB development. This initiative was born out of this context, but could solve other problems as well.

The Contract API (defined in plutus-apps/plutus-contract) is using the freer-simple effect system to define all the contract effects. This already allows us to separate the interface from the implementation, and to have multiple implementations/interpreters for one interface. Currently, there are two implementations for the Contract API:

    one for the plutus-apps emulator (inside plutus-apps/plutus-contract)

    one for plutus-apps’ Plutus Application Backend (inside plutus-apps/plutus-pab)

Therefore, we can leverage this separatation of interface and implementation in order to move the interface out of plutus-apps.

## Decision
","## Context

There are multiple implementations of a Plutus Application Backend (PAB) external of IO Global, and also other tools related to Plutus smart contracts. Some of them are using the same contract interface as the official implementation, but some of them use a different interface. However, as the ecosystem evolves, it would be beneficial to create a well defined standard, that other off-chain tools can use as a reference, or as an interface to implement.

Currently, as we are getting close to the Vasil hardfork, testing tools and Plutus Application backend tools are at a hurry to update their dependencies and get to a Vasil compliant/compatible state. However, tools that are depending on plutus-apps are blocked by the PAB development. This initiative was born out of this context, but could solve other problems as well.

The Contract API (defined in plutus-apps/plutus-contract) is using the freer-simple effect system to define all the contract effects. This already allows us to separate the interface from the implementation, and to have multiple implementations/interpreters for one interface. Currently, there are two implementations for the Contract API:

    one for the plutus-apps emulator (inside plutus-apps/plutus-contract)

    one for plutus-apps’ Plutus Application Backend (inside plutus-apps/plutus-pab)

Therefore, we can leverage this separatation of interface and implementation in order to move the interface out of plutus-apps.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
There are multiple implementations of a Plutus Application Backend (PAB) external of IO Global, and also other tools related to Plutus smart contracts. Some of them are using the same contract interface as the official implementation, but some of them use a different interface. However, as the ecosystem evolves, it would be beneficial to create a well defined standard, that other off-chain tools can use as a reference, or as an interface to implement.

Currently, as we are getting close to the Vasil hardfork, testing tools and Plutus Application backend tools are at a hurry to update their dependencies and get to a Vasil compliant/compatible state. However, tools that are depending on plutus-apps are blocked by the PAB development. This initiative was born out of this context, but could solve other problems as well.

The Contract API (defined in plutus-apps/plutus-contract) is using the freer-simple effect system to define all the contract effects. This already allows us to separate the interface from the implementation, and to have multiple implementations/interpreters for one interface. Currently, there are two implementations for the Contract API:

    one for the plutus-apps emulator (inside plutus-apps/plutus-contract)

    one for plutus-apps’ Plutus Application Backend (inside plutus-apps/plutus-pab)

Therefore, we can leverage this separatation of interface and implementation in order to move the interface out of plutus-apps."
"We are in search for a means to document our architectural and design decisions for all of our components. In order to do that, there is practice called architectural decision records (“ADR”), that we can integrate into our workflow.

This does not replace actual architecture documentation, but provides people who are contributing:

    the means to understand architectural and design decisions that were made

    a framework for proposing changes to the current architecture

For each decision, it is important to consider the following factors:

    what we have decided to do

    why we have made this decision

    what we expect the impact of this decision to be

    what we have learned in the process

As we’re already using rST, Sphinxdoc and readthedocs, it would be practical to integrate these ADRs as part of our current documentation infrastructure.","We will use ADRs to document, propose and discuss any important or significant architectural and design decisions.

    The ADR format will follow the format described in Implications section.

    We will follow the convention of storing those ADRs as rST or Markdown formatted documents stored under the docs/adr directory, as exemplified in Nat Pryce’s adr-tools. This does not imply that we will be using adr-tools itself, as we might diverge from the proposed structure.

    We will keep rejected ADRs

    We will strive, if possible, to create an ADR as early as possible in relation to the actual implementation.","Architectural Decision Record

## Context
We are in search for a means to document our architectural and design decisions for all of our components. In order to do that, there is practice called architectural decision records (“ADR”), that we can integrate into our workflow.

This does not replace actual architecture documentation, but provides people who are contributing:

    the means to understand architectural and design decisions that were made

    a framework for proposing changes to the current architecture

For each decision, it is important to consider the following factors:

    what we have decided to do

    why we have made this decision

    what we expect the impact of this decision to be

    what we have learned in the process

As we’re already using rST, Sphinxdoc and readthedocs, it would be practical to integrate these ADRs as part of our current documentation infrastructure.

## Decision
","## Context

We are in search for a means to document our architectural and design decisions for all of our components. In order to do that, there is practice called architectural decision records (“ADR”), that we can integrate into our workflow.

This does not replace actual architecture documentation, but provides people who are contributing:

    the means to understand architectural and design decisions that were made

    a framework for proposing changes to the current architecture

For each decision, it is important to consider the following factors:

    what we have decided to do

    why we have made this decision

    what we expect the impact of this decision to be

    what we have learned in the process

As we’re already using rST, Sphinxdoc and readthedocs, it would be practical to integrate these ADRs as part of our current documentation infrastructure.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
We are in search for a means to document our architectural and design decisions for all of our components. In order to do that, there is practice called architectural decision records (“ADR”), that we can integrate into our workflow.

This does not replace actual architecture documentation, but provides people who are contributing:

    the means to understand architectural and design decisions that were made

    a framework for proposing changes to the current architecture

For each decision, it is important to consider the following factors:

    what we have decided to do

    why we have made this decision

    what we expect the impact of this decision to be

    what we have learned in the process

As we’re already using rST, Sphinxdoc and readthedocs, it would be practical to integrate these ADRs as part of our current documentation infrastructure."
"We need a central observability platform where we ingest all our logs, metrics and traces so that ops, devs and devops can analyze performance, reliability and uptime. We can try to build and host such a platform on our own or use SaaS providers like DataDog.
Decision Drivers
ReliabilityFeature-richnessCostMaintenanceDevelopmentVendor lock level","SaaS solution (DataDog) is a clear winner due to its hastle-free service usage, continuous improvement as well as its competitive pricing.
Positive Consequences
Easy access to observability for devs and devopsWell known integration target for third-party servicesMonitoring setup is well known in the industryNo special knowledge about setup of the plaftorm
Negative Consequences
None.","Architectural Decision Record

## Context
We need a central observability platform where we ingest all our logs, metrics and traces so that ops, devs and devops can analyze performance, reliability and uptime. We can try to build and host such a platform on our own or use SaaS providers like DataDog.
Decision Drivers
ReliabilityFeature-richnessCostMaintenanceDevelopmentVendor lock level

## Decision
","## Context

We need a central observability platform where we ingest all our logs, metrics and traces so that ops, devs and devops can analyze performance, reliability and uptime. We can try to build and host such a platform on our own or use SaaS providers like DataDog.
Decision Drivers
ReliabilityFeature-richnessCostMaintenanceDevelopmentVendor lock level","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
We need a central observability platform where we ingest all our logs, metrics and traces so that ops, devs and devops can analyze performance, reliability and uptime. We can try to build and host such a platform on our own or use SaaS providers like DataDog.
Decision Drivers
ReliabilityFeature-richnessCostMaintenanceDevelopmentVendor lock level"
"What protocol(s) shall we use as the new standard for authentication and authorization. It would be supported by our new centralized authority server and should be implemented in all new clients and resource systems needing authentication or authorization. A requirement might be made that the authority service need to support other protocols for legacy systems but all new systems should be encourage to use the same protocol.
Decision Drivers
SecureWell defined and well reviewed standardEasy to implement by client and resource systemsSupport for non web client systems i.e. mobile devices","Chosen option: ""OAuth 2.0 + OpenID Connect"", because it is secure and well examined and and has support libraries for our tech stack.","Architectural Decision Record

## Context
What protocol(s) shall we use as the new standard for authentication and authorization. It would be supported by our new centralized authority server and should be implemented in all new clients and resource systems needing authentication or authorization. A requirement might be made that the authority service need to support other protocols for legacy systems but all new systems should be encourage to use the same protocol.
Decision Drivers
SecureWell defined and well reviewed standardEasy to implement by client and resource systemsSupport for non web client systems i.e. mobile devices

## Decision
","## Context

What protocol(s) shall we use as the new standard for authentication and authorization. It would be supported by our new centralized authority server and should be implemented in all new clients and resource systems needing authentication or authorization. A requirement might be made that the authority service need to support other protocols for legacy systems but all new systems should be encourage to use the same protocol.
Decision Drivers
SecureWell defined and well reviewed standardEasy to implement by client and resource systemsSupport for non web client systems i.e. mobile devices","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
What protocol(s) shall we use as the new standard for authentication and authorization. It would be supported by our new centralized authority server and should be implemented in all new clients and resource systems needing authentication or authorization. A requirement might be made that the authority service need to support other protocols for legacy systems but all new systems should be encourage to use the same protocol.
Decision Drivers
SecureWell defined and well reviewed standardEasy to implement by client and resource systemsSupport for non web client systems i.e. mobile devices"
"It is the offical policy of the Digital Iceland and stated in the Techical Direction that it is to be implemented as free and open source. Open source software by definition is open to anyone to use, modify, distribute and study. These permissions are enforced an open source license. There are a number of well-known and widely used open source licenses available and we need to choose a license that best fits the goals of digital iceland.
There are two main types of open source licences: more permissive licences that confer broad freedoms and minimal obligations (e.g., the MIT, BSD and the Apache 2.0 licences); and sharealike licences that require licensing adaptations with the same licence if they distribute them (e.g., the GNU GPL).
Development for Digital Iceland will be open and free with minimum complications for development for all involved. Reuse and transparency will be promoted.
Decision Drivers
The primary motivation is to encourage, co-development, collabiration, transparency and reuse of the software.It is important to build on the experience of similar government led inititives in other countries.Digital Iceland has no patents or intellecatual property that needs to be protected or guarded by the license chosen.It is not a concern for Digital Iceland that the license restricts usage in other projects, be it open or closed source.","The MIT license was chosen, for the following reasons:
It is the least restrictive of the licenses.It is very consise, simple and easy to understand and therefore should be clear to users and developers.Digital Iceland does not require protection of patents or existing intelletual property.Well known government lead initiatives like uk.gov and X-Road use the MIT license.The MIT license is the best known and most widely used free and open-source license in the world.","Architectural Decision Record

## Context
It is the offical policy of the Digital Iceland and stated in the Techical Direction that it is to be implemented as free and open source. Open source software by definition is open to anyone to use, modify, distribute and study. These permissions are enforced an open source license. There are a number of well-known and widely used open source licenses available and we need to choose a license that best fits the goals of digital iceland.
There are two main types of open source licences: more permissive licences that confer broad freedoms and minimal obligations (e.g., the MIT, BSD and the Apache 2.0 licences); and sharealike licences that require licensing adaptations with the same licence if they distribute them (e.g., the GNU GPL).
Development for Digital Iceland will be open and free with minimum complications for development for all involved. Reuse and transparency will be promoted.
Decision Drivers
The primary motivation is to encourage, co-development, collabiration, transparency and reuse of the software.It is important to build on the experience of similar government led inititives in other countries.Digital Iceland has no patents or intellecatual property that needs to be protected or guarded by the license chosen.It is not a concern for Digital Iceland that the license restricts usage in other projects, be it open or closed source.

## Decision
","## Context

It is the offical policy of the Digital Iceland and stated in the Techical Direction that it is to be implemented as free and open source. Open source software by definition is open to anyone to use, modify, distribute and study. These permissions are enforced an open source license. There are a number of well-known and widely used open source licenses available and we need to choose a license that best fits the goals of digital iceland.
There are two main types of open source licences: more permissive licences that confer broad freedoms and minimal obligations (e.g., the MIT, BSD and the Apache 2.0 licences); and sharealike licences that require licensing adaptations with the same licence if they distribute them (e.g., the GNU GPL).
Development for Digital Iceland will be open and free with minimum complications for development for all involved. Reuse and transparency will be promoted.
Decision Drivers
The primary motivation is to encourage, co-development, collabiration, transparency and reuse of the software.It is important to build on the experience of similar government led inititives in other countries.Digital Iceland has no patents or intellecatual property that needs to be protected or guarded by the license chosen.It is not a concern for Digital Iceland that the license restricts usage in other projects, be it open or closed source.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
It is the offical policy of the Digital Iceland and stated in the Techical Direction that it is to be implemented as free and open source. Open source software by definition is open to anyone to use, modify, distribute and study. These permissions are enforced an open source license. There are a number of well-known and widely used open source licenses available and we need to choose a license that best fits the goals of digital iceland.
There are two main types of open source licences: more permissive licences that confer broad freedoms and minimal obligations (e.g., the MIT, BSD and the Apache 2.0 licences); and sharealike licences that require licensing adaptations with the same licence if they distribute them (e.g., the GNU GPL).
Development for Digital Iceland will be open and free with minimum complications for development for all involved. Reuse and transparency will be promoted.
Decision Drivers
The primary motivation is to encourage, co-development, collabiration, transparency and reuse of the software.It is important to build on the experience of similar government led inititives in other countries.Digital Iceland has no patents or intellecatual property that needs to be protected or guarded by the license chosen.It is not a concern for Digital Iceland that the license restricts usage in other projects, be it open or closed source."
"Viskuausan is proving to be more complex and larger platform than just a simple documentation site from static content. Which React framework provides the most out-of-the-box features that we need?
Decision Drivers
Should use NodeJS and React as outlined in SÍ technical direction​Should be able to support markdown content rendered to HTMLShould be open sourceShould be customizable to island.is UI design","Chosen option: NextJS + NestJS
NextJS is the chosen web framework for all island.is websites needing server side rendering. As Viskuausan will probably be merged with island.is main website, creating it using same frameworks makes it easy to merge later on. It is easier to reuse Island UI components using NextJS over Docusaurus. Docusaurus main advantage over Next is out-of-the-box markdown support but it is easy to add markdown support in NextJS using Remark library.
NestJS is used to create backend services and Viskuausan needs few backend services related to the X-Road and API GW integrations. Provides functionalities like ORM, dependency injection, unit testing.","Architectural Decision Record

## Context
Viskuausan is proving to be more complex and larger platform than just a simple documentation site from static content. Which React framework provides the most out-of-the-box features that we need?
Decision Drivers
Should use NodeJS and React as outlined in SÍ technical direction​Should be able to support markdown content rendered to HTMLShould be open sourceShould be customizable to island.is UI design

## Decision
","## Context

Viskuausan is proving to be more complex and larger platform than just a simple documentation site from static content. Which React framework provides the most out-of-the-box features that we need?
Decision Drivers
Should use NodeJS and React as outlined in SÍ technical direction​Should be able to support markdown content rendered to HTMLShould be open sourceShould be customizable to island.is UI design","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Viskuausan is proving to be more complex and larger platform than just a simple documentation site from static content. Which React framework provides the most out-of-the-box features that we need?
Decision Drivers
Should use NodeJS and React as outlined in SÍ technical direction​Should be able to support markdown content rendered to HTMLShould be open sourceShould be customizable to island.is UI design"
"We want a monorepo tool to help us to scale development up for multiple projects and teams. It should not be too much in the way, but help us manage code, dependencies and CI/CD.
Decision Drivers
Low complexity and overhead in development.Fit for our stack.Optimize CI/CD with dependency graphs and/or caching.Flexible.","Chosen option: ""Nx"", because:
It's specially designed around our stack (TypeScript, React, Node.JS, NPM, ESLint, Prettier, Cypress, Jest, NextJS).It's relatively easy to learn with focused documentation.It has schematics to generate apps, libraries and components that includes all of our tools.It is opinionated, which gives us a good base to start developing faster. Many things can still be configured or extended.","Architectural Decision Record

## Context
We want a monorepo tool to help us to scale development up for multiple projects and teams. It should not be too much in the way, but help us manage code, dependencies and CI/CD.
Decision Drivers
Low complexity and overhead in development.Fit for our stack.Optimize CI/CD with dependency graphs and/or caching.Flexible.

## Decision
","## Context

We want a monorepo tool to help us to scale development up for multiple projects and teams. It should not be too much in the way, but help us manage code, dependencies and CI/CD.
Decision Drivers
Low complexity and overhead in development.Fit for our stack.Optimize CI/CD with dependency graphs and/or caching.Flexible.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
We want a monorepo tool to help us to scale development up for multiple projects and teams. It should not be too much in the way, but help us manage code, dependencies and CI/CD.
Decision Drivers
Low complexity and overhead in development.Fit for our stack.Optimize CI/CD with dependency graphs and/or caching.Flexible."
"We're building websites and web applications that share a common design system with reusable components. How do we write CSS styles in a way that is performant and safe?
Decision Drivers
Should be performant, with code splitting, caching and minimal runtime overhead.Needs to have easy access to our design system constants. These should optimally be shared with JS logic.Should be type-safe to catch issues when refactoring.Reusable components should be closed, not accepting arbitrary styles/classes.We want a pattern for responsive props with atomic layout components.","Chosen option: Treat, because it combines the best of both worlds from CSS-in-JS and CSS modules.
We'll create shared components that have responsive props, but are otherwise closed for modifications. Theme variables are defined in a shared library with TypeScript.
Example:
// Good:<Box padding""small"" /><Box padding={{xs: 'small', md: 'medium'}} /><Input large /><Text preset=""heading3"" as=""p"" />
// Bad:<Box className={customLayout} /><Input style={{ height: 50, padding: 16 }} /><Text className={styles.heading} />
Positive Consequences
Treat is statically extracted at build time, so it has minimal runtime.Styles load in parallel with JS, also when code splitting.Styles are written in TypeScript which gives us type safety when referring to shared variables, styles and helpers.Styles are in special files, separate from markup and components giving us clear separation with good visibility into the rendered markup.We can pull in responsive layout component patterns from Braid, which gives us a good base to lay out components and pages.
Negative Consequences
We are choosing a pretty new framework, so it may 1) have bugs or issues, 2) be an obstacle for new developers or 3) be discontinued.When we're generating responsive styles at build time we need to be mindful at how many variations we allow (eg media queries, columns, whitespace), since they can easily bloat our CSS with unused styles.","Architectural Decision Record

## Context
We're building websites and web applications that share a common design system with reusable components. How do we write CSS styles in a way that is performant and safe?
Decision Drivers
Should be performant, with code splitting, caching and minimal runtime overhead.Needs to have easy access to our design system constants. These should optimally be shared with JS logic.Should be type-safe to catch issues when refactoring.Reusable components should be closed, not accepting arbitrary styles/classes.We want a pattern for responsive props with atomic layout components.

## Decision
","## Context

We're building websites and web applications that share a common design system with reusable components. How do we write CSS styles in a way that is performant and safe?
Decision Drivers
Should be performant, with code splitting, caching and minimal runtime overhead.Needs to have easy access to our design system constants. These should optimally be shared with JS logic.Should be type-safe to catch issues when refactoring.Reusable components should be closed, not accepting arbitrary styles/classes.We want a pattern for responsive props with atomic layout components.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
We're building websites and web applications that share a common design system with reusable components. How do we write CSS styles in a way that is performant and safe?
Decision Drivers
Should be performant, with code splitting, caching and minimal runtime overhead.Needs to have easy access to our design system constants. These should optimally be shared with JS logic.Should be type-safe to catch issues when refactoring.Reusable components should be closed, not accepting arbitrary styles/classes.We want a pattern for responsive props with atomic layout components."
"Know it before they do! We need a tool to discover, triage, and prioritize errors in real-time.","Chosen option: Sentry, because it ranks higher in a community survey regarding our stack (Javascript). It's also much cheaper and offers the choice to be completely free if we self-host it.","Architectural Decision Record

## Context
Know it before they do! We need a tool to discover, triage, and prioritize errors in real-time.

## Decision
","## Context

Know it before they do! We need a tool to discover, triage, and prioritize errors in real-time.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Know it before they do! We need a tool to discover, triage, and prioritize errors in real-time."
"As of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has defaults that differ between schematic types. In order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files and directories.
Decision Drivers
Provide consistency when navigating the codebaseThe earlier we decide on this, the better","Chosen option: Name files after their default export. If that default export is a React Component, or a class, then the file name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid using kebab-case and snake_case and make sure the name follows the default export of the file.
Naming directories should follow these guidelines: Only use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. island-ui instead of islandUi: import { Box } from '@island.is/island-ui/core'
Use PascalCase for directories only containing React components:
components/CtaButton/index.tsimport 'components/CtaButton'
or:
components/CtaButton/CtaButton.tsximport 'components/CtaButton/CtaButton'
rather than
components/cta-button/CtaButton.tsx
In all other cases, use camelCase.
Positive Consequences
Easier to navigate the codebaseFile names are more readable, and developers know what to expectThis approach is the most common practice, and something most JS and TS developers are familiar with.","Architectural Decision Record

## Context
As of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has defaults that differ between schematic types. In order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files and directories.
Decision Drivers
Provide consistency when navigating the codebaseThe earlier we decide on this, the better

## Decision
","## Context

As of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has defaults that differ between schematic types. In order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files and directories.
Decision Drivers
Provide consistency when navigating the codebaseThe earlier we decide on this, the better","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
As of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has defaults that differ between schematic types. In order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files and directories.
Decision Drivers
Provide consistency when navigating the codebaseThe earlier we decide on this, the better"
"island.is will be maintaining and publishing content from many different government agencies and institutions. Their technical skill may vary a great deal, the content skill may also be lacking, therefore it is paramount for the system to be user friendly and intuitive.
Agencies and institutions should have enough autonomy with regards to editing content they are responsible for, to minimise the manual labour required by the island.is editors.
Which CMS system would best suit the needs of island.is?
Decision Drivers
Content needs to be editable by non technical usersContent needs to be accessible across multiple domains and platformsSetup should be simple for developers new to the projectThe system should manage flexible content structures to limit systems impact on designThe system should be user friendly and easy to use for a non technical personThe system needs to offer a suitable workflow option to ease content management once multiple agencies start to contribute","Devs narrowed the choice down to two options Contentful and Contentstack.
Both systems meet the required featureset.
A decision from management was made to use Contentful. Contentful is deemed to have a larger presence in the Icelandic dev community. Contentful is also believed to have a stronger funding base. Contentful is already implemented in some of our projects.","Architectural Decision Record

## Context
island.is will be maintaining and publishing content from many different government agencies and institutions. Their technical skill may vary a great deal, the content skill may also be lacking, therefore it is paramount for the system to be user friendly and intuitive.
Agencies and institutions should have enough autonomy with regards to editing content they are responsible for, to minimise the manual labour required by the island.is editors.
Which CMS system would best suit the needs of island.is?
Decision Drivers
Content needs to be editable by non technical usersContent needs to be accessible across multiple domains and platformsSetup should be simple for developers new to the projectThe system should manage flexible content structures to limit systems impact on designThe system should be user friendly and easy to use for a non technical personThe system needs to offer a suitable workflow option to ease content management once multiple agencies start to contribute

## Decision
","## Context

island.is will be maintaining and publishing content from many different government agencies and institutions. Their technical skill may vary a great deal, the content skill may also be lacking, therefore it is paramount for the system to be user friendly and intuitive.
Agencies and institutions should have enough autonomy with regards to editing content they are responsible for, to minimise the manual labour required by the island.is editors.
Which CMS system would best suit the needs of island.is?
Decision Drivers
Content needs to be editable by non technical usersContent needs to be accessible across multiple domains and platformsSetup should be simple for developers new to the projectThe system should manage flexible content structures to limit systems impact on designThe system should be user friendly and easy to use for a non technical personThe system needs to offer a suitable workflow option to ease content management once multiple agencies start to contribute","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
island.is will be maintaining and publishing content from many different government agencies and institutions. Their technical skill may vary a great deal, the content skill may also be lacking, therefore it is paramount for the system to be user friendly and intuitive.
Agencies and institutions should have enough autonomy with regards to editing content they are responsible for, to minimise the manual labour required by the island.is editors.
Which CMS system would best suit the needs of island.is?
Decision Drivers
Content needs to be editable by non technical usersContent needs to be accessible across multiple domains and platformsSetup should be simple for developers new to the projectThe system should manage flexible content structures to limit systems impact on designThe system should be user friendly and easy to use for a non technical personThe system needs to offer a suitable workflow option to ease content management once multiple agencies start to contribute"
"We want to be able to roll out new features gradually, perform A/B testing and target individual groups with a new feature. Also, we want to be able to flip a switch to turn features on or off for everyone.
Decision Drivers
Ease of setupEase of maintenanceCostDeveloper experienceUsability/UXOperational concernsHandling of PII","Chosen option: ""ConfigCat"", because:
We can probably get away with using it for very low costWe can start using it almost right away with little configuration
If we decide later that we would like some of the features of LaunchDarkly, we want to be able to quickly swap. Thus, it is vital that we write some kind of service-agnostic wrapper.
Positive Consequences
We can start using feature flags across our stack.
Negative Consequences
Complexity of applications will increase","Architectural Decision Record

## Context
We want to be able to roll out new features gradually, perform A/B testing and target individual groups with a new feature. Also, we want to be able to flip a switch to turn features on or off for everyone.
Decision Drivers
Ease of setupEase of maintenanceCostDeveloper experienceUsability/UXOperational concernsHandling of PII

## Decision
","## Context

We want to be able to roll out new features gradually, perform A/B testing and target individual groups with a new feature. Also, we want to be able to flip a switch to turn features on or off for everyone.
Decision Drivers
Ease of setupEase of maintenanceCostDeveloper experienceUsability/UXOperational concernsHandling of PII","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
We want to be able to roll out new features gradually, perform A/B testing and target individual groups with a new feature. Also, we want to be able to flip a switch to turn features on or off for everyone.
Decision Drivers
Ease of setupEase of maintenanceCostDeveloper experienceUsability/UXOperational concernsHandling of PII"
"Is there available tool that is compliant to requirements? Can the tool provide functionality for API Gateway? Can the tool provide functionality for API Development Portal?
Requirements
Since the API Gateway is intended for students and startups to gather open government data, the following requirements need to be fulfilled. The student / startup is defined as Consumer of service. The organization that deliver the service as open service is defined as Provider of service. The open service is hosted on organizations X-Road server. The API Gateway must provide functionality for
Registration of services with rate limitSelf-service portalAPI key for ConsumerRate limit for Consumer
Provider
Register open service in API gateway.Set rate limit on open service in API gateway.
Consumer
Register as service user.Register application intended to use API (Consideration).Get API key for that application or consumer.Register what API to use in the application.Ability to test the API from API console with application API key (Consideration).
Considerations
API Keys / Rate Limits Is it sufficient to have only One API key for each Consumer, or is it required to define different Consumer applications with different API key. If a typical Consumer has created application and has valid API key, it is likely that he will not bother to register new application and get new API key. He could reuse the already given one. Consumer could also use the ability to register another application and get new API key with fresh rate limits.
Consumer registration What will be used to validate / approve students or startups for access on services. Should it be registered by SSN or some unique id, or only by email, with ability to reregister with new email again and again.
Decision Drivers
Vendor lock in for runtimeOpen source or notInstallation optionsFunctional abilityMarket presencePricing
Pricing model for API management solutions are complex. Usually based on transaction count, or CPU instances. Sometimes pricing is variation of annual fee and transactional fee. All prices that are exposed in this documentation are estimates, needed to be negotiated with vendor.
There is consideration that most API management providers are aiming customers in hosted solutions, instead of on-prem installation, that would provide lock in for that vendor.
Functional ability for decision
Functional ability to consider when evaluating the tool. The following list was taken from wikipedia page for API Management.
Gateway: a server that acts as an API front-end, receives API requests, enforces throttling and security policies, passes requests to the back-end service and then passes the response back to the requester. A gateway often includes a transformation engine to orchestrate and modify the requests and responses on the fly. A gateway can also provide functionality such as collecting analytics data and providing caching. The gateway can provide functionality to support authentication, authorization, security, audit and regulatory compliance.Publishing tools: a collection of tools that API providers use to define APIs, for instance using the OpenAPI or RAML specifications, generate API documentation, manage access and usage policies for APIs, test and debug the execution of API, including security testing and automated generation of tests and test suites, deploy APIs into production, staging, and quality assurance environments, and coordinate the overall API lifecycle.Developer portal/API store: community site, typically branded by an API provider, that can encapsulate for API users in a single convenient source information and functionality including documentation, tutorials, sample code, software development kits, an interactive API console and sandbox to trial APIs, the ability to subscribe to the APIs and manage subscription keys such as OAuth2 Client ID and Client Secret, and obtain support from the API provider and user and community.Reporting and analytics: functionality to monitor API usage and load (overall hits, completed transactions, number of data objects returned, amount of compute time and other internal resources consumed, volume of data transferred). This can include real-time monitoring of the API with alerts being raised directly or via a higher-level network management system, for instance, if the load on an API has become too great, as well as functionality to analyze historical data, such as transaction logs, to detect usage trends. Functionality can also be provided to create synthetic transactions that can be used to test the performance and behavior of API endpoints. The information gathered by the reporting and analytics functionality can be used by the API provider to optimize the API offering within an organization's overall continuous improvement process and for defining software Service-Level Agreements for APIs.Monetization: functionality to support charging for access to commercial APIs. This functionality can include support for setting up pricing rules, based on usage, load and functionality, issuing invoices and collecting payments including multiple types of credit card payments.","We recommend using hosted solution from AWS for API gateway. The reason is that if we look at the requirements and other architectural decisions in the project, the AWS solution does both fit in the architecture and pricing based on usage is cheaper than in other options. We made a pricing estimate for five years. Based on 100 million API calls per month the price of using AWS is one third of bought enterprise or homemade solutions. If the usage is 20 million API calls per month, we are looking at one fifth of the enterprise solutions. Note that there is also cost in using the management API and storage cost of logging, but it seems to be fraction of the total cost of using the product. Since the requirement is to host X-Road services that are defined as open and hosted at organization, we need to access the service through open X-Road server. Currently there will be installed X-Road server on AWS environment to use by island.is, that server can also be used by API gateway. Downside is that all request needs to go to the AWS environment, and then go back to Iceland. According to vendor lock in, then the investment cost of this option is not in the range that it will stop us to change to other solution. That could happen if requirement changes or the usage will be more than expected. This decision is based on the requirements and intended usage. If those requirements change, for example we would use the API portal as portal for Viskuausan, or more intense usage is expected, other options could be more relevant.
Following is the decision phases used to get this conclusion.
Second phase of decision
For Open Source tool, we recommend usage of Kong Community Edition, with custom made API Developer Portal and Analytics. The analytics part could be based on ELK stack through plugin. The Kong community is large, and there are some Developers Portals available in the Open Source community. There is also some plugin available for logging and monitoring available. Kong API Gateway provides rest interface that can be used for customizing API Portal, and ability to create custom plugins for custom implementations. This decision provides more custom code to be developed and we need to rely on that the community can provide plugins. We also rely on that the Kong product will remain open source, but lot of plugins are only available for Enterprise edition.
WSO2 could be considered, since the whole suite is Open Source, so API Developer Portal and Admin UI is part of their Open Source offering. It is not recommend to use it without paid support plan.
For Vendor specific tool we recommend Software AG API Management. It is fully functional with customizable Developer Portal, and analysis tool. It has both partner and customers in Iceland. Current pricing model is based on transaction count and same applies for On-Prem vs. Hosted implementations. There are no cloud provider or runtime lock in. Implementation is that the tool needs to be installed and configured. The Developer Portal needs to be customized. For starter it is also option to host the installation at Advania for further evaluation. Analytics are fully integrated to ELK stack.
For hosted solutions, we recommend AWS, since it best fits the architectural decisions made for island.is
First phase of decision
In the first decision phase the following tools were initially pinpointed for further analysis. That was based on the option to run the API Management tool on premise. In decision outcome above, the tools have been narrowed to two options.
If open source options are not a requirement, it is suggested to evaluate the following tools.
​Software AG API Management​​IBM API Connect​​Google Apigee Edge​​Mulesoft Anypoint​​Axway Ampify​
These are the tools that are most mature, and do not provide lock in for runtime platform. They need to be evaluated based on pricing and technical ability.
If true open source is required, it is suggested to evaluate the following tools.
​WSO2​​Kong​
These tools provide open source offering, but in most cases the features for Api Developer Portal and Publishing tools are only part of enterprise offering with subscription. Evaluation is needed for validating if the open source offering of the tool contains what is needed for implementation. We need to evaluate pricing of enterprise offering against the price of creating/implementing required pieces, like custom Api Developer Portal.
For hosted solution, the following options is considered
​AWS​​Microsoft Azure API Management​​Google Apigee Edge​
Downside is that they are all platform dependent, to the owners proposed platform with more limited on-premise options. These are all top of the line tools according to capabilities.
For all considered tools we need to check what underlying software components are required. For example, data storage, queuing, and logging capacity. We need to take into consideration effort and ability to build Developer portal, compared to customizing the tool offering.
Other tools that we looked at did in our opinion lack functionality or other ability for further considered, even though many of them could be considered. Note that the list provided is not all existing Api Management tools, so other options might apply.
Positive Consequences
Api Management tools are listed and grouped based on if they have open source option or not.
Negative Consequences
All considered options are vendor lock in.","Architectural Decision Record

## Context
Is there available tool that is compliant to requirements? Can the tool provide functionality for API Gateway? Can the tool provide functionality for API Development Portal?
Requirements
Since the API Gateway is intended for students and startups to gather open government data, the following requirements need to be fulfilled. The student / startup is defined as Consumer of service. The organization that deliver the service as open service is defined as Provider of service. The open service is hosted on organizations X-Road server. The API Gateway must provide functionality for
Registration of services with rate limitSelf-service portalAPI key for ConsumerRate limit for Consumer
Provider
Register open service in API gateway.Set rate limit on open service in API gateway.
Consumer
Register as service user.Register application intended to use API (Consideration).Get API key for that application or consumer.Register what API to use in the application.Ability to test the API from API console with application API key (Consideration).
Considerations
API Keys / Rate Limits Is it sufficient to have only One API key for each Consumer, or is it required to define different Consumer applications with different API key. If a typical Consumer has created application and has valid API key, it is likely that he will not bother to register new application and get new API key. He could reuse the already given one. Consumer could also use the ability to register another application and get new API key with fresh rate limits.
Consumer registration What will be used to validate / approve students or startups for access on services. Should it be registered by SSN or some unique id, or only by email, with ability to reregister with new email again and again.
Decision Drivers
Vendor lock in for runtimeOpen source or notInstallation optionsFunctional abilityMarket presencePricing
Pricing model for API management solutions are complex. Usually based on transaction count, or CPU instances. Sometimes pricing is variation of annual fee and transactional fee. All prices that are exposed in this documentation are estimates, needed to be negotiated with vendor.
There is consideration that most API management providers are aiming customers in hosted solutions, instead of on-prem installation, that would provide lock in for that vendor.
Functional ability for decision
Functional ability to consider when evaluating the tool. The following list was taken from wikipedia page for API Management.
Gateway: a server that acts as an API front-end, receives API requests, enforces throttling and security policies, passes requests to the back-end service and then passes the response back to the requester. A gateway often includes a transformation engine to orchestrate and modify the requests and responses on the fly. A gateway can also provide functionality such as collecting analytics data and providing caching. The gateway can provide functionality to support authentication, authorization, security, audit and regulatory compliance.Publishing tools: a collection of tools that API providers use to define APIs, for instance using the OpenAPI or RAML specifications, generate API documentation, manage access and usage policies for APIs, test and debug the execution of API, including security testing and automated generation of tests and test suites, deploy APIs into production, staging, and quality assurance environments, and coordinate the overall API lifecycle.Developer portal/API store: community site, typically branded by an API provider, that can encapsulate for API users in a single convenient source information and functionality including documentation, tutorials, sample code, software development kits, an interactive API console and sandbox to trial APIs, the ability to subscribe to the APIs and manage subscription keys such as OAuth2 Client ID and Client Secret, and obtain support from the API provider and user and community.Reporting and analytics: functionality to monitor API usage and load (overall hits, completed transactions, number of data objects returned, amount of compute time and other internal resources consumed, volume of data transferred). This can include real-time monitoring of the API with alerts being raised directly or via a higher-level network management system, for instance, if the load on an API has become too great, as well as functionality to analyze historical data, such as transaction logs, to detect usage trends. Functionality can also be provided to create synthetic transactions that can be used to test the performance and behavior of API endpoints. The information gathered by the reporting and analytics functionality can be used by the API provider to optimize the API offering within an organization's overall continuous improvement process and for defining software Service-Level Agreements for APIs.Monetization: functionality to support charging for access to commercial APIs. This functionality can include support for setting up pricing rules, based on usage, load and functionality, issuing invoices and collecting payments including multiple types of credit card payments.

## Decision
","## Context

Is there available tool that is compliant to requirements? Can the tool provide functionality for API Gateway? Can the tool provide functionality for API Development Portal?
Requirements
Since the API Gateway is intended for students and startups to gather open government data, the following requirements need to be fulfilled. The student / startup is defined as Consumer of service. The organization that deliver the service as open service is defined as Provider of service. The open service is hosted on organizations X-Road server. The API Gateway must provide functionality for
Registration of services with rate limitSelf-service portalAPI key for ConsumerRate limit for Consumer
Provider
Register open service in API gateway.Set rate limit on open service in API gateway.
Consumer
Register as service user.Register application intended to use API (Consideration).Get API key for that application or consumer.Register what API to use in the application.Ability to test the API from API console with application API key (Consideration).
Considerations
API Keys / Rate Limits Is it sufficient to have only One API key for each Consumer, or is it required to define different Consumer applications with different API key. If a typical Consumer has created application and has valid API key, it is likely that he will not bother to register new application and get new API key. He could reuse the already given one. Consumer could also use the ability to register another application and get new API key with fresh rate limits.
Consumer registration What will be used to validate / approve students or startups for access on services. Should it be registered by SSN or some unique id, or only by email, with ability to reregister with new email again and again.
Decision Drivers
Vendor lock in for runtimeOpen source or notInstallation optionsFunctional abilityMarket presencePricing
Pricing model for API management solutions are complex. Usually based on transaction count, or CPU instances. Sometimes pricing is variation of annual fee and transactional fee. All prices that are exposed in this documentation are estimates, needed to be negotiated with vendor.
There is consideration that most API management providers are aiming customers in hosted solutions, instead of on-prem installation, that would provide lock in for that vendor.
Functional ability for decision
Functional ability to consider when evaluating the tool. The following list was taken from wikipedia page for API Management.
Gateway: a server that acts as an API front-end, receives API requests, enforces throttling and security policies, passes requests to the back-end service and then passes the response back to the requester. A gateway often includes a transformation engine to orchestrate and modify the requests and responses on the fly. A gateway can also provide functionality such as collecting analytics data and providing caching. The gateway can provide functionality to support authentication, authorization, security, audit and regulatory compliance.Publishing tools: a collection of tools that API providers use to define APIs, for instance using the OpenAPI or RAML specifications, generate API documentation, manage access and usage policies for APIs, test and debug the execution of API, including security testing and automated generation of tests and test suites, deploy APIs into production, staging, and quality assurance environments, and coordinate the overall API lifecycle.Developer portal/API store: community site, typically branded by an API provider, that can encapsulate for API users in a single convenient source information and functionality including documentation, tutorials, sample code, software development kits, an interactive API console and sandbox to trial APIs, the ability to subscribe to the APIs and manage subscription keys such as OAuth2 Client ID and Client Secret, and obtain support from the API provider and user and community.Reporting and analytics: functionality to monitor API usage and load (overall hits, completed transactions, number of data objects returned, amount of compute time and other internal resources consumed, volume of data transferred). This can include real-time monitoring of the API with alerts being raised directly or via a higher-level network management system, for instance, if the load on an API has become too great, as well as functionality to analyze historical data, such as transaction logs, to detect usage trends. Functionality can also be provided to create synthetic transactions that can be used to test the performance and behavior of API endpoints. The information gathered by the reporting and analytics functionality can be used by the API provider to optimize the API offering within an organization's overall continuous improvement process and for defining software Service-Level Agreements for APIs.Monetization: functionality to support charging for access to commercial APIs. This functionality can include support for setting up pricing rules, based on usage, load and functionality, issuing invoices and collecting payments including multiple types of credit card payments.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Is there available tool that is compliant to requirements? Can the tool provide functionality for API Gateway? Can the tool provide functionality for API Development Portal?
Requirements
Since the API Gateway is intended for students and startups to gather open government data, the following requirements need to be fulfilled. The student / startup is defined as Consumer of service. The organization that deliver the service as open service is defined as Provider of service. The open service is hosted on organizations X-Road server. The API Gateway must provide functionality for
Registration of services with rate limitSelf-service portalAPI key for ConsumerRate limit for Consumer
Provider
Register open service in API gateway.Set rate limit on open service in API gateway.
Consumer
Register as service user.Register application intended to use API (Consideration).Get API key for that application or consumer.Register what API to use in the application.Ability to test the API from API console with application API key (Consideration).
Considerations
API Keys / Rate Limits Is it sufficient to have only One API key for each Consumer, or is it required to define different Consumer applications with different API key. If a typical Consumer has created application and has valid API key, it is likely that he will not bother to register new application and get new API key. He could reuse the already given one. Consumer could also use the ability to register another application and get new API key with fresh rate limits.
Consumer registration What will be used to validate / approve students or startups for access on services. Should it be registered by SSN or some unique id, or only by email, with ability to reregister with new email again and again.
Decision Drivers
Vendor lock in for runtimeOpen source or notInstallation optionsFunctional abilityMarket presencePricing
Pricing model for API management solutions are complex. Usually based on transaction count, or CPU instances. Sometimes pricing is variation of annual fee and transactional fee. All prices that are exposed in this documentation are estimates, needed to be negotiated with vendor.
There is consideration that most API management providers are aiming customers in hosted solutions, instead of on-prem installation, that would provide lock in for that vendor.
Functional ability for decision
Functional ability to consider when evaluating the tool. The following list was taken from wikipedia page for API Management.
Gateway: a server that acts as an API front-end, receives API requests, enforces throttling and security policies, passes requests to the back-end service and then passes the response back to the requester. A gateway often includes a transformation engine to orchestrate and modify the requests and responses on the fly. A gateway can also provide functionality such as collecting analytics data and providing caching. The gateway can provide functionality to support authentication, authorization, security, audit and regulatory compliance.Publishing tools: a collection of tools that API providers use to define APIs, for instance using the OpenAPI or RAML specifications, generate API documentation, manage access and usage policies for APIs, test and debug the execution of API, including security testing and automated generation of tests and test suites, deploy APIs into production, staging, and quality assurance environments, and coordinate the overall API lifecycle.Developer portal/API store: community site, typically branded by an API provider, that can encapsulate for API users in a single convenient source information and functionality including documentation, tutorials, sample code, software development kits, an interactive API console and sandbox to trial APIs, the ability to subscribe to the APIs and manage subscription keys such as OAuth2 Client ID and Client Secret, and obtain support from the API provider and user and community.Reporting and analytics: functionality to monitor API usage and load (overall hits, completed transactions, number of data objects returned, amount of compute time and other internal resources consumed, volume of data transferred). This can include real-time monitoring of the API with alerts being raised directly or via a higher-level network management system, for instance, if the load on an API has become too great, as well as functionality to analyze historical data, such as transaction logs, to detect usage trends. Functionality can also be provided to create synthetic transactions that can be used to test the performance and behavior of API endpoints. The information gathered by the reporting and analytics functionality can be used by the API provider to optimize the API offering within an organization's overall continuous improvement process and for defining software Service-Level Agreements for APIs.Monetization: functionality to support charging for access to commercial APIs. This functionality can include support for setting up pricing rules, based on usage, load and functionality, issuing invoices and collecting payments including multiple types of credit card payments."
"We need to maintain the quality of the codebase, minimize the time between introducing quality degradation and discovering it and make sure we have deployable artefacts at all times. In the context of a monorepo we need to do this efficiently in order to make this process scale for an ever-growing number of projects in the repository.
Terms
code integration - this is a process that checks the integrity/quality of the code - static code analysis, code formatting, compilation, running automated tests, etc. The process is usually in the form of one or more scripts and uses tools local to the repository with minimum external dependencies.artefact building - this is a process that packages artefacts, labels them and usually publishes them to a central artefact repository so that they can be used by the deployment process. This process makes sense to be executed only after code integration process finishes successfully.continuous integration - the practice of running the code integration process triggered by events such aspushing new revisions of the code to the code repositoryopening a Pull Requestfixed scheduleetc.We also run the artefact building process after a successful code integration process to have artefacts ready for deployment at all times.continuous integration platform (CI platform from here on) - it is a platform (self-hosted or SaaS) that provides integrations to make it easy to run your continuous integration and publish the results
pushing new revisions of the code to the code repositoryopening a Pull Requestfixed scheduleetc.
Decision Drivers (Policy)
The code integration process needs to be independent from CI platform integrationBenefitsEasier troubleshooting and development of the processEasier migration to a different CI platformEasier experimentationEasier to run as part of the development processDrawbacksNeeds more knowledge and experienceWe use Docker as much as possible to implement the steps in the integration processBenefitsPlatform independenceRepeatabilitySecurityDrawbacks:Needs expertise in Dockerfile and Docker in generalWe only build the code affected by the change but re-tag all unchanged code artefactsBenefitsBe able to release a consistent version of all necessary services. The Docker images for all services in the monorepo should have the same Docker image tag available for a commit hashSupports the monorepo benefits and ideologyDrawbacksCan be tricky, especially for artefacts that are not Docker images(currently we do not plan to have those)We support only Linux as a target operating system when we cannot use DockerBenefitsSame as the OS we use in our production environment, which minimizes the chances of failure because of OS differencesWe minimize the effort and complexity of supporting different operating systemsLinux tooling works on all major other OSs today - macOS and WindowsDrawbacks:Devs that use non-Linux OS might need to install additional software and customizations
BenefitsEasier troubleshooting and development of the processEasier migration to a different CI platformEasier experimentationEasier to run as part of the development processDrawbacksNeeds more knowledge and experience
Easier troubleshooting and development of the processEasier migration to a different CI platformEasier experimentationEasier to run as part of the development process
Needs more knowledge and experience
BenefitsPlatform independenceRepeatabilitySecurityDrawbacks:Needs expertise in Dockerfile and Docker in general
Platform independenceRepeatabilitySecurity
Needs expertise in Dockerfile and Docker in general
BenefitsBe able to release a consistent version of all necessary services. The Docker images for all services in the monorepo should have the same Docker image tag available for a commit hashSupports the monorepo benefits and ideologyDrawbacksCan be tricky, especially for artefacts that are not Docker images(currently we do not plan to have those)
Be able to release a consistent version of all necessary services. The Docker images for all services in the monorepo should have the same Docker image tag available for a commit hashSupports the monorepo benefits and ideology
Can be tricky, especially for artefacts that are not Docker images(currently we do not plan to have those)
BenefitsSame as the OS we use in our production environment, which minimizes the chances of failure because of OS differencesWe minimize the effort and complexity of supporting different operating systemsLinux tooling works on all major other OSs today - macOS and WindowsDrawbacks:Devs that use non-Linux OS might need to install additional software and customizations
Same as the OS we use in our production environment, which minimizes the chances of failure because of OS differencesWe minimize the effort and complexity of supporting different operating systemsLinux tooling works on all major other OSs today - macOS and Windows
Devs that use non-Linux OS might need to install additional software and customizations
CI Platform ","GitHub Actions
Number 1 CI platform on GitHub at the time of this writingEasy customization of which parts of the CI process to run depending on branching patterns and pull requestsGood integration of code health with the pull request processAs a GitHub open-source project, we have an unlimited number of ""compute""-minutes that come as a part of the packageSupports parallelisation of the process which can be pretty important in the context of monorepoSupport using own runners which can be helpful to maximize speed, minimize costs and increase security.","Architectural Decision Record

## Context
We need to maintain the quality of the codebase, minimize the time between introducing quality degradation and discovering it and make sure we have deployable artefacts at all times. In the context of a monorepo we need to do this efficiently in order to make this process scale for an ever-growing number of projects in the repository.
Terms
code integration - this is a process that checks the integrity/quality of the code - static code analysis, code formatting, compilation, running automated tests, etc. The process is usually in the form of one or more scripts and uses tools local to the repository with minimum external dependencies.artefact building - this is a process that packages artefacts, labels them and usually publishes them to a central artefact repository so that they can be used by the deployment process. This process makes sense to be executed only after code integration process finishes successfully.continuous integration - the practice of running the code integration process triggered by events such aspushing new revisions of the code to the code repositoryopening a Pull Requestfixed scheduleetc.We also run the artefact building process after a successful code integration process to have artefacts ready for deployment at all times.continuous integration platform (CI platform from here on) - it is a platform (self-hosted or SaaS) that provides integrations to make it easy to run your continuous integration and publish the results
pushing new revisions of the code to the code repositoryopening a Pull Requestfixed scheduleetc.
Decision Drivers (Policy)
The code integration process needs to be independent from CI platform integrationBenefitsEasier troubleshooting and development of the processEasier migration to a different CI platformEasier experimentationEasier to run as part of the development processDrawbacksNeeds more knowledge and experienceWe use Docker as much as possible to implement the steps in the integration processBenefitsPlatform independenceRepeatabilitySecurityDrawbacks:Needs expertise in Dockerfile and Docker in generalWe only build the code affected by the change but re-tag all unchanged code artefactsBenefitsBe able to release a consistent version of all necessary services. The Docker images for all services in the monorepo should have the same Docker image tag available for a commit hashSupports the monorepo benefits and ideologyDrawbacksCan be tricky, especially for artefacts that are not Docker images(currently we do not plan to have those)We support only Linux as a target operating system when we cannot use DockerBenefitsSame as the OS we use in our production environment, which minimizes the chances of failure because of OS differencesWe minimize the effort and complexity of supporting different operating systemsLinux tooling works on all major other OSs today - macOS and WindowsDrawbacks:Devs that use non-Linux OS might need to install additional software and customizations
BenefitsEasier troubleshooting and development of the processEasier migration to a different CI platformEasier experimentationEasier to run as part of the development processDrawbacksNeeds more knowledge and experience
Easier troubleshooting and development of the processEasier migration to a different CI platformEasier experimentationEasier to run as part of the development process
Needs more knowledge and experience
BenefitsPlatform independenceRepeatabilitySecurityDrawbacks:Needs expertise in Dockerfile and Docker in general
Platform independenceRepeatabilitySecurity
Needs expertise in Dockerfile and Docker in general
BenefitsBe able to release a consistent version of all necessary services. The Docker images for all services in the monorepo should have the same Docker image tag available for a commit hashSupports the monorepo benefits and ideologyDrawbacksCan be tricky, especially for artefacts that are not Docker images(currently we do not plan to have those)
Be able to release a consistent version of all necessary services. The Docker images for all services in the monorepo should have the same Docker image tag available for a commit hashSupports the monorepo benefits and ideology
Can be tricky, especially for artefacts that are not Docker images(currently we do not plan to have those)
BenefitsSame as the OS we use in our production environment, which minimizes the chances of failure because of OS differencesWe minimize the effort and complexity of supporting different operating systemsLinux tooling works on all major other OSs today - macOS and WindowsDrawbacks:Devs that use non-Linux OS might need to install additional software and customizations
Same as the OS we use in our production environment, which minimizes the chances of failure because of OS differencesWe minimize the effort and complexity of supporting different operating systemsLinux tooling works on all major other OSs today - macOS and Windows
Devs that use non-Linux OS might need to install additional software and customizations
CI Platform 

## Decision
","## Context

We need to maintain the quality of the codebase, minimize the time between introducing quality degradation and discovering it and make sure we have deployable artefacts at all times. In the context of a monorepo we need to do this efficiently in order to make this process scale for an ever-growing number of projects in the repository.
Terms
code integration - this is a process that checks the integrity/quality of the code - static code analysis, code formatting, compilation, running automated tests, etc. The process is usually in the form of one or more scripts and uses tools local to the repository with minimum external dependencies.artefact building - this is a process that packages artefacts, labels them and usually publishes them to a central artefact repository so that they can be used by the deployment process. This process makes sense to be executed only after code integration process finishes successfully.continuous integration - the practice of running the code integration process triggered by events such aspushing new revisions of the code to the code repositoryopening a Pull Requestfixed scheduleetc.We also run the artefact building process after a successful code integration process to have artefacts ready for deployment at all times.continuous integration platform (CI platform from here on) - it is a platform (self-hosted or SaaS) that provides integrations to make it easy to run your continuous integration and publish the results
pushing new revisions of the code to the code repositoryopening a Pull Requestfixed scheduleetc.
Decision Drivers (Policy)
The code integration process needs to be independent from CI platform integrationBenefitsEasier troubleshooting and development of the processEasier migration to a different CI platformEasier experimentationEasier to run as part of the development processDrawbacksNeeds more knowledge and experienceWe use Docker as much as possible to implement the steps in the integration processBenefitsPlatform independenceRepeatabilitySecurityDrawbacks:Needs expertise in Dockerfile and Docker in generalWe only build the code affected by the change but re-tag all unchanged code artefactsBenefitsBe able to release a consistent version of all necessary services. The Docker images for all services in the monorepo should have the same Docker image tag available for a commit hashSupports the monorepo benefits and ideologyDrawbacksCan be tricky, especially for artefacts that are not Docker images(currently we do not plan to have those)We support only Linux as a target operating system when we cannot use DockerBenefitsSame as the OS we use in our production environment, which minimizes the chances of failure because of OS differencesWe minimize the effort and complexity of supporting different operating systemsLinux tooling works on all major other OSs today - macOS and WindowsDrawbacks:Devs that use non-Linux OS might need to install additional software and customizations
BenefitsEasier troubleshooting and development of the processEasier migration to a different CI platformEasier experimentationEasier to run as part of the development processDrawbacksNeeds more knowledge and experience
Easier troubleshooting and development of the processEasier migration to a different CI platformEasier experimentationEasier to run as part of the development process
Needs more knowledge and experience
BenefitsPlatform independenceRepeatabilitySecurityDrawbacks:Needs expertise in Dockerfile and Docker in general
Platform independenceRepeatabilitySecurity
Needs expertise in Dockerfile and Docker in general
BenefitsBe able to release a consistent version of all necessary services. The Docker images for all services in the monorepo should have the same Docker image tag available for a commit hashSupports the monorepo benefits and ideologyDrawbacksCan be tricky, especially for artefacts that are not Docker images(currently we do not plan to have those)
Be able to release a consistent version of all necessary services. The Docker images for all services in the monorepo should have the same Docker image tag available for a commit hashSupports the monorepo benefits and ideology
Can be tricky, especially for artefacts that are not Docker images(currently we do not plan to have those)
BenefitsSame as the OS we use in our production environment, which minimizes the chances of failure because of OS differencesWe minimize the effort and complexity of supporting different operating systemsLinux tooling works on all major other OSs today - macOS and WindowsDrawbacks:Devs that use non-Linux OS might need to install additional software and customizations
Same as the OS we use in our production environment, which minimizes the chances of failure because of OS differencesWe minimize the effort and complexity of supporting different operating systemsLinux tooling works on all major other OSs today - macOS and Windows
Devs that use non-Linux OS might need to install additional software and customizations
CI Platform ","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
We need to maintain the quality of the codebase, minimize the time between introducing quality degradation and discovering it and make sure we have deployable artefacts at all times. In the context of a monorepo we need to do this efficiently in order to make this process scale for an ever-growing number of projects in the repository.
Terms
code integration - this is a process that checks the integrity/quality of the code - static code analysis, code formatting, compilation, running automated tests, etc. The process is usually in the form of one or more scripts and uses tools local to the repository with minimum external dependencies.artefact building - this is a process that packages artefacts, labels them and usually publishes them to a central artefact repository so that they can be used by the deployment process. This process makes sense to be executed only after code integration process finishes successfully.continuous integration - the practice of running the code integration process triggered by events such aspushing new revisions of the code to the code repositoryopening a Pull Requestfixed scheduleetc.We also run the artefact building process after a successful code integration process to have artefacts ready for deployment at all times.continuous integration platform (CI platform from here on) - it is a platform (self-hosted or SaaS) that provides integrations to make it easy to run your continuous integration and publish the results
pushing new revisions of the code to the code repositoryopening a Pull Requestfixed scheduleetc.
Decision Drivers (Policy)
The code integration process needs to be independent from CI platform integrationBenefitsEasier troubleshooting and development of the processEasier migration to a different CI platformEasier experimentationEasier to run as part of the development processDrawbacksNeeds more knowledge and experienceWe use Docker as much as possible to implement the steps in the integration processBenefitsPlatform independenceRepeatabilitySecurityDrawbacks:Needs expertise in Dockerfile and Docker in generalWe only build the code affected by the change but re-tag all unchanged code artefactsBenefitsBe able to release a consistent version of all necessary services. The Docker images for all services in the monorepo should have the same Docker image tag available for a commit hashSupports the monorepo benefits and ideologyDrawbacksCan be tricky, especially for artefacts that are not Docker images(currently we do not plan to have those)We support only Linux as a target operating system when we cannot use DockerBenefitsSame as the OS we use in our production environment, which minimizes the chances of failure because of OS differencesWe minimize the effort and complexity of supporting different operating systemsLinux tooling works on all major other OSs today - macOS and WindowsDrawbacks:Devs that use non-Linux OS might need to install additional software and customizations
BenefitsEasier troubleshooting and development of the processEasier migration to a different CI platformEasier experimentationEasier to run as part of the development processDrawbacksNeeds more knowledge and experience
Easier troubleshooting and development of the processEasier migration to a different CI platformEasier experimentationEasier to run as part of the development process
Needs more knowledge and experience
BenefitsPlatform independenceRepeatabilitySecurityDrawbacks:Needs expertise in Dockerfile and Docker in general
Platform independenceRepeatabilitySecurity
Needs expertise in Dockerfile and Docker in general
BenefitsBe able to release a consistent version of all necessary services. The Docker images for all services in the monorepo should have the same Docker image tag available for a commit hashSupports the monorepo benefits and ideologyDrawbacksCan be tricky, especially for artefacts that are not Docker images(currently we do not plan to have those)
Be able to release a consistent version of all necessary services. The Docker images for all services in the monorepo should have the same Docker image tag available for a commit hashSupports the monorepo benefits and ideology
Can be tricky, especially for artefacts that are not Docker images(currently we do not plan to have those)
BenefitsSame as the OS we use in our production environment, which minimizes the chances of failure because of OS differencesWe minimize the effort and complexity of supporting different operating systemsLinux tooling works on all major other OSs today - macOS and WindowsDrawbacks:Devs that use non-Linux OS might need to install additional software and customizations
Same as the OS we use in our production environment, which minimizes the chances of failure because of OS differencesWe minimize the effort and complexity of supporting different operating systemsLinux tooling works on all major other OSs today - macOS and Windows
Devs that use non-Linux OS might need to install additional software and customizations
CI Platform "
"How do we want to organise work in branches and how should changes be released? How should different branches be continuously deployed for QA?
Decision Drivers
We need to have confidence in our releases.We want more structured releases while we're still getting our footing in a shared monorepo.We need simplicity and clear takt time so different teams can plan for what is going out the door from them.It should work well with our agile work environment.","Chosen option: ""OneFlow"" because it provides a single eternal branch with well structured releases.
We'll implement OneFlow with these details:
Release branches are set up the Monday after each sprint. This is sometimes called release trains, where features line up for different release trains.Release and quality managers from each team are responsible for reviewing and approving releases.Releases apply to all apps in the monorepo.Releases are versioned like this: {cycle}.{sprint}.{hotfix}. So version 3.1.2 is the release after cycle 3, sprint 1 with two hot fixes applied.Feature branches are merged using ""Squash and merge"", so they can be easily reverted.There are two ways to build larger features.If the feature is isolated and not likely to cause conflicts, they can stay on long-living feature branches until they are ready to be released.If the feature touches many parts of the codebase, it can be useful to merge changes more often but hide the feature in production with feature flags.If a project needs to deploy updates outside of the sprint rhythm, they should use hotfix branches.
If the feature is isolated and not likely to cause conflicts, they can stay on long-living feature branches until they are ready to be released.If the feature touches many parts of the codebase, it can be useful to merge changes more often but hide the feature in production with feature flags.
Future strategy
With time, we expect to build up better testing capabilities which gives us more confidence in the health of our monorepo. Then we can move quicker, with a simpler GitHub Flow branching strategy and continuous delivery into production.
Hosting environments
We'll set up continuous delivery to different hosting environments:
EnvironmentGit sourceDatabases/servicesFeaturessandboxfeature branchTestAlldevmainTestAllstagingmainProdAllpre-prodrelease/hotfix branchProdFinishedprodlatest release tagProdFinished
We'll probably start with dev, staging, pre-prod and prod environments, since feature branch deployments are more dynamic and difficult to manage.","Architectural Decision Record

## Context
How do we want to organise work in branches and how should changes be released? How should different branches be continuously deployed for QA?
Decision Drivers
We need to have confidence in our releases.We want more structured releases while we're still getting our footing in a shared monorepo.We need simplicity and clear takt time so different teams can plan for what is going out the door from them.It should work well with our agile work environment.

## Decision
","## Context

How do we want to organise work in branches and how should changes be released? How should different branches be continuously deployed for QA?
Decision Drivers
We need to have confidence in our releases.We want more structured releases while we're still getting our footing in a shared monorepo.We need simplicity and clear takt time so different teams can plan for what is going out the door from them.It should work well with our agile work environment.","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
How do we want to organise work in branches and how should changes be released? How should different branches be continuously deployed for QA?
Decision Drivers
We need to have confidence in our releases.We want more structured releases while we're still getting our footing in a shared monorepo.We need simplicity and clear takt time so different teams can plan for what is going out the door from them.It should work well with our agile work environment."
"Multiple projects need to show data visually using charts and graphs. In order to provide unified look and feel across island.is we should commit to a single approach to implementing charts, i.e. choose one library for the whole repository.
Requirements
The charting library should:
support rendering all standard charts, i.e. bar, line, pie,support custom styling of elements (colors, fonts, tooltips, legends, axis)support lazy/dynamic loading to minimize js bundlesTypescript support
Decision Drivers
Meet all requirements listed aboveAPI qualityPricingBundle sizeTypescript support","Chosen option: ""Recharts"", because it meets all requirements, and overall has a very nice, dev-friendly API. It is the most popular (downloads per week) react charting library on github, and recommended across the community. We can customize how it looks, and start using it quickly without much groundwork.
Positive Consequences
We can start implementing charts and graphs as needed in island.is
Negative Consequences
It is a big dependency, but almost all chart libraries are big due to their nature. We will minimize the impact of this by enforcing charts to be lazy loaded in our codebase.","Architectural Decision Record

## Context
Multiple projects need to show data visually using charts and graphs. In order to provide unified look and feel across island.is we should commit to a single approach to implementing charts, i.e. choose one library for the whole repository.
Requirements
The charting library should:
support rendering all standard charts, i.e. bar, line, pie,support custom styling of elements (colors, fonts, tooltips, legends, axis)support lazy/dynamic loading to minimize js bundlesTypescript support
Decision Drivers
Meet all requirements listed aboveAPI qualityPricingBundle sizeTypescript support

## Decision
","## Context

Multiple projects need to show data visually using charts and graphs. In order to provide unified look and feel across island.is we should commit to a single approach to implementing charts, i.e. choose one library for the whole repository.
Requirements
The charting library should:
support rendering all standard charts, i.e. bar, line, pie,support custom styling of elements (colors, fonts, tooltips, legends, axis)support lazy/dynamic loading to minimize js bundlesTypescript support
Decision Drivers
Meet all requirements listed aboveAPI qualityPricingBundle sizeTypescript support","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
Multiple projects need to show data visually using charts and graphs. In order to provide unified look and feel across island.is we should commit to a single approach to implementing charts, i.e. choose one library for the whole repository.
Requirements
The charting library should:
support rendering all standard charts, i.e. bar, line, pie,support custom styling of elements (colors, fonts, tooltips, legends, axis)support lazy/dynamic loading to minimize js bundlesTypescript support
Decision Drivers
Meet all requirements listed aboveAPI qualityPricingBundle sizeTypescript support"
We want to record architectural decisions made in this project. Which format and structure should these records follow?,"Chosen option: ""MADR 2.1.0"", because
Implicit assumptions should be made explicit.Design documentation is important to enable people understanding the decisions later on.See also A rational design process: How and why to fake it.The MADR format is lean and fits our development style.The MADR structure is comprehensible and facilitates usage & maintenance.The MADR project is vivid.Version 2.1.0 is the latest one available when starting to document ADRs.","Architectural Decision Record

## Context
We want to record architectural decisions made in this project. Which format and structure should these records follow?

## Decision
","## Context

We want to record architectural decisions made in this project. Which format and structure should these records follow?","This is a Architectural Decision Record. Provide a Decision for the Context given below.

## Context
We want to record architectural decisions made in this project. Which format and structure should these records follow?"
