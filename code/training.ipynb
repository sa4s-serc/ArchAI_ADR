{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(model, experiment='0_shot', path='../data/):\n",
    "    if(experiment=='0_shot'):\n",
    "        df = pd.read_csv(os.path.join(path, '0_shot.csv'))\n",
    "        if(model=='gpt3'):\n",
    "            return df['context_input_gpt3'].tolist(), df['decision'].tolist()\n",
    "        elif(model=='chatGPT'):\n",
    "            return df['context_input_chatGPT'].tolist(), df['decision'].tolist()\n",
    "        elif(model=='T5'):\n",
    "            return df['context_input_T5'].tolist(), df['decision'].tolist()\n",
    "        else:\n",
    "            return df['context'].tolist(), df['decision'].tolist()\n",
    "    elif(experiment=='few_shot'):\n",
    "        df = pd.read_csv(os.path.join(path, 'few_shot.csv'))\n",
    "        if(model=='gpt3'):\n",
    "            return df['context_input_gpt3'].tolist(), df['decision'].tolist()\n",
    "        else:\n",
    "            return df['context'].tolist(), df['decision'].tolist()\n",
    "        \n",
    "    else:\n",
    "        df = pd.read_csv(os.path.join(path, '0_shot.csv'))\n",
    "        if(model=='gpt2'):\n",
    "            return df['context_input_gpt3'].tolist(), df['decision'].tolist()\n",
    "        elif(model=='T5'):\n",
    "            return df['context'].tolist(), df['decision'].tolist()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_data(context, decision, length):\n",
    "    context_new, decision_new = [], []\n",
    "    for c,d in zip(context, decision):\n",
    "        if(len(c) < length and len(d) < length):\n",
    "            context_new.append(c)\n",
    "            decision_new.append(d)\n",
    "        \n",
    "    return context_new, decision_new\n",
    "\n",
    "def print_results(predictions, references):\n",
    "    results = rouge.compute(predictions=predictions, references=references)\n",
    "    ic(results)\n",
    "    results = bleu.compute(predictions=predictions, references=references)\n",
    "    ic(results)\n",
    "    results = meteor.compute(predictions=predictions, references=references)\n",
    "    ic(results)\n",
    "    results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "    results = {\n",
    "        'precision': np.average(results['precision']), 'recall': np.average(results['recall']),\n",
    "        'f1': np.average(results['f1']), 'hashcode': results['hashcode']\n",
    "        }\n",
    "    ic(results)\n",
    "\n",
    "def store_output(model_name, context, decision, predicted_decision, experiment='0_shot'):\n",
    "    df = pd.DataFrame(list(zip(context, decision, predicted_decision)), columns =['context', 'decision', 'predicted_decision'])\n",
    "    if(experiment=='0_shot'):\n",
    "        df.to_csv('../output/0_shot/'+model_name+'.csv', index=False)\n",
    "    elif(experiment=='few_shot'):\n",
    "        df.to_csv('../output/few_shot/'+model_name+'.csv', index=False)\n",
    "    else:\n",
    "        df.to_csv('../output/fine_tune/'+model_name+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from icecream import ic\n",
    "import numpy as np\n",
    "import wandb\n",
    "wandb.init(project=\"adr_gpt2\")\n",
    "from transformers.integrations import WandbCallback\n",
    "import logging\n",
    "\n",
    "import evaluate\n",
    "rouge = evaluate.load('rouge')\n",
    "bleu = evaluate.load('bleu')\n",
    "meteor = evaluate.load('meteor')\n",
    "bertscore = evaluate.load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch dataset\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenizer, data):\n",
    "        self.examples = []\n",
    "        for text in data:\n",
    "            input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "            self.examples.append(torch.tensor(input_ids))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| 'tokenizer loaded'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| 'Model loaded'\n",
      "You are adding a <class 'transformers.integrations.WandbCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "WandbCallback\n"
     ]
    }
   ],
   "source": [
    "logging.getLogger(\"transformers\").setLevel(logging.WARNING)\n",
    "\n",
    "# Tokenize the data\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "ic('tokenizer loaded')\n",
    "\n",
    "context, decision = load_data('gpt2', experiment='fine_tune')\n",
    "context, decision = select_data(context, decision, 2000)\n",
    "print(len(context), len(decision))\n",
    "context_train, context_test, decision_train, decision_test = train_test_split(context, decision, test_size=0.20, random_state=42)\n",
    "\n",
    "train_dataset = [context_train[i] + decision_train[i] for i in range(len(context_train))]\n",
    "val_dataset = [context_test[i] + decision_test[i] for i in range(len(context_test))]\n",
    "train_dataset = TextDataset(tokenizer, train_dataset)\n",
    "val_dataset = TextDataset(tokenizer, val_dataset)\n",
    "test_dataset = TextDataset(tokenizer, context_test)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", cache_dir='../cache')\n",
    "ic('Model loaded')\n",
    "\n",
    "# Fine-tune the model on the dataset\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../checkpoints/\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=20, # number of training epochs\n",
    "    report_to=\"wandb\",\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=2, # batch size for training\n",
    "    per_device_eval_batch_size=2,  # batch size for evaluation\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    callbacks=[WandbCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| results: {'rouge1': 0.1550460300490164,\n",
      "              'rouge2': 0.021454479691571493,\n",
      "              'rougeL': 0.08993452079531158,\n",
      "              'rougeLsum': 0.14206922188863258}\n",
      "ic| results: {'bleu': 0.009986268716749973,\n",
      "              'brevity_penalty': 1.0,\n",
      "              'length_ratio': 2.625189681335357,\n",
      "              'precisions': [0.12215799614643545,\n",
      "                             0.018940858136838035,\n",
      "                             0.003683598293912369,\n",
      "                             0.0011668611435239206],\n",
      "              'reference_length': 1977,\n",
      "              'translation_length': 5190}\n",
      "ic| results: {'meteor': 0.17607888354746867}\n",
      "ic| results: {'f1': 0.8146063312888145,\n",
      "              'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.24.0)',\n",
      "              'precision': 0.7966932952404022,\n",
      "              'recall': 0.8341554552316666}\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Config\n",
    "logging.getLogger(\"transformers\").setLevel(logging.WARNING)\n",
    "\n",
    "def evaluate(model_name, context, decision, experiment):\n",
    "    generator = pipeline('text-generation', model='../checkpoints/checkpoint-93', tokenizer='gpt2')\n",
    "    predicted_decision = []\n",
    "    for input_text in context:\n",
    "        output_text = generator(input_text, max_length=500, pad_token_id=tokenizer.eos_token_id)\n",
    "        output_text = output_text[0]['generated_text']\n",
    "        predicted_decision.append(output_text[len(input_text):])\n",
    "\n",
    "    store_output(model_name, context, decision, predicted_decision, experiment)\n",
    "    print_results(predicted_decision, decision)\n",
    "\n",
    "evaluate('gpt2', context_test, decision_test, experiment='fine_tune')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from icecream import ic\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
    "from transformers import TextDataset, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "import wandb\n",
    "wandb.init(project=\"t5_adr\")\n",
    "from transformers.integrations import WandbCallback\n",
    "import logging\n",
    "\n",
    "import evaluate\n",
    "rouge = evaluate.load('rouge')\n",
    "bleu = evaluate.load('bleu')\n",
    "meteor = evaluate.load('meteor')\n",
    "bertscore = evaluate.load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch dataset\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenizer, input_ids, output_ids):\n",
    "        self.input_ids = input_ids['input_ids']\n",
    "        self.attention_mask = input_ids['attention_mask']\n",
    "        self.output_ids = output_ids['input_ids']\n",
    "        self.output_attention_mask = output_ids['attention_mask']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'decoder_input_ids': self.output_ids[idx][:-1],\n",
    "            'decoder_attention_mask': self.output_attention_mask[idx][:-1],\n",
    "            'labels': self.output_ids[idx][1:]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| 'Tokenizing done'\n",
      "ic| len(train_dataset): 73, len(val_dataset): 19\n",
      "ic| 'Model loaded'\n",
      "You are adding a <class 'transformers.integrations.WandbCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "WandbCallback\n"
     ]
    }
   ],
   "source": [
    "logging.getLogger(\"transformers\").setLevel(logging.WARNING)\n",
    "\n",
    "model_name = \"google/flan-t5-small\"\n",
    "model_max_length = 2000\n",
    "\n",
    "# Load the data\n",
    "context, decision = load_data('T5', experiment='fine_tune')\n",
    "context, decision = select_data(context, decision, model_max_length*4)\n",
    "\n",
    "# Tokenize the data\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=model_max_length)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer)\n",
    "ic('Tokenizing done')\n",
    "\n",
    "input_ids = tokenizer.batch_encode_plus(context, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output_ids = tokenizer.batch_encode_plus(decision, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "textDataset = TextDataset(tokenizer, input_ids, output_ids)\n",
    "indices = np.arange(len(context))\n",
    "train_indices, test_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "test_context, test_decision = [context[i] for i in test_indices], [decision[i] for i in test_indices]\n",
    "train_dataset, val_dataset = torch.utils.data.Subset(textDataset, train_indices), torch.utils.data.Subset(textDataset, test_indices)\n",
    "ic(len(train_dataset), len(val_dataset))\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, cache_dir='../cache')\n",
    "ic('Model loaded')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../checkpoints/\" + model_name, #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=20, # number of training epochs\n",
    "    report_to=\"wandb\",\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=2, # batch size for training\n",
    "    per_device_eval_batch_size=2,  # batch size for evaluation\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    callbacks=[WandbCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, model, tokenizer, max_length=2000):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    output = model.generate(input_ids=input_ids, max_length=max_length, do_sample=True)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "def evaluate_t5(model_name, context, decision, experiment, model_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir='../cache', model_max_length=model_max_length)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path, cache_dir='../cache')\n",
    "    \n",
    "    predicted_decision = []\n",
    "    for c in context:\n",
    "        predicted_decision.append(generate_text(c, model, tokenizer))\n",
    "\n",
    "    store_output(model_name, context, decision, predicted_decision, experiment)\n",
    "    print(model_name)\n",
    "    print_results(predicted_decision, decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google/flan-t5-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| results: {'rouge1': 0.18980749443085573,\n",
      "              'rouge2': 0.036371821912997385,\n",
      "              'rougeL': 0.12535303706718598,\n",
      "              'rougeLsum': 0.1684382640814316}\n",
      "ic| results: {'bleu': 0.017895622088931726,\n",
      "              'brevity_penalty': 0.2424245268778062,\n",
      "              'length_ratio': 0.4137249364586275,\n",
      "              'precisions': [0.4232081911262799,\n",
      "                             0.08644536652835408,\n",
      "                             0.035739313244569026,\n",
      "                             0.0227111426543648],\n",
      "              'reference_length': 3541,\n",
      "              'translation_length': 1465}\n",
      "ic| results: {'meteor': 0.13891576128316147}\n",
      "ic| results: {'f1': 0.8311444991513303,\n",
      "              'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.24.0)',\n",
      "              'precision': 0.8418689527009663,\n",
      "              'recall': 0.8217253779110155}\n"
     ]
    }
   ],
   "source": [
    "logging.getLogger(\"transformers\").setLevel(logging.WARNING)\n",
    "model_path = \"../checkpoints/google/flan-t5-small/checkpoint-740/\"\n",
    "evaluate_t5(model_name, test_context, test_decision, 'fine_tune', model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
