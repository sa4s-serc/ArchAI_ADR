{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home2/rudra.dhar/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--bleu/9e0985c1200e367cce45605ce0ecb5ede079894e0f24f54613fca08eeb8aff76 (last modified on Tue Aug 15 17:26:46 2023) since it couldn't be found locally at evaluate-metric--bleu, or remotely on the Hugging Face Hub.\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 104]\n",
      "[nltk_data]     Connection reset by peer>\n",
      "[nltk_data] Error loading punkt: <urlopen error [Errno 104] Connection\n",
      "[nltk_data]     reset by peer>\n",
      "[nltk_data] Error loading omw-1.4: <urlopen error [Errno 104]\n",
      "[nltk_data]     Connection reset by peer>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from icecream import ic\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "import evaluate\n",
    "rouge = evaluate.load('rouge')\n",
    "bleu = evaluate.load('bleu')\n",
    "meteor = evaluate.load('meteor')\n",
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdata():\n",
    "    path = '/home2/rudra.dhar/Data/SE/adr/data_folder/'\n",
    "\n",
    "    directory = path + 'architecture-master'\n",
    "    context, decision = [], []\n",
    "    for filename in os.scandir(directory):\n",
    "        #if filename.path.endswith(\".md\") and filename.is_file():\n",
    "        with open(filename.path, 'r') as f:\n",
    "            content = f.read()\n",
    "            context.append(content[content.find('## Context')+12:content.find('## Decision')-2])\n",
    "            decision.append(content[content.find('## Decision')+13:content.find('## Status')])\n",
    "        \n",
    "\n",
    "    directory = path + 'adr'\n",
    "    #context, decision = [], []\n",
    "    for filename in os.scandir(directory):\n",
    "        #if filename.path.endswith(\".md\") and filename.is_file():\n",
    "        with open(filename.path, 'r') as f:\n",
    "            content = f.read()\n",
    "            context.append(content[2:content.find('## Decision')-2])\n",
    "            if ('## Pros and Cons' in content):\n",
    "                decision.append(content[content.find('## Decision')+21:content.find('## Pros and Cons')])\n",
    "            elif ('## License' in content):\n",
    "                decision.append(content[content.find('## Decision')+21:content.find('## License')])\n",
    "            \n",
    "        \n",
    "\n",
    "    directory = path + 'examples_extracted'\n",
    "    #context, decision = [], []\n",
    "    for filename in os.scandir(directory):\n",
    "        with open(filename.path, 'r') as f:\n",
    "            content = f.read()\n",
    "            context.append(content[12:content.find('## Decision')-2])\n",
    "            decision.append(content[content.find('## Decision')+13:])\n",
    "        \n",
    "\n",
    "    directory = path + 'cardano'\n",
    "    #context, decision = [], []\n",
    "    for filename in os.scandir(directory):\n",
    "        with open(filename.path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            context.append(content[12:content.find('## Decision')-2])\n",
    "            decision.append(content[content.find('## Decision')+17:-2])\n",
    "        \n",
    "\n",
    "    directory = path + 'island'\n",
    "    #context, decision = [], []\n",
    "    for filename in os.scandir(directory):\n",
    "        with open(filename, \"r\", encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            if('\\nConsidered Options' in content):\n",
    "                context.append(content[content.find('Context and Problem Statement')+30:content.find('\\nConsidered Options')])\n",
    "            elif('\\nDecision Drivers' in content):\n",
    "                context.append(content[content.find('Context and Problem Statement')+30:content.find('\\nDecision Drivers')])\n",
    "            else:\n",
    "                context.append(content[content.find('Context and Problem Statement')+30:content.find('\\nDecision Outcome')])\n",
    "\n",
    "            if('\\nPros and Cons of ' in content):\n",
    "                decision.append(content[content.find('Decision Outcome')+17:content.find('\\nPros and Cons of ')])\n",
    "            else:\n",
    "                decision.append(content[content.find('Decision Outcome')+17:])\n",
    "\n",
    "        \n",
    "    context_input_gpt3 = [\"Architectural Decision Record\\n\\n## Context\\n\" +c+ \"\\n\\n## Decision\\n\" for c in context]\n",
    "    context_input_chatGPT = [\"## Context\\n\\n\" +c for c in context]\n",
    "    context_input_T5 = [\"This is a Architectural Decision Record. Provide a Decision for the Context given below.\\n\\n## Context\\n\" +c for c in context]\n",
    "    \n",
    "    return context, decision, context_input_gpt3, context_input_chatGPT, context_input_T5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# gpt_3\n",
    "context, decision, context_input, _ = getdata()\n",
    "\n",
    "random_index = np.random.randint(len(context_input), size=(5))\n",
    "context = [context[i] for i in random_index]\n",
    "decision = [decision[i] for i in random_index]\n",
    "context_input = [context_input[i] for i in random_index]\n",
    "print(len(context_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# gpt_4\n",
    "context, decision, _, context_input = getdata()\n",
    "\n",
    "random_index = np.random.randint(len(context_input), size=(5))\n",
    "context = [context[i] for i in random_index]\n",
    "decision = [decision[i] for i in random_index]\n",
    "context_input = [context_input[i] for i in random_index]\n",
    "print(len(context_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# T5\n",
    "_, decision, _, _, context = getdata()\n",
    "\n",
    "random_index = np.random.randint(len(context), size=(5))\n",
    "context = [context[i] for i in random_index]\n",
    "decision = [decision[i] for i in random_index]\n",
    "print(len(context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_2(context, decision, model):\n",
    "\n",
    "    # Total length of tokens should be less than 1024\n",
    "    # approx less than 512 tokens for context and 512 tokens for decision\n",
    "    # approx 512 tokens = 2000 characters\n",
    "    context_truncated = [c[:2000]+\"\\n\\n## Decision\" for c in context]\n",
    "    decision_reference_truncated = [c[:2000] for c in decision]\n",
    "\n",
    "    generator = pipeline('text-generation', model=model, device = 0)\n",
    "\n",
    "    prediction = generator(context_truncated, max_length=1000, num_return_sequences=1)\n",
    "    prediction = [p[0]['generated_text'] for p in prediction]\n",
    "\n",
    "    predicted_decision = []\n",
    "    for p in range(len(prediction)):\n",
    "        predicted_decision.append(prediction[p][len(context_truncated[p]):])\n",
    "\n",
    "    results = rouge.compute(predictions=predicted_decision, references=decision_reference_truncated)\n",
    "    ic(results)\n",
    "    results = bleu.compute(predictions=predicted_decision, references=decision_reference_truncated)\n",
    "    ic(results)\n",
    "    results = meteor.compute(predictions=predicted_decision, references=decision_reference_truncated)\n",
    "    ic(results)\n",
    "    results = bertscore.compute(predictions=predicted_decision, references=decision_reference_truncated, lang=\"en\")\n",
    "    results = {\n",
    "        'precision': np.average(results['precision']), 'recall': np.average(results['recall']),\n",
    "        'f1': np.average(results['f1']), 'hashcode': results['hashcode']\n",
    "        }\n",
    "    ic(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################# gpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.2827944942381562, 'rouge2': 0.023629997207363966, 'rougeL': 0.13010563380281692, 'rougeLsum': 0.25695689287238577}\n",
      "{'bleu': 0.0, 'precisions': [0.23645833333333333, 0.027139874739039668, 0.0020920502092050207, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.6080402010050252, 'translation_length': 960, 'reference_length': 597}\n",
      "{'meteor': 0.21987588412929074}\n",
      "{'precision': 0.804944634437561, 'recall': 0.80194091796875, 'f1': 0.8033516108989716, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.24.0)'}\n"
     ]
    }
   ],
   "source": [
    "for model in ['gpt2']:\n",
    "    ic('#############################################\\n',model)\n",
    "    gpt_2(context, decision, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Api GPT3-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "path = '/home2/rudra.dhar/codes/SE/adr/'\n",
    "load_dotenv(path + '.env')\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def askGPT3(text, model, max_tokens):    \n",
    "    response = openai.Completion.create(\n",
    "        model=model,\n",
    "        prompt=text,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return response.choices[0].text\n",
    "\n",
    "def gpt_3(context, decision, model, max_tokens):\n",
    "\n",
    "    # Total length of tokens should be less than 2048 for ada, 4096 for text-davinci-003\n",
    "    # approx half for context and half for decision\n",
    "    # approx 1000 tokens = 4000 characters\n",
    "\n",
    "    max_char = max_tokens*4\n",
    "    context_truncated = [c[:max_char] for c in context]\n",
    "    decision_reference_truncated = [c[:max_char] for c in decision]\n",
    "\n",
    "    predicted_decision = []\n",
    "    for c in context_truncated:\n",
    "        predicted_decision.append(askGPT3(c, model, max_tokens))\n",
    "    \n",
    "    for i in range(len(predicted_decision)):\n",
    "        if('## Decision' in predicted_decision[i]):\n",
    "            predicted_decision[i] = predicted_decision[i][predicted_decision[i].find('## Decision')+12:]\n",
    "            predicted_decision[i] = predicted_decision[i][:predicted_decision[i].find('\\n## ')]\n",
    "\n",
    "    results = rouge.compute(predictions=predicted_decision, references=decision_reference_truncated)\n",
    "    ic(results)\n",
    "    results = bleu.compute(predictions=predicted_decision, references=decision_reference_truncated)\n",
    "    ic(results)\n",
    "    results = meteor.compute(predictions=predicted_decision, references=decision_reference_truncated)\n",
    "    ic(results)\n",
    "    results = bertscore.compute(predictions=predicted_decision, references=decision_reference_truncated, lang=\"en\")\n",
    "    results = {\n",
    "        'precision': np.average(results['precision']), 'recall': np.average(results['recall']),\n",
    "        'f1': np.average(results['f1']), 'hashcode': results['hashcode']\n",
    "        }\n",
    "    ic(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| '#############################################\\n': '#############################################\n",
      "                                                        '\n",
      "    model: 'ada'\n",
      "    max_tokens: 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| results: {'rouge1': 0.2084249540892277,\n",
      "              'rouge2': 0.023471263998767988,\n",
      "              'rougeL': 0.10763358254162425,\n",
      "              'rougeLsum': 0.16692793878168832}\n",
      "ic| results: {'bleu': 0.0,\n",
      "              'brevity_penalty': 1.0,\n",
      "              'length_ratio': 2.019582245430809,\n",
      "              'precisions': [0.17711700064641242,\n",
      "                             0.02529182879377432,\n",
      "                             0.001951854261548471,\n",
      "                             0.0],\n",
      "              'reference_length': 766,\n",
      "              'translation_length': 1547}\n",
      "ic| results: {'meteor': 0.19089581761534208}\n",
      "ic| results: {'f1': 0.8133254528045655,\n",
      "              'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.24.0)',\n",
      "              'precision': 0.8014590024948121,\n",
      "              'recall': 0.8261906743049622}\n",
      "ic| '#############################################\\n': '#############################################\n",
      "                                                        '\n",
      "    model: 'text-davinci-003'\n",
      "    max_tokens: 2000\n",
      "ic| results: {'rouge1': 0.3209721030962269,\n",
      "              'rouge2': 0.1074677191604261,\n",
      "              'rougeL': 0.184654740552106,\n",
      "              'rougeLsum': 0.23664392055341757}\n",
      "ic| results: {'bleu': 0.0643009516571181,\n",
      "              'brevity_penalty': 0.7933337672593627,\n",
      "              'length_ratio': 0.8120104438642297,\n",
      "              'precisions': [0.3279742765273312,\n",
      "                             0.1053484602917342,\n",
      "                             0.04738562091503268,\n",
      "                             0.026359143327841845],\n",
      "              'reference_length': 766,\n",
      "              'translation_length': 622}\n",
      "ic| results: {'meteor': 0.251564816411767}\n",
      "ic| results: {'f1': 0.8519887089729309,\n",
      "              'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.24.0)',\n",
      "              'precision': 0.8588044047355652,\n",
      "              'recall': 0.8477995157241821}\n"
     ]
    }
   ],
   "source": [
    "context, decision, context_input, _ = getdata()\n",
    "random_index = np.random.randint(len(context_input), size=(5))\n",
    "context = [context[i] for i in random_index]\n",
    "decision = [decision[i] for i in random_index]\n",
    "context_input = [context_input[i] for i in random_index]\n",
    "print(len(context_input))\n",
    "\n",
    "for m in [('ada', 1000), ('text-davinci-003', 2000)]:\n",
    "    model, max_tokens = m\n",
    "    ic('#############################################\\n',model)\n",
    "    gpt_3(context_input, decision, model, max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_chatGPT(text, model, max_tokens):    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"This is an Architectural Decision Record. Give a ## Decision corresponding to the ## Context provided by the user\"\n",
    "            },\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": text\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def chatGPT(context, decision, model, max_tokens):\n",
    "    max_char = max_tokens*4\n",
    "    context_truncated = [c[:max_char] for c in context]\n",
    "    decision_reference_truncated = [c[:max_char] for c in decision]\n",
    "\n",
    "    predicted_decision = []\n",
    "    for c in context_truncated:\n",
    "        if(model == 'gpt-4'):\n",
    "            time.sleep(30)\n",
    "        predicted_decision.append(ask_chatGPT(c, model, max_tokens))\n",
    "\n",
    "    for i in range(len(predicted_decision)):\n",
    "        if('## Decision' in predicted_decision[i]):\n",
    "            predicted_decision[i] = predicted_decision[i][predicted_decision[i].find('## Decision')+12:]\n",
    "            predicted_decision[i] = predicted_decision[i][:predicted_decision[i].find('\\n## ')]\n",
    "\n",
    "    \n",
    "    results = rouge.compute(predictions=predicted_decision, references=decision_reference_truncated)\n",
    "    ic(results)\n",
    "    results = bleu.compute(predictions=predicted_decision, references=decision_reference_truncated)\n",
    "    ic(results)\n",
    "    results = meteor.compute(predictions=predicted_decision, references=decision_reference_truncated)\n",
    "    ic(results)\n",
    "    results = bertscore.compute(predictions=predicted_decision, references=decision_reference_truncated, lang=\"en\")\n",
    "    results = {\n",
    "        'precision': np.average(results['precision']), 'recall': np.average(results['recall']),\n",
    "        'f1': np.average(results['f1']), 'hashcode': results['hashcode']\n",
    "        }\n",
    "    ic(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "#############################################\n",
      " gpt-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| results: {'rouge1': 0.2361695019093232,\n",
      "              'rouge2': 0.08940728444487843,\n",
      "              'rougeL': 0.17369755919904878,\n",
      "              'rougeLsum': 0.18926912390268893}\n",
      "ic| results: {'bleu': 0.04269381380644307,\n",
      "              'brevity_penalty': 1.0,\n",
      "              'length_ratio': 1.6458333333333333,\n",
      "              'precisions': [0.19746835443037974,\n",
      "                             0.041025641025641026,\n",
      "                             0.025974025974025976,\n",
      "                             0.015789473684210527],\n",
      "              'reference_length': 240,\n",
      "              'translation_length': 395}\n",
      "ic| results: {'meteor': 0.29025597520096386}\n",
      "ic| results: {'f1': 0.862637209892273,\n",
      "              'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.24.0)',\n",
      "              'precision': 0.8516870021820069,\n",
      "              'recall': 0.8746695756912232}\n"
     ]
    }
   ],
   "source": [
    "context, decision, _, context_input = getdata()\n",
    "random_index = np.random.randint(len(context_input), size=(5))\n",
    "context = [context[i] for i in random_index]\n",
    "decision = [decision[i] for i in random_index]\n",
    "context_input = [context_input[i] for i in random_index]\n",
    "print(len(context_input))\n",
    "\n",
    "for m in [('gpt-3.5-turbo', 2000), ('gpt-4', 4000)]:\n",
    "    model, max_tokens = m\n",
    "    print('#############################################\\n',model)\n",
    "    chatGPT(context_input, decision, model, max_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_t5(context, decision, model_name):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name, cache_dir='/scratch/rudra.dhar/cache', model_max_length=4096)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name, cache_dir='/scratch/rudra.dhar/cache', device_map=\"balanced\")\n",
    "\n",
    "    predicted_decision = []\n",
    "    \n",
    "    for c in context:\n",
    "        input_ids = tokenizer(c, return_tensors=\"pt\").input_ids.cuda()\n",
    "        #print(len(input_ids[0]))\n",
    "        outputs = model.generate(input_ids, max_length=len(input_ids[0])*4, min_length= int(len(input_ids[0])/8))\n",
    "        predicted_decision.append(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "    results = rouge.compute(predictions=predicted_decision, references=decision)\n",
    "    ic(results)\n",
    "    results = bleu.compute(predictions=predicted_decision, references=decision)\n",
    "    ic(results)\n",
    "    results = meteor.compute(predictions=predicted_decision, references=decision)\n",
    "    ic(results)\n",
    "    results = bertscore.compute(predictions=predicted_decision, references=decision, lang=\"en\")\n",
    "    results = {\n",
    "        'precision': np.average(results['precision']), 'recall': np.average(results['recall']),\n",
    "        'f1': np.average(results['f1']), 'hashcode': results['hashcode']\n",
    "        }\n",
    "    ic(results)\n",
    "\n",
    "    model_name = model_name.replace('/', '_')\n",
    "    df = pd.DataFrame(list(zip(context, decision, predicted_decision)), columns =['context', 'decision', 'predicted_decision'])\n",
    "    df.to_csv('/home2/rudra.dhar/Data/SE/adr/output/'+model_name+'.csv', index=False)\n",
    "    del tokenizer, model, input_ids, outputs, predicted_decision, df\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| '#############################################\\n': '#############################################\n",
      "                                                        '\n",
      "    model: 'google/flan-t5-xl'\n",
      "ic| results: {'rouge1': 0.20977577029170852,\n",
      "              'rouge2': 0.1359309406440554,\n",
      "              'rougeL': 0.2050271669397532,\n",
      "              'rougeLsum': 0.2092171110738314}\n",
      "ic| results: {'bleu': 0.0012491881646322443,\n",
      "              'brevity_penalty': 0.043664418982504526,\n",
      "              'length_ratio': 0.2420591456736035,\n",
      "              'precisions': [0.17647058823529413,\n",
      "                             0.038901601830663615,\n",
      "                             0.013888888888888888,\n",
      "                             0.00702576112412178],\n",
      "              'reference_length': 1826,\n",
      "              'translation_length': 442}\n",
      "ic| results: {'meteor': 0.16364070273487932}\n",
      "ic| results: {'f1': 0.8253636837005616,\n",
      "              'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.24.0)',\n",
      "              'precision': 0.8273008108139038,\n",
      "              'recall': 0.8256754159927369}\n"
     ]
    }
   ],
   "source": [
    "#_, decision, _, _, context = getdata()\n",
    "t5_models = ['t5-small', 't5-base', 't5-large', 't5-3b', 'bigscience/T0_3B',\n",
    "             'google/flan-t5-small','google/flan-t5-base', 'google/flan-t5-large', 'google/flan-t5-xl']\n",
    "t5_models = ['google/flan-t5-base', 'google/flan-t5-large', 'google/flan-t5-xl']\n",
    "for model in t5_models:\n",
    "    ic('#############################################\\n',model)\n",
    "    ask_t5(context, decision, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f769965d9b0c4cf598ccb9e63772807c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8566439738e842d6b902b04216a4cc40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c462fa3f22124467a0a08348c5426bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632022bfecd54764b4c02870aa841fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf863c02dd00406b8b244ae7b5be889a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2', device_map=\"auto\", model_kwargs={\"cache_dir\": \"/scratch/rudra.dhar/cache/\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    model_kwargs={\"cache_dir\": \"/scratch/rudra.dhar/cache/\"},\n",
    ")\n",
    "\n",
    "sequences = pipeline(\n",
    "    'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
