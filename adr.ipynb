{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from icecream import ic\n",
    "import evaluate\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = '/home2/rudra.dhar/Data/SE/adr/data_folder/'\n",
    "\n",
    "directory = path + 'architecture-master'\n",
    "context, decision = [], []\n",
    "for filename in os.scandir(directory):\n",
    "    #if filename.path.endswith(\".md\") and filename.is_file():\n",
    "    with open(filename.path, 'r') as f:\n",
    "        content = f.read()\n",
    "        context.append(content[content.find('## Context')+12:content.find('## Decision')-2])\n",
    "        decision.append(content[content.find('## Decision')+13:content.find('## Status')])\n",
    "    \n",
    "\n",
    "directory = path + 'adr'\n",
    "#context, decision = [], []\n",
    "for filename in os.scandir(directory):\n",
    "    #if filename.path.endswith(\".md\") and filename.is_file():\n",
    "    with open(filename.path, 'r') as f:\n",
    "        content = f.read()\n",
    "        context.append(content[2:content.find('## Decision')-2])\n",
    "        if ('## Pros and Cons' in content):\n",
    "            decision.append(content[content.find('## Decision')+21:content.find('## Pros and Cons')])\n",
    "        elif ('## License' in content):\n",
    "            decision.append(content[content.find('## Decision')+21:content.find('## License')])\n",
    "        \n",
    "    \n",
    "\n",
    "directory = path + 'examples_extracted'\n",
    "#context, decision = [], []\n",
    "for filename in os.scandir(directory):\n",
    "    with open(filename.path, 'r') as f:\n",
    "        content = f.read()\n",
    "        context.append(content[12:content.find('## Decision')-2])\n",
    "        decision.append(content[content.find('## Decision')+13:])\n",
    "    \n",
    "\n",
    "directory = path + 'cardano'\n",
    "#context, decision = [], []\n",
    "for filename in os.scandir(directory):\n",
    "    with open(filename.path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        context.append(content[12:content.find('## Decision')-2])\n",
    "        decision.append(content[content.find('## Decision')+17:-2])\n",
    "    \n",
    "\n",
    "directory = path + 'island'\n",
    "#context, decision = [], []\n",
    "for filename in os.scandir(directory):\n",
    "    with open(filename, \"r\", encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        if('\\nConsidered Options' in content):\n",
    "            context.append(content[content.find('Context and Problem Statement')+30:content.find('\\nConsidered Options')])\n",
    "        elif('\\nDecision Drivers' in content):\n",
    "            context.append(content[content.find('Context and Problem Statement')+30:content.find('\\nDecision Drivers')])\n",
    "        else:\n",
    "            context.append(content[content.find('Context and Problem Statement')+30:content.find('\\nDecision Outcome')])\n",
    "\n",
    "        if('\\nPros and Cons of ' in content):\n",
    "            decision.append(content[content.find('Decision Outcome')+17:content.find('\\nPros and Cons of ')])\n",
    "        else:\n",
    "            decision.append(content[content.find('Decision Outcome')+17:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(decision)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (819 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460 819\n",
      "2577 260\n",
      "613 93\n",
      "353 551\n",
      "156 306\n",
      "769 3078\n",
      "520 402\n",
      "593 1835\n",
      "242 129\n",
      "277 298\n",
      "572 276\n",
      "358 99\n",
      "1057 316\n",
      "501 274\n",
      "245 382\n",
      "728 509\n",
      "386 384\n",
      "110 67\n",
      "188 22\n",
      "93 12\n",
      "33 17\n",
      "97 269\n",
      "184 49\n",
      "47 404\n",
      "90 343\n",
      "257 108\n",
      "77 91\n",
      "31 9\n",
      "259 44\n",
      "134 75\n",
      "243 68\n",
      "200 38\n",
      "37 119\n",
      "56 37\n",
      "94 19\n",
      "54 34\n",
      "69 82\n",
      "86 15\n",
      "84 249\n",
      "83 18\n",
      "79 452\n",
      "70 157\n",
      "64 16\n",
      "30 15\n",
      "69 30\n",
      "44 52\n",
      "57 37\n",
      "157 50\n",
      "89 8\n",
      "77 252\n",
      "39 38\n",
      "89 5\n",
      "47 17\n",
      "39 21\n",
      "176 115\n",
      "65 22\n",
      "203 78\n",
      "195 20\n",
      "97 12\n",
      "77 13\n",
      "89 216\n",
      "60 14\n",
      "268 55\n",
      "42 15\n",
      "258 169\n",
      "72 23\n",
      "126 293\n",
      "1052 287\n",
      "138 51\n",
      "355 496\n",
      "445 277\n",
      "73 188\n",
      "91 197\n",
      "125 116\n",
      "127 151\n",
      "178 720\n",
      "778 166\n",
      "261 271\n",
      "330 263\n",
      "163 132\n",
      "88 83\n",
      "108 33\n",
      "290 100\n",
      "86 160\n",
      "74 102\n",
      "117 341\n",
      "26 44\n",
      "81 265\n",
      "183 75\n",
      "79 98\n",
      "1043 1100\n",
      "327 121\n",
      "91 444\n",
      "128 129\n",
      "20 100\n"
     ]
    }
   ],
   "source": [
    "# To check the length of the context and decisions\n",
    "\n",
    "from transformers import OpenAIGPTTokenizer\n",
    "\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
    "\n",
    "for i,j in zip(context, decision):\n",
    "    input = tokenizer(i, return_tensors=\"pt\")\n",
    "    output = tokenizer(j, return_tensors=\"pt\")\n",
    "    print(len(input['input_ids'][0]), len(output['input_ids'][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total length of tokens should be less than 1024\n",
    "# approx less than 512 tokens for context and 512 tokens for decision\n",
    "# approx 512 tokens = 2000 characters\n",
    "\n",
    "context_truncated = [c[:2000]+\"\\n\\n## Decision\" for c in context]\n",
    "decision_truncated = [c[:2000] for c in decision]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2', device = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "prediction = generator(context_truncated, max_length=1000, num_return_sequences=1)\n",
    "prediction = [p[0]['generated_text'] for p in prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_decision = []\n",
    "for p in range(len(prediction)):\n",
    "    predicted_decision.append(prediction[p][len(context_truncated[p]):])\n",
    "    #print(len(context_1[p]), len(predicted_decision[p]), len(predicted_decision_1[p]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Relating to Bootstrapping\n",
      "\n",
      "All steps are involved in a decision that is made in consultation with a project manager. When the decision is made as one of two ways, the first is to use the default bootstrapping behavior. In other words, it is the policy which allows the bootstrapping of all tasks, and the second if you have the preferences to stop bootstrapping any task at all. Bootstrapping of all other tasks at runtime, even when there are no dependencies at all, means the bootstrapping operation must occur under certain circumstances. This would mean that in bootstrapping, it must occur after the service is started since it must continue to run regardless of resource usage. This could provide a means of reducing network congestion and reducing system overhead - but also of setting up system resources to be run in a consistent way. In this case, you could try to bootstrap all programs using a default bootstrapping behavior like this.\n",
      "\n",
      "# Bootstrapping any task using: - Initialisation of all systems\n",
      "\n",
      "- Setting up system resources to be run in a consistent way\n",
      "\n",
      "- Starting/destroying services on a service\n",
      "\n",
      "This is typically the more useful approach, because the approach usually involves an initial bootstrapping procedure. The goal is to get as close as possible to bootstrapping any task at the same time that a system can be launched. It may be worthwhile starting multiple programs at once, but this is generally not enough given a choice between running the program multiple times and running several programs at the same time. For this reason, a bootstrapping procedure is generally preferred over more complicated ones, but the only way to get the desired effect is to create different jobs at the same time.\n",
      "\n",
      "In some scenarios, this approach might allow various service-level services to be bootstrapped at the same time, but then stop with only the most specific tasks being running simultaneously. For such scenarios, a custom bootstrapping procedure would be fine, but for other circumstances (eg, as a bootstrapping procedure for services that require a reboot) this might not be the best idea. A different approach may, however, be used if a service is bootstrapped by itself.\n",
      "\n",
      "Another good thing about this approach is that it avoids some pitfalls:\n",
      "\n",
      "- Creating bootstrapping services that depend on a network service\n",
      "\n",
      "- Using a network service to get a service started at boot-time\n",
      "\n",
      "This approach may seem like a great way to avoid bootstrapping other services, but it is not necessarily the best (especially for tasks that require a reboot\n"
     ]
    }
   ],
   "source": [
    "# print(decision_truncated[0])\n",
    "print(predicted_decision[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'r': 0.25806451612903225, 'p': 0.2, 'f': 0.22535210775639763},\n",
       "  'rouge-2': {'r': 0.0299625468164794,\n",
       "   'p': 0.0215633423180593,\n",
       "   'f': 0.02507836503881739},\n",
       "  'rouge-l': {'r': 0.22580645161290322, 'p': 0.175, 'f': 0.19718309367189063}}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "rouge = Rouge()\n",
    "\n",
    "rouge.get_scores(predicted_decision[0], decision_truncated[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.13891654785954738, 'rouge2': 0.021301803452798473, 'rougeL': 0.07533475743134622, 'rougeLsum': 0.12505483422768704}\n"
     ]
    }
   ],
   "source": [
    "## GPT2 \n",
    "results = rouge.compute(predictions=predicted_decision, references=decision_truncated)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.2748026795136888, 'rouge2': 0.0371589378968784, 'rougeL': 0.1232171640791763, 'rougeLsum': 0.25363069692091345}\n"
     ]
    }
   ],
   "source": [
    "## GPT2-medium\n",
    "results = rouge.compute(predictions=predicted_decision, references=decision_truncated)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.27172758184525314, 'rouge2': 0.03633596038596617, 'rougeL': 0.12200892729184516, 'rougeLsum': 0.24817171875833421}\n"
     ]
    }
   ],
   "source": [
    "## GPT2-large\n",
    "results = rouge.compute(predictions=predicted_decision, references=decision_truncated)\n",
    "print(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gpt3 OpenAi Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gpt-4-0613', 'gpt-4-0314', 'text-davinci-001', 'text-search-curie-query-001', 'davinci', 'gpt-3.5-turbo-16k-0613', 'text-babbage-001', 'curie-instruct-beta', 'text-davinci-003', 'davinci-similarity', 'code-davinci-edit-001', 'text-similarity-curie-001', 'text-embedding-ada-002', 'ada-code-search-text', 'text-search-ada-query-001', 'babbage-search-query', 'ada-similarity', 'gpt-3.5-turbo', 'text-search-ada-doc-001', 'text-search-babbage-query-001', 'code-search-ada-code-001', 'curie-search-document', 'text-search-davinci-query-001', 'text-search-curie-doc-001', 'gpt-3.5-turbo-0301', 'babbage-search-document', 'gpt-4', 'babbage-code-search-text', 'whisper-1', 'gpt-3.5-turbo-16k', 'davinci-instruct-beta', 'davinci-search-query', 'text-similarity-babbage-001', 'text-davinci-002', 'code-search-babbage-text-001', 'babbage', 'text-search-davinci-doc-001', 'code-search-ada-text-001', 'ada-search-query', 'text-similarity-ada-001', 'ada-code-search-code', 'ada', 'text-davinci-edit-001', 'davinci-search-document', 'curie-search-query', 'babbage-similarity', 'ada-search-document', 'text-ada-001', 'text-similarity-davinci-001', 'curie', 'curie-similarity', 'gpt-3.5-turbo-0613', 'babbage-code-search-code', 'code-search-babbage-code-001', 'text-search-babbage-doc-001', 'text-curie-001']\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "openai.api_key = \"OpenAi_API_Key\"\n",
    "\n",
    "models = [m['id'] for m in openai.Model.list()['data']]\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2577 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from transformers import OpenAIGPTTokenizer\n",
    "\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
    "\n",
    "context_, decision_ = [], []\n",
    "for i in range(len(context)):\n",
    "    input = tokenizer(context[i], return_tensors=\"pt\")\n",
    "    context_len = len(input['input_ids'][0])\n",
    "    if context_len < 1024:  #1024 for GPT3, 2048 for GPT3.5, 4096 for GPT4\n",
    "        #context_prompt = context[i] + \"\\n\\n## Decision\\n\\n\"\n",
    "        context_prompt = context[i]\n",
    "        context_.append(context_prompt)\n",
    "        decision_.append(decision[i])\n",
    "    \n",
    "context, decision = context_, decision_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def askGPT(text):\n",
    "    #openai.api_key = \"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\"\n",
    "    \n",
    "    # response = openai.Completion.create(\n",
    "    #     #engine=\"babbage\",\n",
    "    #     model=\"babbage\",\n",
    "    #     prompt=text,\n",
    "    #     max_tokens=1024,\n",
    "    # )\n",
    "    # return response.choices[0].text\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages = [{\"role\":\"system\", \"content\":\"Give Architecture Decisions for the given Context\"}, {\"role\":\"user\", \"content\":text}],\n",
    "        max_tokens=2048,\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_decision = []\n",
    "for c in context[:5]:\n",
    "    predicted_decision.append(askGPT(c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| predicted_decision[3]: ('Given the context:\n",
      "                           '\n",
      "                            '\n",
      "                           '\n",
      "                            '1. Documented Architectural Decisions: All architectural decisions should be '\n",
      "                            'documented in detail and readily accessible. This includes the thought '\n",
      "                            'process, alternatives considered, reasons for selecting the chosen approach, '\n",
      "                            'and potential impacts. Documentation promotes transparency and '\n",
      "                            'understanding.\n",
      "                           '\n",
      "                            '\n",
      "                           '\n",
      "                            '2. Collaborative Decision Making: Instead of a single person or team making '\n",
      "                            'important decisions, a collaborative framework should be put in place. '\n",
      "                            'Technologies and processes that promote collaborative decision-making should '\n",
      "                            'be adopted.\n",
      "                           '\n",
      "                            '\n",
      "                           '\n",
      "                            '3. Regular Review Meetings: To ensure that everyone has an understanding of '\n",
      "                            'the decisions made and their rationale, regular review meetings should be '\n",
      "                            'organized. These meetings can be attended by project members from different '\n",
      "                            'locations and time zones through virtual platforms.\n",
      "                           '\n",
      "                            '\n",
      "                           '\n",
      "                            '4. Decision Review Procedure: A formal procedure should be put in place for '\n",
      "                            'reviewing past decisions. This procedure should account for changes to '\n",
      "                            'circumstances or conditions. \n",
      "                           '\n",
      "                            '\n",
      "                           '\n",
      "                            '5. Architectural Steering Group: An architectural steering group should be '\n",
      "                            'established to review major decisions. This group can help ensure that '\n",
      "                            'decisions align with strategic goals and to provide independent viewpoints.\n",
      "                           '\n",
      "                            '\n",
      "                           '\n",
      "                            '6. Diversity in Team Composition: In order to consider various perspectives '\n",
      "                            'and possibilities, the architecture team should be diverse. Having members '\n",
      "                            'from varied background and experiences can enrich the decision-making '\n",
      "                            'process.\n",
      "                           '\n",
      "                            '\n",
      "                           '\n",
      "                            '7. Training and Knowledge Sharing: Training sessions and knowledge sharing '\n",
      "                            'should be conducted regularly to ensure a strong shared understanding of '\n",
      "                            'technical aspects among all contributors.\n",
      "                           '\n",
      "                            '\n",
      "                           '\n",
      "                            '8. Use of Architectural Design Patterns: To ensure sound and consistent '\n",
      "                            'architecture decisions, the use of proven architectural design patterns '\n",
      "                            'should be encouraged. \n",
      "                           '\n",
      "                            '\n",
      "                           '\n",
      "                            '9. Adoption of a Framework for Architectural Decision Making: A '\n",
      "                            'well-established architectural decision-making framework should be adopted '\n",
      "                            'to guide the process and ensure rigor. This promotes consistency and '\n",
      "                            'quality.\n",
      "                           '\n",
      "                            '\n",
      "                           '\n",
      "                            '10. Tools for Decision Tracking: Architectural decision management tools '\n",
      "                            'should be used to record, track, and revisit decisions. This facilitates '\n",
      "                            'transparency, traceability and continuous learning.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Given the context:\\n\\n1. Documented Architectural Decisions: All architectural decisions should be documented in detail and readily accessible. This includes the thought process, alternatives considered, reasons for selecting the chosen approach, and potential impacts. Documentation promotes transparency and understanding.\\n\\n2. Collaborative Decision Making: Instead of a single person or team making important decisions, a collaborative framework should be put in place. Technologies and processes that promote collaborative decision-making should be adopted.\\n\\n3. Regular Review Meetings: To ensure that everyone has an understanding of the decisions made and their rationale, regular review meetings should be organized. These meetings can be attended by project members from different locations and time zones through virtual platforms.\\n\\n4. Decision Review Procedure: A formal procedure should be put in place for reviewing past decisions. This procedure should account for changes to circumstances or conditions. \\n\\n5. Architectural Steering Group: An architectural steering group should be established to review major decisions. This group can help ensure that decisions align with strategic goals and to provide independent viewpoints.\\n\\n6. Diversity in Team Composition: In order to consider various perspectives and possibilities, the architecture team should be diverse. Having members from varied background and experiences can enrich the decision-making process.\\n\\n7. Training and Knowledge Sharing: Training sessions and knowledge sharing should be conducted regularly to ensure a strong shared understanding of technical aspects among all contributors.\\n\\n8. Use of Architectural Design Patterns: To ensure sound and consistent architecture decisions, the use of proven architectural design patterns should be encouraged. \\n\\n9. Adoption of a Framework for Architectural Decision Making: A well-established architectural decision-making framework should be adopted to guide the process and ensure rigor. This promotes consistency and quality.\\n\\n10. Tools for Decision Tracking: Architectural decision management tools should be used to record, track, and revisit decisions. This facilitates transparency, traceability and continuous learning.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic(predicted_decision[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.07264237727831882, 'rouge2': 0.00802396970911307, 'rougeL': 0.031300044226004874, 'rougeLsum': 0.06670202103639192}\n"
     ]
    }
   ],
   "source": [
    "## GPT3- ada\n",
    "results = rouge.compute(predictions=predicted_decision, references=decision)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.18085945119736505, 'rouge2': 0.016952121090620678, 'rougeL': 0.08149779727878756, 'rougeLsum': 0.1595116904880347}\n"
     ]
    }
   ],
   "source": [
    "## GPT3- babbage\n",
    "results = rouge.compute(predictions=predicted_decision, references=decision)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.24216926658474966, 'rouge2': 0.027831038938636483, 'rougeL': 0.10746614757775286, 'rougeLsum': 0.22088300897784402}\n"
     ]
    }
   ],
   "source": [
    "## GPT3- curie\n",
    "results = rouge.compute(predictions=predicted_decision, references=decision)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.1401032972575228, 'rouge2': 0.021487973101327638, 'rougeL': 0.06386788679443815, 'rougeLsum': 0.13315541850659562}\n"
     ]
    }
   ],
   "source": [
    "## GPT3- davinci\n",
    "results = rouge.compute(predictions=predicted_decision, references=decision)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.2486760871535559, 'rouge2': 0.03920972752106796, 'rougeL': 0.12644511199081915, 'rougeLsum': 0.2277128884357124}\n"
     ]
    }
   ],
   "source": [
    "## GPT3.5 - text-davinci-003\n",
    "results = rouge.compute(predictions=predicted_decision, references=decision)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.3009730775431527, 'rouge2': 0.05438354616037107, 'rougeL': 0.14078610491409285, 'rougeLsum': 0.2703467940852909}\n"
     ]
    }
   ],
   "source": [
    "## GPT3.5 - gpt-3.5-turbo # Without using System role\n",
    "results = rouge.compute(predictions=predicted_decision, references=decision)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.2831935181562054, 'rouge2': 0.042458198342067696, 'rougeL': 0.13136335188127132, 'rougeLsum': 0.2571209576031062}\n"
     ]
    }
   ],
   "source": [
    "## GPT3.5 - gpt-3.5-turbo # With using System role\n",
    "results = rouge.compute(predictions=predicted_decision, references=decision)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.22975033633747075, 'rouge2': 0.04346140459643355, 'rougeL': 0.12162401675650944, 'rougeLsum': 0.2114825864613532}\n"
     ]
    }
   ],
   "source": [
    "## GPT3.5 - gpt-3.5-turbo # With using System role all_data\n",
    "results = rouge.compute(predictions=predicted_decision, references=decision)\n",
    "print(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75839780b4154801b6e181130befa68d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762d2a2aae184cadb036c971a39faabc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/rudra.dhar/miniconda3/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-3b automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd7ea457a0543f1959ee6975033202c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/11.4G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-3b\", cache_dir='/scratch/rudra.dhar/cache')\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-3b\", cache_dir='/scratch/rudra.dhar/cache')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T5 uses relative positional embeddings, hence no need for context truncation. \n",
    "https://github.com/huggingface/transformers/issues/16986"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2732 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "task_postfix = \"\\n\\n## Decision\\n\\n\"\n",
    "#task_prefix = \"Generate Architecture Decisions for the given Context\\n\\n\"\n",
    "predicted_decision = []\n",
    "\n",
    "for i in range(len(context)):\n",
    "    input_ids = tokenizer(context[i]+task_postfix, return_tensors=\"pt\").input_ids\n",
    "    #print(len(input_ids[0]))\n",
    "    outputs = model.generate(input_ids, max_length=2048)\n",
    "    predicted_decision.append(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| predicted_decision[3]: ('s        s ## Decisions ## Context ## Decisions ## Context ## Context ## '\n",
      "                            'Context ## Context ## Context ## Context ## Context ## Context ## Context ## '\n",
      "                            'Context......s ## Decision s.s..ssss ##')\n",
      "ic| context[3]+task_postfix: ('## Context\n",
      "                             '\n",
      "                              '\n",
      "                             '\n",
      "                              \"Historically, error handling has not been Clojure's strong suit. For the \"\n",
      "                              'most part, errors take the form of a JVM exception, with a long stack trace '\n",
      "                              \"that includes a lot of Clojure's implementation as well as stack frames that \"\n",
      "                              'pertain directly to user code.\n",
      "                             '\n",
      "                              '\n",
      "                             '\n",
      "                              'Additionally, prior to the advent of `clojure.spec`, Clojure errors were '\n",
      "                              'often \"deep\": a very generic error (like a NullPointerException) would be '\n",
      "                              'thrown from far within a branch, rather than eagerly validating inputs.\n",
      "                             '\n",
      "                              '\n",
      "                             '\n",
      "                              'There are Clojure libraries which make an attempt to improve the situation, '\n",
      "                              \"but they typically do it by overriding Clojure's default exception printing \"\n",
      "                              'functions across the board, and are sometimes \"lossy\", dropping information '\n",
      "                              'that could be desirable to a developer.\n",
      "                             '\n",
      "                              '\n",
      "                             '\n",
      "                              'Spec provides an opportunity to improve the situation across the board, and '\n",
      "                              'with Arachne we want to be on the leading edge of providing helpful error '\n",
      "                              'messages that point straight to the problem, minimize time spent trying to '\n",
      "                              \"figure out what's going on, and let developers get straight back to working \"\n",
      "                              'on what matters to them.\n",
      "                             '\n",
      "                              '\n",
      "                             '\n",
      "                              \"Ideally, Arachne's error handling should exhibit the following qualities:\n",
      "                             \"\n",
      "                              '\n",
      "                             '\n",
      "                              '- Never hide possibly relevant information.\n",
      "                             '\n",
      "                              '- Allow module developers to be as helpful as possible to people using their '\n",
      "                              'tools.\n",
      "                             '\n",
      "                              '- Provide rich, colorful, multi-line detailed explanations of what went '\n",
      "                              'wrong (when applicable.)\n",
      "                             '\n",
      "                              '- Be compatible with existing Clojure error-handling practices for errors '\n",
      "                              \"thrown from libraries that Arachne doesn't control.\n",
      "                             \"\n",
      "                              '- Not violate expectations of experienced Clojure programmers.\n",
      "                             '\n",
      "                              '- Be robust enough not to cause additional problems.\n",
      "                             '\n",
      "                              '- Not break existing logging tools for production use.\n",
      "                             '\n",
      "                              '\n",
      "                             '\n",
      "                              '\n",
      "                             '\n",
      "                              '\n",
      "                             '\n",
      "                              '## Decision\n",
      "                             '\n",
      "                              '\n",
      "                             ')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'## Context\\n\\nHistorically, error handling has not been Clojure\\'s strong suit. For the most part, errors take the form of a JVM exception, with a long stack trace that includes a lot of Clojure\\'s implementation as well as stack frames that pertain directly to user code.\\n\\nAdditionally, prior to the advent of `clojure.spec`, Clojure errors were often \"deep\": a very generic error (like a NullPointerException) would be thrown from far within a branch, rather than eagerly validating inputs.\\n\\nThere are Clojure libraries which make an attempt to improve the situation, but they typically do it by overriding Clojure\\'s default exception printing functions across the board, and are sometimes \"lossy\", dropping information that could be desirable to a developer.\\n\\nSpec provides an opportunity to improve the situation across the board, and with Arachne we want to be on the leading edge of providing helpful error messages that point straight to the problem, minimize time spent trying to figure out what\\'s going on, and let developers get straight back to working on what matters to them.\\n\\nIdeally, Arachne\\'s error handling should exhibit the following qualities:\\n\\n- Never hide possibly relevant information.\\n- Allow module developers to be as helpful as possible to people using their tools.\\n- Provide rich, colorful, multi-line detailed explanations of what went wrong (when applicable.)\\n- Be compatible with existing Clojure error-handling practices for errors thrown from libraries that Arachne doesn\\'t control.\\n- Not violate expectations of experienced Clojure programmers.\\n- Be robust enough not to cause additional problems.\\n- Not break existing logging tools for production use.\\n\\n\\n\\n## Decision\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic(predicted_decision[3])\n",
    "#ic(context[3]+task_postfix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.1610488315019266, 'rouge2': 0.022533226149654105, 'rougeL': 0.09885479828843866, 'rougeLsum': 0.14434250874698384}\n"
     ]
    }
   ],
   "source": [
    "# T5-small\n",
    "results = rouge.compute(predictions=predicted_decision, references=decision)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.11970807676479195, 'rouge2': 0.016733677886901305, 'rougeL': 0.07682710334385254, 'rougeLsum': 0.10399354529591437}\n"
     ]
    }
   ],
   "source": [
    "# T5-base\n",
    "results = rouge.compute(predictions=predicted_decision, references=decision)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.10544956788291188, 'rouge2': 0.010530021414159743, 'rougeL': 0.06368265633429697, 'rougeLsum': 0.0948349236009566}\n"
     ]
    }
   ],
   "source": [
    "# T5-large\n",
    "results = rouge.compute(predictions=predicted_decision, references=decision)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.09808531151107666, 'rouge2': 0.008833037948976573, 'rougeL': 0.061372678600135504, 'rougeLsum': 0.08092215668539712}\n"
     ]
    }
   ],
   "source": [
    "# T5-3b\n",
    "results = rouge.compute(predictions=predicted_decision, references=decision)\n",
    "print(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/mt0-small\", cache_dir='/scratch/rudra.dhar/cache')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/mt0-small\", cache_dir='/scratch/rudra.dhar/cache', torch_dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "#model = DataParallel(model, device_ids=list(range(torch.cuda.device_count())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_postfix = \"\\n\\n## Decision\\n\\n\"\n",
    "#task_prefix = \"Generate Architecture Decisions for the given Context\\n\\n\"\n",
    "predicted_decision = []\n",
    "\n",
    "for i in range(len(context)):\n",
    "\n",
    "    inputs = tokenizer.encode(context[i]+task_postfix, return_tensors=\"pt\").to('cuda')\n",
    "    #print(len(inputs[0]))\n",
    "    outputs = model.generate(inputs, max_length= 4*len(inputs[0]), min_length= int(len(inputs[0])/8), num_beams=2, early_stopping=True)\n",
    "    predicted_decision.append(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| predicted_decision[0]: ('Arachne should provide an orderly, structured startup (and shutdown) '\n",
      "                            'procedure, and make it possible for modules and application authors to hook '\n",
      "                            'into it to ensure that their own code initializes, starts and stops as '\n",
      "                            'desired. Additionally, it must be possible for different system components '\n",
      "                            'to have dependencies on eachother, such that when starting, services start '\n",
      "                            '*after* the services upon which they depend. Stopping should occur in '\n",
      "                            'reverse-dependency order, such that a service is never in a state where it '\n",
      "                            'is running but one of its dependencies is stopped')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Arachne should provide an orderly, structured startup (and shutdown) procedure, and make it possible for modules and application authors to hook into it to ensure that their own code initializes, starts and stops as desired. Additionally, it must be possible for different system components to have dependencies on eachother, such that when starting, services start *after* the services upon which they depend. Stopping should occur in reverse-dependency order, such that a service is never in a state where it is running but one of its dependencies is stopped'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic(predicted_decision[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.049081480353366874, 'rouge2': 0.0039349584165071505, 'rougeL': 0.03320878100739684, 'rougeLsum': 0.04289647649698987}\n"
     ]
    }
   ],
   "source": [
    "# mt0-small\n",
    "results = rouge.compute(predictions=predicted_decision, references=decision)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.06020371742695052, 'rouge2': 0.005320204422447742, 'rougeL': 0.05234972027048991, 'rougeLsum': 0.060238076122571364}\n"
     ]
    }
   ],
   "source": [
    "# mt0-base\n",
    "results = rouge.compute(predictions=predicted_decision, references=decision)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.05882533217916382, 'rouge2': 0.006359292718529815, 'rougeL': 0.04936740567057264, 'rougeLsum': 0.0571762644851139}\n"
     ]
    }
   ],
   "source": [
    "# mt0-large\n",
    "results = rouge.compute(predictions=predicted_decision, references=decision)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.1997558340110492, 'rouge2': 0.02714840034405855, 'rougeL': 0.1038840211300788, 'rougeLsum': 0.16875886560965186}\n"
     ]
    }
   ],
   "source": [
    "# T0_3B\n",
    "results = rouge.compute(predictions=predicted_decision, references=decision)\n",
    "print(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flan-t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\", cache_dir='/scratch/rudra.dhar/cache')\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\", cache_dir='/scratch/rudra.dhar/cache', device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2739 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2739\n",
      "651\n",
      "387\n",
      "189\n",
      "825\n",
      "553\n",
      "640\n",
      "280\n",
      "316\n",
      "615\n",
      "400\n",
      "1186\n",
      "569\n",
      "278\n",
      "766\n",
      "410\n"
     ]
    }
   ],
   "source": [
    "#task_postfix = \"\\n\\n## Decision\\n\\n\"\n",
    "task_postfix = \"\\n\\nGenerate Architecture Decisions for the above Context\\n\\n\"\n",
    "predicted_decision = []\n",
    "\n",
    "for i in range(len(context)):\n",
    "    input_ids = tokenizer(context[i]+task_postfix, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    print(len(input_ids[0]))\n",
    "    outputs = model.generate(input_ids, max_length= 4*len(input_ids[0]), min_length= int(len(input_ids[0])/8), num_beams=2, early_stopping=True)\n",
    "    predicted_decision.append(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| predicted_decision[3]: ('Be compatible with existing Clojure error-handling practices for errors '\n",
      "                            \"thrown from libraries that Arachne doesn't control. - Be robust enough not \"\n",
      "                            'to cause additional problems. - Be robust enough not to cause additional '\n",
      "                            'problems.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Be compatible with existing Clojure error-handling practices for errors thrown from libraries that Arachne doesn't control. - Be robust enough not to cause additional problems. - Be robust enough not to cause additional problems.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic(predicted_decision[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.23026462680328255, 'rouge2': 0.023995739359983143, 'rougeL': 0.11502119708630912, 'rougeLsum': 0.1932619088080651}\n"
     ]
    }
   ],
   "source": [
    "# flan-t5-small\n",
    "results = rouge.compute(predictions=predicted_decision, references=decision)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.21651860751127877, 'rouge2': 0.02325270521914872, 'rougeL': 0.1118891155649692, 'rougeLsum': 0.17853674007211434}\n"
     ]
    }
   ],
   "source": [
    "# flan-t5-base\n",
    "results = rouge.compute(predictions=predicted_decision, references=decision)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.23849635959313403, 'rouge2': 0.030784912552023737, 'rougeL': 0.11709810049895392, 'rougeLsum': 0.1954424730636321}\n"
     ]
    }
   ],
   "source": [
    "# flan-t5-large\n",
    "results = rouge.compute(predictions=predicted_decision, references=decision)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.10012739431114606, 'rouge2': 0.011836331806799329, 'rougeL': 0.06551202759480268, 'rougeLsum': 0.09311356889977983}\n"
     ]
    }
   ],
   "source": [
    "# flan-t5-xl\n",
    "results = rouge.compute(predictions=predicted_decision, references=decision)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
